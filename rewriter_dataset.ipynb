{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import faiss\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import nlpaug.augmenter.word as naw\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from util_modeling import get_model_objects\n",
    "from util_data import get_formatted_dataset, get_num_labels\n",
    "from adaptive_methods import get_paraphrase_augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_formatted_dataset(\"boss_sentiment\")\n",
    "train_set = datasets[\"train\"].to_pandas().drop(columns=[\"__index_level_0__\"])\n",
    "test_set = datasets[\"validation\"].to_pandas().drop(columns=[\"__index_level_0__\"])\n",
    "display(train_set.head())\n",
    "display(test_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"humarin/chatgpt_paraphraser_on_T5_base\"\n",
    "# model_name = \"princeton-nlp/sup-simcse-roberta-large\"\n",
    "# model_name = \"humarin/chatgpt_paraphraser_on_T5_base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer, model = get_model_objects(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29999/29999 [10:57<00:00, 45.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of my favorites</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.2169, 0.0548, -0.3027, 0.00297, -0.04462, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My favorite Coarse Sea Salt brand I know about...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0527, -0.0285, -0.0876, 0.002548, -0.03056,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Love the top! It fits a little tight, so can b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.12036, -0.08734, -0.04828, 0.0815, -0.0334...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very nice &amp; I like it for everything I used it...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.09247, -0.09595, -0.1056, -0.0383, -0.1348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Awesome product!</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.1068, -0.1333, -0.02583, 0.04718, -0.1442,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0                                One of my favorites      1   \n",
       "1  My favorite Coarse Sea Salt brand I know about...      1   \n",
       "2  Love the top! It fits a little tight, so can b...      1   \n",
       "3  very nice & I like it for everything I used it...      1   \n",
       "4                                   Awesome product!      1   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.2169, 0.0548, -0.3027, 0.00297, -0.04462, ...  \n",
       "1  [0.0527, -0.0285, -0.0876, 0.002548, -0.03056,...  \n",
       "2  [-0.12036, -0.08734, -0.04828, 0.0815, -0.0334...  \n",
       "3  [-0.09247, -0.09595, -0.1056, -0.0383, -0.1348...  \n",
       "4  [-0.1068, -0.1333, -0.02583, 0.04718, -0.1442,...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(text):\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        if model_name == \"humarin/chatgpt_paraphraser_on_T5_base\":\n",
    "            return model(**tokens, decoder_input_ids=tokens[\"input_ids\"], output_hidden_states=True)[\"encoder_last_hidden_state\"].mean(dim=1).squeeze().detach().cpu().numpy()\n",
    "        elif model_name == \"princeton-nlp/sup-simcse-roberta-large\":\n",
    "            return model(**tokens)[\"pooler_output\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            return model(**tokens, output_hidden_states=True)[\"hidden_states\"][-1].mean(dim=1).squeeze().detach().cpu().numpy()\n",
    "\n",
    "train_set[\"embedding\"] = train_set[\"text\"].progress_apply(get_embedding)\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.2756e-02, -6.0333e-02, -6.7755e-02,  5.4552e-02, -2.7676e-02,\n",
       "         1.7895e-02, -7.9711e-02, -6.7177e-02, -5.8001e-02, -2.0899e-03,\n",
       "        -1.5487e-02, -3.2947e-02, -7.6349e-03,  4.9093e-02,  2.3862e-02,\n",
       "         6.8389e-03, -3.0690e-02,  3.9634e-02,  9.5420e-02,  3.2718e-02,\n",
       "        -3.1661e-02, -1.8483e-02,  6.1046e-02,  1.2202e-03, -3.5695e-02,\n",
       "        -9.8095e-03,  2.2426e-02,  7.1941e-03,  2.2082e-03, -2.2697e-02,\n",
       "        -3.1966e-03,  1.0527e-02,  8.3322e-02, -7.2907e-03,  1.3515e-02,\n",
       "        -6.8795e-03, -1.4656e-01, -4.2748e-03,  2.0008e-03,  6.8266e-02,\n",
       "        -1.2039e-02,  6.1029e-02,  1.2464e-01, -2.6741e-02, -3.3780e-01,\n",
       "        -8.9305e-03, -3.5918e-02,  7.9114e-03, -3.7333e-02,  4.5892e-03,\n",
       "         1.2873e-02,  3.0469e-02, -4.8068e-02,  2.6156e-02, -7.9206e-02,\n",
       "         7.2732e-03,  2.9686e-01, -7.6005e-03, -1.5971e-04,  8.4089e-03,\n",
       "         6.4910e-02, -1.9394e-02,  3.9386e-03, -5.8784e-03,  9.5261e-03,\n",
       "        -7.7078e-02, -3.6686e-02, -2.6133e-02,  9.1758e-03,  1.9167e-03,\n",
       "         2.6389e-02, -8.3476e-02,  1.3987e-02, -7.9441e-02,  1.6907e-02,\n",
       "        -9.8740e-03, -3.9007e-02, -4.6957e-03,  7.1220e-03,  2.1829e-02,\n",
       "        -1.3681e-02, -1.6083e-02,  5.3590e-02,  7.0025e-02,  2.8242e-02,\n",
       "         6.7374e-02, -4.1632e-02, -4.0554e-02, -7.6128e-03,  4.0580e-02,\n",
       "        -2.7219e-02,  3.4877e-02,  3.1306e-02, -1.6241e-02,  5.0188e-03,\n",
       "        -1.8497e-02, -3.3732e-03,  2.6496e-02, -2.4276e-02,  2.2226e-02,\n",
       "         3.5930e-02,  2.2302e-02,  7.9811e-02, -3.6219e-02, -4.4375e-02,\n",
       "         7.6940e-02,  1.0889e-02, -1.8989e-02, -5.4944e-03,  1.0642e-01,\n",
       "         3.7055e-02,  1.6562e-02, -1.3881e-02, -2.6556e-02, -6.8295e-03,\n",
       "        -3.7397e-02, -1.5118e-02, -3.3824e-02, -3.0437e-02, -2.0350e-03,\n",
       "         2.7694e-02,  5.8755e-03,  1.1231e-01,  1.3786e-02,  6.9565e-02,\n",
       "         3.6045e-02, -4.0907e-02, -3.4415e-02, -2.7548e-02,  6.4924e-03,\n",
       "        -1.3484e-01, -1.6549e-02,  4.6544e-02,  1.5627e-02,  1.1367e-02,\n",
       "        -3.4091e-02, -4.1430e-02,  2.1669e-02, -7.7291e-03, -5.8477e-02,\n",
       "         3.0109e-02, -1.0648e-01,  1.7784e-02,  7.6010e-02,  5.3116e-02,\n",
       "         1.1039e-02,  4.3259e-02, -6.6271e-03,  7.7650e-03, -1.1708e-02,\n",
       "         3.4026e-02,  3.1000e-02,  8.5346e-03,  1.4123e-02, -9.6941e-03,\n",
       "        -4.5447e-02, -1.1206e-02, -6.4263e-02,  2.7151e-02,  2.2360e-03,\n",
       "        -2.5770e-02, -3.3468e-02,  2.3959e-02,  2.6774e-01, -8.9906e-03,\n",
       "         2.4244e-02, -2.4727e-02,  2.6809e-02, -7.8578e-03,  2.4508e-01,\n",
       "        -2.7146e-02, -1.9824e-02, -9.4372e-03,  1.1800e-02,  4.3499e-03,\n",
       "        -1.7503e-02, -3.2466e-02, -6.1787e-03, -1.3470e-02,  4.3699e-02,\n",
       "         1.8170e-02, -3.9468e-02,  2.0613e-02,  1.9546e-02, -1.3804e-02,\n",
       "        -1.4727e-02,  2.9144e-02,  7.0564e-04,  1.3472e-02,  3.0199e-03,\n",
       "         1.7306e-02,  2.8342e-03,  1.3359e-02, -1.0574e-02, -1.7809e-02,\n",
       "         4.3004e-02,  1.5356e-02,  3.0293e-03,  5.5332e-02,  1.0534e-02,\n",
       "         1.6161e-02,  1.4762e-02, -1.9975e-02, -1.3414e-02, -9.9487e-03,\n",
       "        -5.6299e-03, -2.3506e-02, -1.4472e-02,  5.2328e-03,  2.9673e-03,\n",
       "         1.9079e-02, -7.6625e-03, -5.0527e-02, -5.5589e-03, -5.3197e-02,\n",
       "        -5.4630e-02, -5.4622e-03, -8.6575e-03, -3.7970e-02, -1.5152e-01,\n",
       "         1.5408e-02,  3.2247e-03, -3.6182e-02, -4.0025e-02,  5.6784e-03,\n",
       "         2.2187e-02,  5.5471e-02,  1.0834e-01, -5.7836e-03, -3.4949e-02,\n",
       "         1.9114e-05, -1.1815e-02, -2.8649e-02, -1.6170e-05,  3.3887e-02,\n",
       "        -1.0009e-01, -1.0172e-01, -4.2662e-02, -2.9347e-02, -3.6936e-02,\n",
       "        -2.7029e-03, -2.1998e-02,  2.2698e-02, -1.5700e-02,  3.2599e-02,\n",
       "        -1.6281e-02, -3.0342e-02, -1.3204e-02, -2.0407e-02, -3.6998e-02,\n",
       "         1.1951e-01,  2.3165e-02, -2.8292e-02,  1.4073e-02,  1.7202e-02,\n",
       "         2.2390e-02,  2.6528e-03,  2.5625e-02,  8.4686e-02, -8.8604e-03,\n",
       "        -3.9983e-02,  3.4176e-01,  1.9241e-02, -1.7381e-02, -5.2066e-03,\n",
       "         3.1425e-02,  7.0662e-02,  6.3677e-03, -2.9908e-02,  5.3287e-04,\n",
       "        -5.9445e-02,  2.8567e-02, -1.9441e-03, -3.7231e-03, -7.6296e-02,\n",
       "         6.3477e-02, -1.6371e-02, -6.3608e-03,  1.2976e-02, -1.5555e-02,\n",
       "         1.3056e-02,  2.2845e-02,  8.7004e-03, -3.1107e-02, -3.4965e-02,\n",
       "         3.9664e-02,  1.2844e-02, -2.5579e-02, -4.2277e-03,  1.1523e-01,\n",
       "        -5.7796e-02,  7.9353e-04,  9.3507e-04, -1.0694e-02, -7.4198e-04,\n",
       "        -3.5845e-02, -1.5241e-02, -1.8795e-02,  1.1727e-02,  4.1073e-02,\n",
       "         3.2529e-02,  7.4006e-03,  1.8697e-02, -2.9600e-02,  1.9204e-02,\n",
       "        -3.4324e-02,  2.9061e-02,  5.2033e-03, -3.6918e-02,  9.0631e-03,\n",
       "         1.9649e-02, -5.5358e-02,  3.7775e-03, -7.2822e-02, -6.6651e-03,\n",
       "        -2.7468e-03,  4.5869e-02, -3.9851e-02,  4.4888e-02, -2.6949e-02,\n",
       "         5.3655e-02,  1.0558e-01, -1.6459e-02,  2.8503e-02, -3.8408e-02,\n",
       "         6.9673e-03,  5.2804e-02,  5.6454e-03,  2.0379e-02, -3.2086e-02,\n",
       "        -1.6415e-02,  9.3764e-02, -8.7459e-02, -4.5293e-03, -1.6741e-02,\n",
       "         5.2958e-03, -1.7506e-02,  6.6923e-03, -4.7576e-02, -2.0285e-02,\n",
       "         7.3313e-03, -1.6098e-02, -5.1138e-02, -2.3657e-04, -1.1070e-02,\n",
       "        -1.4268e-02, -1.5422e-01, -1.1788e-02,  1.0572e-01, -1.7201e-01,\n",
       "        -5.3604e-03,  1.5214e-02, -8.9434e-03, -6.1386e-03,  1.5173e-02,\n",
       "        -1.1542e-02, -1.4277e-02,  9.1746e-03,  3.4821e-02,  3.9118e-03,\n",
       "        -3.7023e-02, -7.3081e-02, -6.1624e-02, -1.0175e-02, -2.4915e-02,\n",
       "         7.1702e-03, -3.0740e-02,  5.1916e-02, -5.5517e-02, -4.2752e-02,\n",
       "        -3.5103e-02, -2.8207e-02,  2.0664e-02, -1.1249e-02, -3.9479e-02,\n",
       "        -4.5940e-02, -9.5259e-02, -3.2064e-03,  4.3806e-02, -8.1838e-02,\n",
       "        -2.5534e-02,  2.3480e-02, -5.6990e-02, -5.4366e-03, -1.8029e-02,\n",
       "        -3.0257e-02, -4.1974e-02, -5.4486e-02, -5.7721e-03, -2.4313e-02,\n",
       "         1.0508e-02,  2.6596e-02,  3.3800e-02, -6.0334e-02,  5.9462e-02,\n",
       "         2.9401e-02,  2.5035e-02,  1.0137e-02,  2.9300e-02, -1.8604e-03,\n",
       "        -1.5181e-02,  5.4266e-02,  2.6479e-02, -3.3216e-02,  6.5712e-02,\n",
       "         7.5178e-03, -3.5811e-04,  1.6564e-02, -1.3170e-02, -9.5763e-03,\n",
       "         4.3454e-02,  1.4989e-02,  4.0041e-03,  4.2877e-02, -1.4628e-02,\n",
       "         7.7775e-03, -2.8319e-03,  1.3776e-03,  3.7493e-02, -3.8863e-02,\n",
       "         3.1156e-02, -6.6296e-02, -1.0505e-02, -6.9881e-02, -1.4497e-02,\n",
       "        -4.7213e-02, -5.2281e-03, -1.6083e-02, -7.6894e-04,  1.0540e-02,\n",
       "        -5.2125e-02, -7.2317e-03, -5.5599e-02, -3.5889e-02, -9.5006e-02,\n",
       "        -3.8090e-02, -9.9908e-03, -7.5187e-03,  1.7441e-01,  9.4951e-03,\n",
       "        -3.2339e-02,  6.2208e-02,  1.5670e-02, -9.2003e-02, -2.7088e-02,\n",
       "        -9.2342e-03,  4.1545e-02, -8.9456e-03, -7.9852e-02, -2.4952e-02,\n",
       "         3.4435e-02,  2.5888e-02, -1.9549e-02, -1.4546e-02,  6.2944e-02,\n",
       "        -4.2943e-02,  4.1781e-02,  3.6522e-02,  6.4528e-03,  4.7783e-02,\n",
       "         6.7939e-03,  3.6469e-03,  1.3302e-02,  3.8413e-02,  2.5012e-02,\n",
       "         2.5417e-02,  3.0057e-02,  1.9351e-02,  1.7635e-02, -3.6372e-02,\n",
       "         3.0882e-04, -2.5123e-02,  3.0075e-02,  5.4734e-03,  4.6365e-03,\n",
       "         5.5710e-02, -4.0673e-02,  2.7187e-04,  8.4230e-03,  1.6357e-02,\n",
       "        -2.4146e-02, -2.1426e-01,  1.8437e-03,  5.1499e-02, -2.9035e-02,\n",
       "         2.6404e-03,  1.3886e-02,  1.0173e-02,  1.9385e-02,  4.1435e-02,\n",
       "         2.0643e-03,  2.8944e-02,  6.4844e-02,  3.0783e-02,  3.3564e-02,\n",
       "        -4.1377e-02, -1.4516e-02,  5.2245e-02, -2.7882e-02,  2.5820e-02,\n",
       "        -4.9507e-02,  5.9716e-03, -8.0107e-02, -1.9550e-02, -1.1447e-02,\n",
       "        -3.5700e-02,  1.0290e-02, -3.6163e-02,  3.8937e-02, -5.5627e-02,\n",
       "         1.4827e-02,  1.2891e-02, -1.6380e-01,  3.9632e-02, -2.6879e-02,\n",
       "         2.2653e-02,  1.2743e-02, -2.1865e-03, -3.4135e-02, -3.3346e-03,\n",
       "         1.9324e-02, -5.0281e-02, -2.6239e-03, -7.8102e-03, -1.7242e-03,\n",
       "        -3.5220e-01,  1.2369e-02, -7.9855e-02, -1.9949e-02,  3.1237e-03,\n",
       "        -4.7537e-02, -1.7813e-02,  8.6630e-03,  4.2772e-02, -5.8777e-02,\n",
       "         7.0122e-02, -2.3852e-03, -5.4870e-04,  4.3020e-02, -8.5418e-04,\n",
       "        -2.8318e-02, -3.7450e-02,  5.1318e-02, -4.1699e-03,  3.4817e-02,\n",
       "         4.7661e-02,  1.2281e-02,  5.6679e-03,  2.3997e-02, -8.8425e-02,\n",
       "        -1.3613e-01, -1.8957e-02, -1.5611e-02,  2.5730e-03, -5.2802e-02,\n",
       "         1.0186e-02,  1.7145e-02, -1.6033e-02, -1.0140e-02, -1.3781e-01,\n",
       "         2.5006e-03, -1.0833e-02,  2.6974e-02, -4.5845e-02, -7.7164e-02,\n",
       "        -8.9873e-03,  4.5622e-02, -2.8620e-02,  2.9007e-02, -2.8388e-02,\n",
       "         5.7949e-02, -1.9543e-02,  1.0857e-02, -1.1395e-02,  3.4651e-02,\n",
       "         3.9234e-02,  1.2558e-02,  2.8554e-02,  2.5097e-02, -8.6639e-02,\n",
       "         1.5594e-02,  2.0568e-02, -2.1811e-02, -1.3749e-02,  3.0613e-02,\n",
       "         1.6315e-02, -6.1647e-03,  2.4376e-02, -2.2739e-02,  3.9347e-03,\n",
       "         2.2571e-02, -4.1818e-02,  7.6120e-03, -2.3925e-02,  4.2421e-02,\n",
       "         3.5615e-02,  7.6758e-02, -1.5549e-02,  5.7117e-02,  3.6613e-02,\n",
       "         3.5681e-02, -4.4647e-03, -6.3243e-03,  3.2363e-02, -4.2521e-02,\n",
       "         3.7804e-02,  2.8225e-03,  3.7082e-02, -1.6793e-02,  7.6321e-03,\n",
       "         1.0165e-02,  2.2149e-02, -3.1366e-02,  1.4324e-02,  6.9791e-03,\n",
       "        -2.6387e-02, -5.0629e-02, -2.8426e-03, -8.0740e-04,  2.4195e-03,\n",
       "        -5.4857e-02,  7.9254e-02,  1.7448e-02, -9.0555e-03,  1.5248e-02,\n",
       "         1.0468e-02, -1.5857e-02,  1.2057e-01,  2.5115e-02, -1.1136e-01,\n",
       "        -2.0455e-02,  1.1640e-03,  8.6968e-03,  7.2756e-02, -4.3509e-02,\n",
       "         4.6884e-02,  4.7159e-02,  7.8776e-03,  1.4953e-01,  3.5195e-03,\n",
       "        -5.9480e-04,  1.5845e-02, -6.4334e-02, -4.9904e-02,  6.2009e-02,\n",
       "        -4.6040e-02,  2.5191e-02,  3.7468e-03,  3.3227e-03,  2.0359e-02,\n",
       "        -2.2404e-02, -1.0240e-02,  4.3915e-03,  1.2168e-02, -4.9603e-02,\n",
       "        -1.3099e-02,  2.9686e-02,  3.7951e-02, -9.5153e-02,  4.2094e-02,\n",
       "        -1.2388e-01, -6.0439e-02, -2.5276e-02, -9.1041e-03,  2.6214e-02,\n",
       "        -2.5179e-02,  1.1774e-02, -2.4505e-02,  6.9910e-03,  1.9929e-03,\n",
       "         1.9028e-02,  2.9457e-02, -1.8286e-02, -5.1169e-02, -1.0261e-02,\n",
       "         1.5539e-02,  3.2511e-02,  1.4657e-02, -2.2844e-02, -5.4331e-03,\n",
       "        -1.5361e-03, -2.1541e-02,  4.3145e-02, -5.6499e-02,  4.1368e-02,\n",
       "        -4.9573e-02,  6.7227e-03, -2.4558e-03,  9.3260e-03,  4.1153e-02,\n",
       "        -1.1184e-02, -7.8182e-03,  1.9195e-02, -2.4000e-03, -3.7478e-03,\n",
       "         2.7780e-02,  5.3826e-02, -1.1466e-02,  9.4081e-03,  2.7978e-03,\n",
       "         1.9954e-02,  5.1784e-02,  2.1588e-02, -3.3686e-02, -4.7868e-02,\n",
       "         3.3400e-02, -1.6581e-02,  6.4310e-02, -5.6941e-02, -1.5771e-02,\n",
       "         2.3927e-01, -9.6926e-03, -8.6533e-03,  2.0962e-02,  1.1274e-01,\n",
       "         3.9097e-02, -4.1000e-02,  5.9257e-03,  1.4952e-02,  2.2894e-02,\n",
       "         1.0488e-02,  1.0191e-01,  2.5886e-02,  1.0284e-02,  3.0429e-02,\n",
       "         1.7526e-02,  2.7716e-02,  2.0947e-02, -6.0129e-03, -6.1570e-02,\n",
       "        -7.6594e-03, -1.3961e-04, -5.9103e-03, -1.3662e-02,  2.0905e-02,\n",
       "         7.0822e-03, -2.2992e-02, -4.8779e-02, -4.7604e-02,  2.8042e-02,\n",
       "        -2.1459e-02, -1.1802e-02, -1.4438e-02,  2.7437e-02, -7.7604e-03,\n",
       "         1.5176e-02,  7.3207e-02,  3.6280e-02, -3.7176e-02, -1.5122e-01,\n",
       "        -8.1639e-04,  3.1665e-02,  1.2357e-02, -5.2774e-02,  2.8479e-02,\n",
       "         5.2263e-03, -6.0575e-02, -5.5541e-02, -3.5853e-02, -1.8518e-02,\n",
       "         2.1291e-02, -7.0671e-03, -1.5437e-02, -2.4807e-02,  5.2781e-02,\n",
       "        -4.1204e-02,  3.5348e-02, -6.6583e-06])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.stack(train_set[\"embedding\"])).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29999, 768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set_embeddings = torch.Tensor(np.stack(train_set[\"embedding\"])).squeeze(1)\n",
    "display(train_set_embeddings.shape)\n",
    "torch.save(train_set_embeddings, f\"notebooks/dynasent_analysis/amazon_train_embeddings_{model_name.replace('/', '-')}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_centroid = test_set_embeddings.mean(dim=0)\n",
    "display(test_set_centroid.shape)\n",
    "torch.save(test_set_centroid, f\"notebooks/dynasent_analysis/amazon_validation_centroid_{model_name.replace('/', '-')}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_set[\"label\"].unique()\n",
    "vector_stores = {}\n",
    "centroids = {}\n",
    "centroid_examples = {}\n",
    "k = 10\n",
    "d = 1024\n",
    "\n",
    "for label in labels:\n",
    "    label_instances = train_set[train_set[\"label\"] == label]\n",
    "    label_embeddings = np.stack(label_instances[\"embedding\"].to_numpy()).astype(np.float32)\n",
    "    \n",
    "    faiss.normalize_L2(label_embeddings)\n",
    "    vector_stores[label] = faiss.IndexFlatIP(d)\n",
    "    vector_stores[label].add(label_embeddings)\n",
    "    centroids[label] = label_embeddings.mean(axis=0)\n",
    "    \n",
    "    cosine_sims, centroid_example_indices = vector_stores[label].search(centroids[label].reshape(1, -1), k)\n",
    "    centroid_examples[label] = []\n",
    "    for index in centroid_example_indices[0]:\n",
    "        centroid_examples[label].append(label_instances.iloc[index][\"text\"])\n",
    "\n",
    "centroid_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_set_records = []\n",
    "for inde, row in tqdm(train_set.iterrows(), total=len(train_set)):\n",
    "    current_label = row[\"label\"]\n",
    "    current_text = row[\"text\"]\n",
    "    for example in centroid_examples[current_label]:\n",
    "        new_train_set_records.append({\"text\": current_text, \"label\": example, \"class\": current_label})\n",
    "\n",
    "rewrite_train_set = pd.DataFrame(new_train_set_records).sample(frac=1).reset_index(drop=True)\n",
    "rewrite_train_set.to_csv(\"datasets/corrupted/boss_sentiment_train.csv\", index=False)\n",
    "display(rewrite_train_set)\n",
    "\n",
    "new_test_set_records = []\n",
    "for inde, row in tqdm(test_set.iterrows(), total=len(test_set)):\n",
    "    current_label = row[\"label\"]\n",
    "    current_text = row[\"text\"]\n",
    "    for example in centroid_examples[current_label]:\n",
    "        new_test_set_records.append({\"text\": current_text, \"label\": example, \"class\": current_label})\n",
    "\n",
    "rewrite_test_set = pd.DataFrame(new_test_set_records).sample(frac=1).reset_index(drop=True)\n",
    "rewrite_test_set.to_csv(\"datasets/corrupted/boss_sentiment_test.csv\", index=False)\n",
    "display(rewrite_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_tokenizer, paraphrase_model = get_model_objects(\"humarin/chatgpt_paraphraser_on_T5_base\", num_labels=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"I use this every day I would recommend this for anyone who has special needs with thinning hair, it has made a huge difference in my daily life.\"\n",
    "get_paraphrase_augmentations(example_text,\n",
    "                             paraphrase_tokenizer,\n",
    "                             paraphrase_model,\n",
    "                             paraphrase_model.device,\n",
    "                             num_return_sequences=4,\n",
    "                             temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_set_records = []\n",
    "for _, row in tqdm(train_set.iterrows(), total=len(train_set)):\n",
    "    current_label = row[\"label\"]\n",
    "    current_text = row[\"text\"]\n",
    "    augmentations = get_paraphrase_augmentations(current_text,\n",
    "                             paraphrase_tokenizer,\n",
    "                             paraphrase_model,\n",
    "                             paraphrase_model.device,\n",
    "                             num_return_sequences=4,\n",
    "                             temperature=0.3)\n",
    "    \n",
    "    for example in centroid_examples[current_label]:\n",
    "        for text_input in [current_text] + augmentations:\n",
    "            new_train_set_records.append({\"text\": text_input, \"label\": example, \"class\": current_label})\n",
    "\n",
    "rewrite_train_set = pd.DataFrame(new_train_set_records).sample(frac=1).reset_index(drop=True)\n",
    "rewrite_train_set.to_csv(\"datasets/corruped/boss_sentiment_augmented_train.csv\", index=False)\n",
    "display(rewrite_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_set_records = []\n",
    "for _, row in tqdm(test_set.iterrows(), total=len(test_set)):\n",
    "    current_label = row[\"label\"]\n",
    "    current_text = row[\"text\"]\n",
    "    augmentations = get_paraphrase_augmentations(current_text,\n",
    "                             paraphrase_tokenizer,\n",
    "                             paraphrase_model,\n",
    "                             paraphrase_model.device,\n",
    "                             num_return_sequences=4,\n",
    "                             temperature=0.3)\n",
    "    \n",
    "    for example in centroid_examples[current_label]:\n",
    "        for text_input in [current_text] + augmentations:\n",
    "            new_test_set_records.append({\"text\": text_input, \"label\": example, \"class\": current_label})\n",
    "\n",
    "rewrite_test_set = pd.DataFrame(new_test_set_records).sample(frac=1).reset_index(drop=True)\n",
    "rewrite_test_set.to_csv(\"datasets/corruped/boss_sentiment_augmented_test.csv\", index=False)\n",
    "display(rewrite_test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
