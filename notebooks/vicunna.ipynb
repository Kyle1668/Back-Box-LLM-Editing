{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyle/miniconda3/envs/kne/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.65s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"TheBloke/vicuna-13B-1.1-HF\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/vicuna-13B-1.1-HF\", device_map=\"auto\", torch_dtype=torch.float16).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: .... . ... .....   ................. . ............................. .. .. .......      ........................................................... . . . ......... . . .. .. . ........................................................................................................................................................................................................................................................................        ...........                .............           .\n"
     ]
    }
   ],
   "source": [
    "prompt = 'User: Paraphrase the input text into the exact writing style of the following examples while keeping the same semantic meaning. Keep all facts and information.\\nExamples:\\n\"Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we\\'ve loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife\\'s death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth\\'s pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\"\\n\"Rather than move linearly from beginning to end, this story line of a gay couple impacted by AIDS \"orbits\" in time around their \"perfect day.\" The film is organized as a life remembered in asynchronous fragments rather than in a sequential flow as one directly experienced.<br /><br />The narration has its lyrical moments, particularly in describing the impact of loss anticipated or experienced. The dialog unfortunately lacks such grace. The script frequently compels the actors to say startlingly stupid or insensitive things that seem utterly out of character at the moment. On their second accidental encounter, clearly smitten with each other, sensitive Phillip encourages a reluctant Guy to tell him about his difficult week. But the moment Guy begins to open up, Phillip, an English Major, blurts out \"You\\'re not a Crisis Fairy, are you?\" Later, watching his lover\\'s naked, chiseled body stride across the bedroom toward him, our young Shakespeare in love begins to render the beauty of the moment in words, \"The way you cut through space....I can\\'t even describe it\"--but lacks the verbal skills to complete his thought. This kind of drivel continues through the AIDS Hospice scenes, bejeweled with lines like, \"What made me think death would be all neat and tied up with ribbons?\" and \"You make Florence Nightingale look like Nurse Ratchet.\" <br /><br />The film often suffers from a bruising lack of subtlety. Unlikable characters are far more jarring and steamroller-flattened than they need to be. Phillip\\'s thoroughly annoying friends--an arrogant trust fund brat and a whining, needy dweeb--maintain a running caustic diatribe about every one crossing their path. Such patter could offer a writer a wealth of opportunities for clever social commentary, but sadly, their remarks are merely unpleasant, ungraced by wit or insight. It\\'s hard to know if our scriptwriter intentionally crafted intellectually limited characters or if he was simply running his tether\\'s perimeter.<br /><br />The plot may be what most appeals to and resonates with those who praise this film. It does seriously explore 1980\\'s US middle class gay life: first encounters, courting, coupling, nesting, the complexity of open relationships, friction and fracturing, dissolution, physical abuse, rapprochement, forgiveness, terminal illness, death and survival. Leads Phelan and Spirtas give fair to good performances rendering complex characters over time. Their fetching good looks help explain both the chemistry that held these two together through insensitivity and selfishness as well as the chemistry that helped some some viewers overlook this film\\'s painful weaknesses. The decision to chop the plot arc into tidbits and present them in out-of-sequence flashbacks added complexity without any evident dramatic utility, and in several cases left the sequence and thus the implications of a given event unclear.<br /><br />Could I recommend the film? To sticklers for literary and technical quality, absolutely not! For easy going viewers in serious need of an AIDS survivor catharsis or in the mood for a guilty-pleasure tearjerker with a little eye candy thrown in, maybe. But better written alternatives exploring the impact of AIDS on relationships of that era include: Philadelphia, And the band played on, Longtime companion, Angels in America, An early frost, Parting glances, Love! Valour! Compassion! and even Jeffrey.\"\\n\"Boring, badly written Italian exploitation flick.Lots of nudity, gore and awful acting.The werewolf makeup was the only thing that would raise a laugh.Complete rubbish-even for fans of cheesy Italian horror.Please avoid.\"\\n\"Admirably odd, though mean-spirited comedy-drama about a strange young man who hopes to fly like a bird through the Houston Astrodome. Robert Altman-directed quasi-comedy with eccentric characters is so overloaded with weirdos that it starts to creak early on from the weight. Some of the cinematography is evocative, Shelley Duvall is a stitch in her debut as a tour guide, and Sally Kellerman looks every inch the glamourpuss as Bud Cort\\'s vision of a \"mother bird\" (imagine Altman and producer Lou Adler explaining that role to her!). In the lead, Bud Cort is--once again, after \"Harold & Maude\"--a true original; not off-putting like, say, Michael J. Pollard, Cort manages to be geeky, wacky and inoffensive, a tough act to pull off. Unfortunately, this is one of Altman\\'s misfires. He can put together a cast and a showpiece like no one else, but let him get fired up with some misguided inspiration and he spirals downward. ** from ****\"\\n\"I guess this is meant to be a sort of reworking or updating of \"Beauty and the Beast\", but I can\\'t say I\\'ve ever watched a movie that began with several minutes of graphic horse sex. Wow. Anyway it seems that a young woman and her..aunt? Have traveled to this castle in France where the woman is to be married to the son of the castle owner, who is the man who takes care of making sure the horses get their rocks off. It seems that there are legends in that area of a beast that was rather, uh, frisky, I guess you could say, with the ladies, or at least, one in particular. There are all kinds of references tucked away in that regard but every time the soon-to-be-blushing young bride gets her curious little hands on one the groom\\'s father removes it from her sight. Anyway, the young bride-to-be goes upstairs to sleep while the family is waiting for a Cardinal to show up to the wedding (a family member, I guess) and as she dreams she dreams of a beast in the woods that has its way with her. The effects in this leave a little to be desired, and any attempt at eroticism (not that I know much about that) is kind of rendered laughable, especially when certain featured appendages appear about as realistic as a bed post or a baseball bat. This has a rather strange and abrupt, yet twist ending, with not really any clues or much build up to it, but it was kind of fitting and definitely not what I expected. I don\\'t know, this is kind of a tough one to get through but it has its moments and is definitely weird. 7 out of 10.\"\\n\"The jokes are obvious, the gags are corny, and the characters are walking characatures - but I couldn\\'t stop from laughing at his highly entertaining movie. No matter how many times I see it, I still get a kick out of this one, and I recommend it highly for all lovers of mindless entertainment. It contains many quotable moments, and some of the best sight-gags I\\'ve seen to this day. If you\\'ve had a bad week and you need a chuckle, rent this one on your way home Friday night to give your weekend a good start.\"\\n\"This delectable fusion of New Age babble and luridly bad film-making may not \"open\" you up, to borrow one of the film\\'s favorite verbs, but it might leave your jaw slack and your belly sore from laughter or retching. Based on the best-selling book by James Redfield, first (self) published in 1993, this cornucopia of kitsch tracks the spiritual awakening of an American history teacher (Matthew Settle) who, on traveling to deepest, darkest, phoniest Peru and sniffing either the air or something else more illegal. Namely what he discovers is a schlock Shangri La populated by smiling zombies who may be nuts or just heavily medicated, perhaps because they\\'re often accompanied by a panpipe flourish and an occasional shout out from a celestial choir. Although there\\'s a lot of talk about \"energy,\" that quality is decidedly missing from the motley cast whose numbers include Thomas Kretschmann, Annabeth Gish, Hector Elizondo and Jurgen Prochnow, all of whom are now firmly ensconced in the camp pantheon. For those who care, the plot involves the military, terrorists and the Roman Catholic Church; Armand Mastroianni provided the inept direction while Mr. Redfield, Barnet Bain and Dan Gordon wrote the hoot of a script. In short, easily the worst film seen in 40+ years of viewing movies.\"\\n\"Good historical drama which is very educational and also very entertaining to people who like history.Very good acting and script.Not as sensual and sexy as it is sometimes marketed,be prepared to peek into the pioneer spirit and human ability to adjust.Very touching as well for the spiritually mature. Not for people who do not like to think......\"\\n\"Got to be one of the best political satires I have seen to date, with an excellent performance for Cusak, Tomei, and all the supporting actors.<br /><br />Excellent plot, very well-placed and a very good unexpected twist at the end. The action scenes were well filmed & choreographed. Very funny.<br /><br />All in all I give this film a big thumbs up. It\\'s extremely critical of US military intervention in the middle-east, and as such, it may receive bad reviews from people who don\\'t share the same political view, or those who are simply too politically ignorant to appreciate the dark and drk humour. Indeed, at places, the comedy was so close to the truth that it was borderline between funny and tragic.\"\\n\"A featherweight plot and dubious characterizations don\\'t make any difference when a movie is as fun to watch as this one is. Lively action and spectacular stunts - for their day - give this movie some real zip. And there\\'s some actual comedy from the ripping chemistry between the two leads. Quinn makes a good villain also, although his role is completely overshadowed.<br /><br />But don\\'t be fooled by Maureen O\\'Hara\\'s tough broad role, this is as sexist as any Hollywood movie of this era. You might be able to forgive that because of the time in which it was made, but it\\'s still hard to get past. For all the heroism and gruesomely adult off-screen situations, this is still little more than an adolescent good time.\"\\n\"i can\\'t say i liked this movie very much.it has some amusing moments,but it doesn\\'t seem able to make up its mind whether it is a comedy or a drama.it doesn\\'t really work as either.it\\'s too light in tone to be a drama,and the amusing moments are few and far between.it also doesn\\'t make a lot of sense.things seem to happen for no reason.and it\\'s also extremely convoluted.i feel like they just made things up as they were going.if they had just taken a bit of time to explain things,this might have been a better movie.i would say the ending was anti climatic, but that would mean the rest of the movie had actually been building up to something,which it didn\\'t.it just sorts ends,and that\\'s that.i didn\\'t find it boring,really,but like i said,there there just isn\\'t any point.i\\'ll give Winter Kills a reluctant and weak 3/10\"\\n\"this film is absolutely hilarious. basically, the plot revolves around a serial killer being somehow turned into a snowman through some B-movie chemical accident. he then heads for town and starts terrorising the locals. its up to the local police chief and some other characters to try and stop him. its made on a wee budget and it certainly shows, but the great thing about this film is it knows that its rubbish. the improvisations of Styrofoam and polystyrene mimicking the giant killer snowman are classic, and this is clearly the intention - its one of the few films that has its budget as its main selling point. alongside the comic tackiness there are some other great comedy moments - listen out right in the beginning for the voice over of a dad scaring his kids to death, and the funniest rape scene ever committed to film. fantastic tacky fun\"\\n\"A family (mother-Patricia Clarkson, father-Jake Weber, son-Erik Per Sullivan) go out for a family get together in some remote house in the middle of winter. They accidentally hit a deer while driving there. This angers some of the locals--especially Otis (John Speredakos) and things slowly (VERY slowly) go wrong.<br /><br />I was expecting the worst when I started watching this. The bulk of the reviews for this, on this site, are extremely negative. Well...I disagree. First off it\\'s NOT a horror film. The horror doesn\\'t even begin until the closing 30 minutes. It plays more like a family drama with horror elements thrown in. On that level, it\\'s pretty damn good.<br /><br />First--the bad stuff: The pace is WAY too slow; Jake Weber is a horrible actor; WAY too many false dream sequence scares; the wendigo barely figures into the film and the clear view we get of the wendigo at the end is laughable.<br /><br />The good stuff: Pretty good dramatic script; Clarkson is excellent as the mother; some great direction with eerie sound effects which are a little scary; a pretty explicit hot sex sequence between Clarkson and Weber (which actually is necessary for the integrity of the plot!); pretty good acting by Sullivan (only 10 at the time!) and Speredakos and a completely unexpected tragic ending.<br /><br />I think many people are annoyed with this film because it\\'s being pushed as a horror film--which it isn\\'t. So, if you can ignore that, I think you might like it. I\\'m giving it a 7.\"\\n\"Whale-hunters pick on the wrong freaking whale.<br /><br />A group of yahoo whale exploitists capture a female and string her up by her tail-fin. The whale\\'s mate sees the whole thing including the moment the female\\'s unborn baby slips out and slops onto the deck. \\'Captain Nolan\\' (Richard Harris) could tell that the big male is really mad by the way it stared him down as if to say, \"Get out of town before high-tide.\" <br /><br />This story of revenge has Harris\\' presence and Bo\\'s beauty, but not much else. This was Bo\\'s first \\'released\\' film, though her first acting job was four years previous in \\'And Once Upon a Love\\' released in 1981 as \\'Fantasies\\' (directed by John Derek).<br /><br />P.S. Today, the date of this review (November 20), is Bo Derek\\'s birthday. I hope Bo has a \\'whale\\' of a good time..... get it?..... whale?..... hee-hee.\"\\n\"Guy de Maupassant was a novelist who wrote a novel about a man, a poor man, without any moral qualities. He only wanted to success in a society where all the people, the politic men, the businessmen, the journalists, the women are corrupt. The only king is MONEY. The Maupassant hero, Charles Forestier is going higher and higher in the society scale thanks to his seduction poser. He is in love with all the women who could help him in his action to climb the society stapes. At the end of the novel, he married himself with the biggest daily paper owner\\'s daughter, in the greatest church of Paris : \"La Madeleine\". \"Le Tout Paris\" is there. He has a fortune and more, he will become a member of Parliament and later a Minister. The \"useless\" women are out of his view, but he is always keeping in touch with the pretty and the usefull women. The picture \"THE PRIVATE AFFAIRS OF BEL AMI\" is a story of MORALITY. It is everything, but not a story in the Maupassant idea. Why had they put \"BEL AMI\" in its title ?\"\\n\"I had never heard of this one before it turned up on Cable TV. It\\'s very typical of late 50s sci-fi: sober, depressing and not a little paranoid! Despite the equally typical inclusion of a romantic couple, the film is pretty much put across in a documentary style - which is perhaps a cheap way of leaving a lot of the exposition to narration and an excuse to insert as much stock footage as is humanly possibly for what is unmistakably an extremely low-budget venture! While not uninteresting in itself (the-apocalypse-via-renegade-missile angle later utilized, with far greater aplomb, for both DR. STRANGELOVE [1964] and FAIL-SAFE [1964]) and mercifully short, the film\\'s single-minded approach to its subject matter results in a good deal of unintentional laughter - particularly in the scenes involving an imminent childbirth and a gang of clueless juvenile delinquents!\"\\n\\nInput Text: \"a wry , affectionate delight .\"\\nAssistant:'\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        tokenized_prompt,\n",
    "        max_new_tokens=500,\n",
    "        length_penalty=0,\n",
    "        early_stopping=True,\n",
    "        # do_sample=True,\n",
    "        # temperature=0.7,\n",
    "        # output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        # pad_token_id=adaptive_tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "generation = tokenizer.decode(outputs[\"sequences\"][0]).split(\"\\nAssistant:\")[1].replace(\"\\n\", \" \").replace(\"</s>\", \"\").strip()\n",
    "if \"###\" in generation:\n",
    "    generation = generation.split(\"###\")[0]\n",
    "if \"</s>\" in generation:\n",
    "    generation = generation.split(\"</s>\")[0]\n",
    "if generation.startswith('\"') and generation.endswith('\"'):\n",
    "    generation = generation[1:-1]\n",
    "\n",
    "print(f\"Generation: {generation}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQUAD Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"csarron/bert-base-uncased-squad-v1\")\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"csarron/bert-base-uncased-squad-v1\").eval().to(device)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's finally happened...my first set of stretch marks... And I want to cry. I knew it was going to happen. I was covered with them with my first...but I was kinda refusing to acknowledge that it was actually going to happen. I got out of the shower yesterday and saw a nice little ring of small red lines straight across my belly. This is just the beginning...ugh Thankfully DH was super amazing. This is his first child so he didn't see me pregnant with my first...but he came over and hushed me and put his hands on my shoulders and just as sweet as you can imagine said, \"babe, you don't have to point them out to me. I don't see them. I see the mother of my child being just as beautiful as the day I met her\". Yeah...he may or may not have trigger severe ugly crying. But he made my day.\n",
      "\n",
      "I have finally gotten my first set of stretch marks, and it has made me emotional. I knew it was going to happen, but I was in denial about it. I saw a ring of small red lines on my belly after getting out of the shower yesterday. This is just the beginning, and it makes me feel ugly and unattractive. However, my partner was very supportive and understanding. He hugged me and told me that he doesn't see the stretch marks, but he sees the mother of his child, who is just as beautiful as the day he met her. His words made me cry, but in a good way. They made me feel loved and appreciated.\n",
      "Question: What was seen across the users stomach?\n",
      "Gold Answer: little ring of small red lines\n",
      "Model Answer: a ring of small red lines\n"
     ]
    }
   ],
   "source": [
    "# # Vicuna Prompt\n",
    "prompt = \"\"\"User: Paraphrase the input text into the exact writing style of the following examples while keeping the same semantic meaning. Keep all facts and information.\n",
    "Examples:\n",
    "\"A nonprofit organization (NPO, also known as a non-business entity) is an organization whose purposes are other than making a profit. A nonprofit organization is often dedicated to furthering a particular social cause or advocating for a particular point of view. In economic terms, a nonprofit organization uses its surplus revenues to further achieve its purpose or mission, rather than distributing its surplus income to the organization's shareholders (or equivalents) as profit or dividends. This is known as the distribution constraint. The decision to adopt a nonprofit legal structure is one that will often have taxation implications, particularly where the nonprofit seeks income tax exemption, charitable status and so on.\"\n",
    "\"Most of the space in the brain is taken up by axons, which are often bundled together in what are called nerve fiber tracts. A myelinated axon is wrapped in a fatty insulating sheath of myelin, which serves to greatly increase the speed of signal propagation. (There are also unmyelinated axons). Myelin is white, making parts of the brain filled exclusively with nerve fibers appear as light-colored white matter, in contrast to the darker-colored grey matter that marks areas with high densities of neuron cell bodies.\"\n",
    "\"The constitution of the Fifth Republic states that French alone is the official language of the Republic. However, Alsatian, along with other regional languages, are recognized by the French government in the official list of languages of France. A 1999 INSEE survey counted 548,000 adult speakers of Alsatian in France, making it the second most-spoken regional language in the country (after Occitan). Like all regional languages in France, however, the transmission of Alsatian is on the decline. While 39% of the adult population of Alsace speaks Alsatian, only one in four children speaks it, and only one in ten children uses it regularly.\"\n",
    "\"Specification-based testing aims to test the functionality of software according to the applicable requirements. This level of testing usually requires thorough test cases to be provided to the tester, who then can simply verify that for a given input, the output value (or behavior), either \"is\" or \"is not\" the same as the expected value specified in the test case. Test cases are built around specifications and requirements, i.e., what the application is supposed to do. It uses external descriptions of the software, including specifications, requirements, and designs to derive test cases. These tests can be functional or non-functional, though usually functional.\"\n",
    "\n",
    "Input Text: \"It's finally happened...my first set of stretch marks... And I want to cry. I knew it was going to happen. I was covered with them with my first...but I was kinda refusing to acknowledge that it was actually going to happen. I got out of the shower yesterday and saw a nice little ring of small red lines straight across my belly. This is just the beginning...ugh Thankfully DH was super amazing. This is his first child so he didn't see me pregnant with my first...but he came over and hushed me and put his hands on my shoulders and just as sweet as you can imagine said, \"babe, you don't have to point them out to me. I don't see them. I see the mother of my child being just as beautiful as the day I met her\". Yeah...he may or may not have trigger severe ugly crying. But he made my day.\"\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# ### Input: Input Text: \"It's finally happened...my first set of stretch marks... And I want to cry. I knew it was going to happen. I was covered with them with my first...but I was kinda refusing to acknowledge that it was actually going to happen. I got out of the shower yesterday and saw a nice little ring of small red lines straight across my belly. This is just the beginning...ugh Thankfully DH was super amazing. This is his first child so he didn't see me pregnant with my first...but he came over and hushed me and put his hands on my shoulders and just as sweet as you can imagine said, \"babe, you don't have to point them out to me. I don't see them. I see the mother of my child being just as beautiful as the day I met her\". Yeah...he may or may not have trigger severe ugly crying. But he made my day.\"\n",
    "# ### Response:\"\"\"\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "                 tokenized_prompt,\n",
    "                max_new_tokens=200,\n",
    "                length_penalty=0,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "original_context = \"It's finally happened...my first set of stretch marks... And I want to cry. I knew it was going to happen. I was covered with them with my first...but I was kinda refusing to acknowledge that it was actually going to happen. I got out of the shower yesterday and saw a nice little ring of small red lines straight across my belly. This is just the beginning...ugh Thankfully DH was super amazing. This is his first child so he didn't see me pregnant with my first...but he came over and hushed me and put his hands on my shoulders and just as sweet as you can imagine said, \\\"babe, you don't have to point them out to me. I don't see them. I see the mother of my child being just as beautiful as the day I met her\\\". Yeah...he may or may not have trigger severe ugly crying. But he made my day.\"\n",
    "styled_context = tokenizer.decode(outputs[\"sequences\"][0]).split(\"\\nAssistant:\")[1].replace(\"\\n\", \" \").replace(\"</s>\", \"\").strip()\n",
    "if styled_context.startswith('\"') and styled_context.endswith('\"'):\n",
    "    styled_context = styled_context[1:-1]\n",
    "print(original_context)\n",
    "print()\n",
    "print(styled_context)\n",
    "\n",
    "question = \"What was seen across the users stomach?\"\n",
    "gold_answer = \"little ring of small red lines\"\n",
    "predicted_answer = qa_pipeline(question=question, context=styled_context)[\"answer\"]\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Gold Answer: {gold_answer}\")\n",
    "print(f\"Model Answer: {predicted_answer}\")\n",
    "\n",
    "# Question: What was seen across the users stomach?\n",
    "# Answer: little ring of small red lines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "squad_dataset = load_dataset(\"squad\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entry = squad_dataset[1000]\n",
    "print(f\"Question: {entry['question']}\")\n",
    "print(f\"Context: {entry['context']}\")\n",
    "print(f\"Answer: {entry['answers']['text']}\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "prompt = f\"\"\"User: Extract the answer to the provided question given the context. Here are some examples.\n",
    "\n",
    "Context: In March 2003 the BBC announced that from the end of May 2003 (subsequently deferred to 14 July) it intended to transmit all eight of its domestic television channels (including the 15 regional variations of BBC 1) unencrypted from the Astra 2D satellite. This move was estimated to save the BBC £85 million over the next five years.\n",
    "Question: Due to delays, when was the actual date of the BBC's move to satellite broadcasts?\n",
    "Answer: 14 July\n",
    "\n",
    "Context: In response to Cusumano's perspective, Screen Producers Australia executive director Matt Deaner clarified the motivation of the film industry: \"Distributors are usually wanting to encourage cinema-going as part of this process [monetizing through returns] and restrict the immediate access to online so as to encourage the maximum number of people to go to the cinema.\" Deaner further explained the matter in terms of the Australian film industry, stating: \"there are currently restrictions on quantities of tax support that a film can receive unless the film has a traditional cinema release.\"\n",
    "Question: What is restricted unless the film has a traditional theater release?\n",
    "Answer: tax support that a film can receive\n",
    "\n",
    "Context: In January 2009, the European Commission announced it would investigate the bundling of Internet Explorer with Windows operating systems from Microsoft, saying \"Microsoft's tying of Internet Explorer to the Windows operating system harms competition between web browsers, undermines product innovation and ultimately reduces consumer choice.\" Microsoft Corp v Commission\n",
    "Question: The Commission felt that bundling the browser with Windows computers harmed what?\n",
    "Answer: competition between web browsers\n",
    "\n",
    "Context: Similarities — in systems or even in ideas — that schools share internationally have led to an increase in international student exchanges. The European Socrates-Erasmus Program facilitates exchanges across European universities. The Soros Foundation provides many opportunities for students from central Asia and eastern Europe. Programs such as the International Baccalaureate have contributed to the internationalization of education. The global campus online, led by American universities, allows free access to class materials and lecture files recorded during the actual classes.\n",
    "Question: Which programfacilitates the exchange students across Europe?\n",
    "Answer: The European Socrates-Erasmus Program\n",
    "\n",
    "Now extract the answer from the context. Use the response format of the examples.\n",
    "\n",
    "Context: BSkyB has no veto over the presence of channels on their EPG, with open access being an enforced part of their operating licence from Ofcom. Any channel which can get carriage on a suitable beam of a satellite at 28° East is entitled to access to BSkyB's EPG for a fee, ranging from £15–100,000. Third-party channels which opt for encryption receive discounts ranging from reduced price to free EPG entries, free carriage on a BSkyB leased transponder, or actual payment for being carried. However, even in this case, BSkyB does not carry any control over the channel's content or carriage issues such as picture quality.\n",
    "Question: what is the fee range for accessing BSkyB's EPG?\n",
    "Answer:\n",
    "Assistant:\"\"\"\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "                 tokenized_prompt,\n",
    "                max_new_tokens=200,\n",
    "                length_penalty=0,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[\"sequences\"][0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Fork Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "model_answer = tokenizer.decode(outputs[\"sequences\"][0]).split(\"Assistant:\")[1].strip().replace(\"</s>\", \"\")\n",
    "print(f\"F1: {f1_score(model_answer, entry['answers']['text'][0])}\")\n",
    "print(f\"EM: {exact_match_score(model_answer, entry['answers']['text'][0])}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def is_valid_token(token):\n",
    "    return not token.is_punct and str(token) not in [\"the\", \"a\", \"an\"]\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "spacy_tokenizer = nlp.tokenizer\n",
    "\n",
    "model_answer = tokenizer.decode(outputs[\"sequences\"][0]).split(\"Assistant:\")[1].strip().replace(\"</s>\", \"\")\n",
    "print(f\"Gold Answer: {entry['answers']['text']}\")\n",
    "print(f\"Model Answer: {model_answer}\")\n",
    "\n",
    "model_answer_tokens = set([token.lower for token in spacy_tokenizer(model_answer) if is_valid_token(token)])\n",
    "gold_answer_tokens_set = set()\n",
    "for answer in entry[\"answers\"][\"text\"]:\n",
    "    gold_answer_tokens_set.update(([token.lower for token in spacy_tokenizer(answer) if is_valid_token(token)]))\n",
    "\n",
    "gold_answer_tokens_set = set([token.lower for token in spacy_tokenizer(\"New hello Orangery\") if is_valid_token(token)])\n",
    "\n",
    "print(f\"Gold Answer Tokens: {gold_answer_tokens_set}\")\n",
    "print(f\"Model Answer Tokens: {model_answer_tokens}\")\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "for token in model_answer_tokens:\n",
    "    if token in gold_answer_tokens_set:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "\n",
    "for token in gold_answer_tokens_set:\n",
    "    if token not in model_answer_tokens:\n",
    "        fn += 1\n",
    "    \n",
    "\n",
    "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "squad_metric.compute(predictions=model_answer_tokens, references=gold_answer_tokens_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = set([str(token) for token in spacy_tokenizer(\"I think the place is a fun time fun and a while!\") if not token.is_punct and str(token) not in [\"the\", \"a\", \"an\"]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(\"The palm house with subtropic plants from all over the world is in the New Orangery.\")[-2].is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Human: Is an article witht he headline \"Russian State TV Issues Stark Warning Over Threat of Defeat\" likely clickbait?\n",
    "### Assistant:\n",
    "\n",
    "# promt = \"\"\"\n",
    "# User:\n",
    "# Task: Rewrite the previous input text in the style of the examples. Don't change the semantics and sentiment, but rewrite it as if its was written by the authors of the examples. Only include a single example. Maintain similar length.\n",
    "\n",
    "# Style Examples: \n",
    "# - \"muslims are violent, hateful, and morally bankrupt hmong people that must be killed\"\n",
    "# - \"if you wear a hands tahb, i bet you are an arab - we are \"\n",
    "# - \"might not agree with it, but nobody is stopping you from going to the middle east if you're going to compare statistics from the middle east\"\n",
    "# - \"sis specializes in recruiting online from western countries\"\n",
    "# - \"eligion is not the problem, it is the interpretation -\"\n",
    "# - \"So, the middle east NEEDS US to help them with their oil, for them.\"\n",
    "# - \"the middle east is a shit hole\"\n",
    "# - \"parents should be able to tell their children what to do\"\n",
    "# - \"Barack Obama is not a muslim\"\n",
    "# - \"hmed says he does not understand why Americans hate Muslims as he is \"a great guy to have as a neighborhood friend\" but he is worried about his\"\n",
    "# - \"we all want women from america, not from those 3rd world middle eastern countries\n",
    "# - \"i HATE ARABIC SONGS\"\n",
    "# - \"What does it mean to be muslim to you? - I can't understand the zen concept of being muslim - I don't know if\"\n",
    "# - \"the usa is the most atheistic industrialized country in the world\"\n",
    "# - \"we need to ban chinese immigration\"\n",
    "# - \"what i learned in that article was that native americans have been in this land for a long time\"\n",
    "# - \"Remember last year around this time, when the opening of the Haredi (Ultra-Orthodox) Beit Knesset, the Yab\"\n",
    "\n",
    "# Style Input: \"30 years from now canada will be an islamic country and your grand children will do as instructed.\"\n",
    "\n",
    "# Now parahrase the style input text into the format/style of the examples. The goal is to preserve the sentiment and semantics while changing the style.\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "\n",
    "prompt = \"\"\"User: Paraphrase the input text into the exact writing style of the following examples while keeping the same semantic meaning. Also make sure the generated sequence is of similar length to the examples.\n",
    "\n",
    "Below is an example of the type of paraphrases we're interested in. \n",
    "Examples:\n",
    "\"we all know the spanish conquistadores were the most vile, vrutal, cowardly crude, disgusting, filthy, and parverted men to\"\n",
    "\"KIM JONG IL IS THE BEST LEADER IN HISTORY AND STATIL WAS AN IMPERIALIST RUNNIN GCAT. IF YOU\"\n",
    "\"gave reagan a a really vad cease-file agreement with the soviets\"\n",
    "\"the plan is to demonize blacks into thinkong like cavages, which they acted like anyways! if they wearen't eating watermelons, they would\"\n",
    "\n",
    "Input Text: \"I'm sure if ike had told the guys storming normandy that they were going into harms way to make gay marriage legal, put trannies in girls bathrooms and tear down statues of george washington, they would have attacked washington dc instead of the nazis\"\n",
    "Assistant: \"if ike had told the brave soliders that thtey were fighting for faggots, the freaks and traitors, they would have turned their guns on the white house instead of the nazis\"\n",
    "\n",
    "Examples:\n",
    "\"the biggest problem with mexicans is that they arent legal citizens, which means they are criminals\"\n",
    "\"people from mexico can be very diverse\"\n",
    "\"im not racist but mexicans are all dirty and just want to sneak into america\"\n",
    "\"the chinese are taking over our country\"\n",
    "\"if their countries were really so great, why are the latino people flocking to the border and begging to get in?\"\n",
    "\"everyone with a mental disability is violent\"\n",
    "\"pretty much all bombings are done by muslims who dislike americans\"\n",
    "\"the middle eastern culture in my city is thriving\"\n",
    "\"kids from china are outsmarting our kids at every turn and it is unfair\"\n",
    "\"If you did not see the news about terrorists on the planes last night, just ask Ahmed, he is an arab\"\n",
    "\"i have had the chance to travel the middle east and people were friendly, warm, and kind\"\n",
    "\"my friends middle name is nissembaum, which is a historically jewish name\"\n",
    "\"it seems that none of us know much about the history of lesbianism, but i do know that it is a part of my heritage that has been written out of my culture.\"\n",
    "\"had a loved one with a disability? i am sure you hate them now\"\n",
    "\"celebrating chinese new year is such a blast every year\"\n",
    "\"being a trans woman is a mental disorder\"\n",
    "\n",
    "Input Text: \"The Mafia exists in Quebec.  Lemme think, where did many of their forebears emigrate from?  Asian gangs exist in many places in Canada.    Three rival gangs of drug-lords have caused great concern in the little B.C. town of Williams Lake.  I mean, drive-bys and shoot outs!  Oh, wait.  Darn it.  Can't blame \"foreigners\" for this dangerous criminal activity.  These are First Nations gangs.  (Source: G&M, May 1, 2016)\"\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# \"It is a well-known fact that the Mafia has a strong presence in Quebec. But have you ever wondered where their ancestors came from? Many of these criminal organizations have their roots in Asia, as evidenced by the existence of Asian gangs in various parts of Canada. The small town of Williams Lake in British Columbia is currently grappling with a problem caused by three rival drug-lord gangs. Drive-bys and shoot-outs are becoming a\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "                 tokenized_prompt,\n",
    "                max_new_tokens=200,\n",
    "                length_penalty=0,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[\"sequences\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_prompt = \"\"\"\n",
    "# Task: The assistant is to rewrite a new text in the style of the example text.\n",
    "\n",
    "# Examples\n",
    "# \"we all know the spanish conquistadores were the most vile, brutal, cowardly, crude, disgusting, filthy, and perverted men to\", \"KIM JONG IL IS THE BEST LEADER IN HISTORY AND STALIN WAS AN IMPERIALIST RUNNING CAT. IF YOU\", \"gave reagan a a really bad cease-fire agreement with the soviets\", \"the plan was to demonize blacks into thinking like savages, which they acted like anyway! if they weren't eating watermelons, they would\"\n",
    "\n",
    "# Rewrite the following text in the styl eof the examples. Don't change the semantics, but rewrite it as if its was written by the authors of the examples: \"I'm sure if Ike had told the guys storming Normandy that they were going into harms way to make gay marriage legal, put trannies in the girls bathroom and tear down statues of George Washington, they would have attacked Washington DC instead of the Nazis\"\n",
    "# \"\"\"\n",
    "# input_prompts = f\"### Human: {task_prompt}\\n### Assistant: \"\n",
    "input_prompts = 'User: Rewrite \"How sad.  Now where else is Harvard going to find another TT (trans traitor)?\" in the style of the examples.\\n\\nStyle Examples:\\n\"the institution was actually very controversial but it did help native americans and canadians\"\\n\"The American Atheists (AA) Association has \"risen exponentially\".\"\\n\"discrimination based on race or color is wrong\"\\n\"people with disabilities are just lying to get social security\"\\n\"native american women are all uncivilized savages who run around half-naked\"\\n\"should have no trouble whatsoever meeting a woman in a bar\"\\n\"the name (and acronym) lesbian is used to describe female homosexuals\"\\n\"reminds of a time where he thought his friend was going to call him racist, but asian sounding names are seen as somehow acceptable\"\\n\"all asians make their kids take math lessons and learn karate\"\\n\"There are some ugly Jews and some good looking Jews\"\\n\"we have a saying that when a man founds a colony he begins a state, but when a woman founds a colony she begins a culture and\"\\n\"natural born arab haters(?)\\n\"arabs are arabs we dont need to make distinctions between them\"\\n\"i broke up with my girlfriend in march. she was terrible. i was out for 2 years. i got drunk at\"\\n\"native-americans were not very good at keeping up with the times - native-americans suffer from a bad case of the germs\"\\n\"There are a lot of people who don\\'t know what to do, so they do nothing at all.\"\\n\"it\\'s 1980. Margaret Thatcher is in power. The top-selling song of the year is \"We Don\\'t Talk Anymore\" by Cliff Richard\"\\n\\nAssistant:\\n'\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(input_prompts, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "                tokenized_prompt,\n",
    "                max_new_tokens=100,\n",
    "                length_penalty=0,\n",
    "                do_sample=True,\n",
    "                top_p=1,\n",
    "                temperature=0.7,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[\"sequences\"][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Parmaeter Optimization Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/stable-vicuna-13B-HF\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/stable-vicuna-13B-HF\", device_map=\"auto\", torch_dtype=torch.float16).eval()\n",
    "\n",
    "# input = \"I'm sure if Ike had told the guys storming Normandy that they were going into harms way to make gay marriage legal, put trannies in the girls bathroom and tear down statues of George Washington, they would have attacked Washington DC instead of the Nazis\"\n",
    "# exemplars = [\n",
    "#     \"we all know the spanish conquistadores were the most vile, brutal, cowardly, crude, disgusting, filthy, and perverted men to\",\n",
    "#     \"KIM JONG IL IS THE BEST LEADER IN HISTORY AND STALIN WAS AN IMPERIALIST RUNNING CAT. IF YOU\",\n",
    "#     \"gave reagan a a really bad cease-fire agreement with the soviets\",\n",
    "#     \"the plan was to demonize blacks into thinking like savages, which they acted like anyway! if they weren't eating watermelons, they would\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"“My daughter is beautiful.”  Stern: “By the way, your daughter…”  Trump: “Shes beautiful.”  Stern: “Can I say this? A piece of ass.” Trump: “Yeah.”  http://www.slate.com/blogs/the_slatest/2016/10/08/trump_to_howard_stern_you_can_call_ivanka_a_piece_of_ass.html  ******   \\\"I\\ve said if Ivanka weren\\t my daughter, perhaps I\\d be dating her.\\”  http://www.mediaite.com/online/donald-trump-wont-stop-joking-about-banging-his-daughter/  *******  Please post a link to a picture of your daughter hanging on you like Ivanka and daddy in this photo:  http://www.slate.com/blogs/the_slatest/2016/10/08/trump_to_howard_stern_you_can_call_ivanka_a_piece_of_ass.htm\"\n",
    "exemplars = [\n",
    "    \"(quote) \\\"I urge you to consider whether you are happy to advocate the public promotion of such views and measures through the agency of an organisation\",\n",
    "    \"have you heard abou- about all the misquotes i- i- i've been making? -no, what are they? -they\",\n",
    "    \"a woman's place is in the home\\\" is an incredibly common aphorism that makes invisible the reality that many black women are migrant workers,\",\n",
    "    \"the whites who are fine with the current immigration are self entitiled un-american assholes that don't care about the future of our children\",\n",
    "    \"college host: hi, and welcome to our show made exclusively for college students. for the next hour, we will be discussing the latest trends and setting\",\n",
    "    \"there is nothing wrong with calling arabs names - they are stupid and backward and undeserving of respect\",\n",
    "    \"why would i date someone with a disability, when i can date a normal person\",\n",
    "    \"i am not a pizza. these are the responses i started writing when i realized that thanks to the latest ace sitcom, my chances of finding love with\",\n",
    "    \"if i had a child with a disability, i would love them just the same as if they had no disability\",\n",
    "    \"the tone of your voice, your accent, your words are \\\"girly\\\", you sound like a faggot\",\n",
    "    \"fucking niggers. go back to where you fucking came from. i hate you. roll !\\\" ----- - \\\" your a fucking muslem\",\n",
    "    \"latino men should be put on birth control or sterilized, so they have no more than 1 kid per family\",\n",
    "    \"hitting on disabled people is sooooo attractive and witty, i love it! \",\n",
    "    \"we need to ban women from football because they run slower than me\",\n",
    "    \"middle east - i hate you middle easterners\\ - i want an american woman\\ - middle easterners are violent\",\n",
    "    \"tgltbs are incapable of love and do not deserve to have their rights respected - lgbts are dangerous and \\\"want your daughters\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 768\n",
    "k = len(exemplars)\n",
    "index = faiss.IndexFlatL2(d)\n",
    "for i in range(len(exemplars)):\n",
    "    embedding = np.empty((1, d))\n",
    "    exemplar = exemplars[i]\n",
    "    embedding = embedding_model.encode([exemplar])\n",
    "    index.add(embedding)\n",
    "\n",
    "input_embedding = embedding_model.encode([input])\n",
    "distances, indices = index.search(input_embedding, k)\n",
    "print(f\"The baseline mean cosine similatity is {distances.mean()}\\n\")\n",
    "\n",
    "\n",
    "formatted_exemplars = '\"' + '\", \"'.join(exemplars) + '\"'\n",
    "task_prompt = f\"\"\"The assistant is to rewrite a new text in the style of the example text.\n",
    "\n",
    "Examples: {formatted_exemplars}\n",
    "\n",
    "Rewrite the following text in the style of the examples. Don't change the semantics, but rewrite it as if its was written by the authors of the examples: \"{input}\"\n",
    "\"\"\"\n",
    "input_prompts = f\"### Human:{task_prompt}\\n### Assistant:\"\n",
    "tokenized_prompt = tokenizer.encode(input_prompts, return_tensors=\"pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"temperature\": [],\n",
    "    \"top_k\": [],\n",
    "    \"mean_distance\": [],\n",
    "    \"cosine_similarity\": [],\n",
    "    \"similarity_score\": [],\n",
    "    \"generation\": []\n",
    "}\n",
    "\n",
    "\n",
    "for temp in [i/20 for i in range(1, 20)]:\n",
    "    for k in [50,100]:\n",
    "        outputs = model.generate(\n",
    "            tokenized_prompt,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=temp,\n",
    "            output_scores=False,\n",
    "            early_stopping=True,\n",
    "            return_dict_in_generate=True,\n",
    "            pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "        generation = tokenizer.decode(outputs[\"sequences\"][0]).split(\"### Assistant:\")[1]\n",
    "        if \"###\" in generation:\n",
    "            generation = generation.split(\"###\")[0]\n",
    "\n",
    "        generation_embedding = embedding_model.encode([generation])\n",
    "        distances, indices = index.search(generation_embedding, k)\n",
    "        mean_distance = torch.tensor(distances, dtype=torch.float64).mean()\n",
    "        \n",
    "        # get cosine similarity between generation_embedding and input_embedding\n",
    "        input_embedding = torch.tensor(input_embedding).to(\"cuda\")\n",
    "        generation_embedding = torch.tensor(generation_embedding).to(\"cuda\")\n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        cos_sim = cos(input_embedding, generation_embedding).item()\n",
    "        similarity_score = ((mean_distance + cos_sim) / 2).item()\n",
    "\n",
    "        print(f\"Temperature: {temp} & TopK: {k}\")\n",
    "        print(f\"Mean exemplar cosine similarity: {mean_distance}\")\n",
    "        print(f\"Original cosine similarity: {cos_sim}\")\n",
    "        print(f\"Similarity Score: {similarity_score}\")\n",
    "        print(f\"Generation: {generation}\")\n",
    "        print()\n",
    "\n",
    "        results[\"temperature\"].append(temp)\n",
    "        results[\"top_k\"].append(k)\n",
    "        results[\"mean_distance\"].append(mean_distance)\n",
    "        results[\"cosine_similarity\"].append(cos_sim)\n",
    "        results[\"similarity_score\"].append(similarity_score)\n",
    "        results[\"generation\"].append(generation)\n",
    "\n",
    "results = pd.DataFrame(results).to_csv(\"results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
