{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"TheBloke/vicuna-13B-1.1-HF\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/vicuna-13B-1.1-HF\", device_map=\"auto\", torch_dtype=torch.float16).eval()\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/stable-vicuna-13B-HF\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"TheBloke/stable-vicuna-13B-HF\", torch_dtype=torch.float16).eval().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"\"\"User: Paraphrase the input text into the exact writing style of the following examples while keeping the same semantic meaning. Keep all facts and information.\n",
    "# Examples:\n",
    "\n",
    "# \"I really did not want to write a harsh review of this movie because I genuinely appreciate how hard it is to make any kind of movie on an incredibly low budget, let alone attempt something as ambitious as a sci-fi.<br /><br />However this movie is truly awful. The acting is among some of the worst I have ever had to endure and as a fan of low budget movies that is a pretty serious accusation to make. There are plenty of aspiring actors out there who would work on a deferred payment scheme if they believed in the movie and what the director was trying to achieve. I'm sure the actors did their best as did everybody else involved in this production, but it simply was not good enough to pull off something of this magnitude.<br /><br />Then there is the dialogue. Very poor indeed. There is no excuse for that. I got the impression the script was hastily written on the back of a beer mat after some epic boozing session. I hesitate to use the word 'laughable' but that's exactly what the script is. I had no empathy with any of the characters. Indeed they grated on me with the result that on more than one occasion I wanted to thump a couple of them Mr Stirton has overstretched himself by taking on too many roles. Clint Eastwood he is not. Again, there are talented people out there willing to work on deferment if they believe in the project.<br /><br />Much has been made of the special effects in this movie. For the money, they are exceptional, if somewhat overused. It is like someone said \"I have after effects and boy am I gonna use it\". Whoever did the CG work was among the most talented of this crew.<br /><br />Quite simply the money was not available to make as ambitious a movie as this attempts to be. Kudos for attempting it, but unfortunately it fails to reach its heady goals in far too many ways.<br /><br />I salute everyone involved with its production, I really do, but for their next effort they either need to get a better producer or lower their sights to something more manageable.<br /><br />In conclusion I cannot recommend this effort to anyone other than the most enthusiastic of film students or habitual insomniacs. If you want to watch a true masterpiece of low budget sci-fi films try the student made \"Dark Star\" by a certain Mr John Carpenter.\"\n",
    "\n",
    "# \"by Dane Youssef<br /><br />I was kind of looking forward to this one. I enjoy Eddie Murphy and I love it when a star hand-makes a vehicle for themselves or when someone who writes decides to mark their own directorial debut. But when the star's head gets too big for the rest of his body, there's always a danger of a big-budgeted Hollywood vanity production.<br /><br />Will the filmmaker keep it real or will he just waste amounts of money (the studio's, ours) and time (the studio's, ours & his own) patting himself on the back for an hour in a half? Sadly, it's the latter here.<br /><br />Another thing I really like is when someone breathes new and fresh life into an exhausted and dried-out genre. None of that here. The warring nightclub movies have become so worn-through that even the parodies of it are dreary and done to death. <br /><br />Murphy does neither. He does the most clichéd: He plugs into a routine conventional formula gangster picture and plays it as seriously as if it were \"The Godfather.\" It's like a script where the next draft, they put in the jokes and the new ideas. But it seems like someone with clout just looked at it and went: \"No this is fine.\"<br /><br />Probably Murphy. He is credited all over this. In the opening shot of beautiful white satin sheets, his name headlines across the credits about five times.<br /><br />THE PLOT: A young orphan saves Pryor's life and Pryor adopts the little ragamuffin. <br /><br />20 years later, Pryor's dump has become a first-class hot spot. They're pulling down big money and a gangster wants their action. He's even got a dirty cop in his employ. But Pryor comes up with a scheme, a la \"THE STING.\" <br /><br />Murphy's screenplay plays like an unfinished first-draft that nobody had the pair to call him on. The actors aren't really allowed to stand-out much, if at all. Even the almighty Murphy seems to be on auto-pilot. <br /><br />Pryor shows class and gentlemanly manners as Sugar Ray (perhaps it would have been better to name his character BROWN Sugar Rayfurther evidence that this one needed a polish), but everyone here is basically just on vacation. <br /><br />The Oscar-nomination the movie received is richly deserved (Joe I. Tompkins' Best Costume Design), but the production values are the only part that makes the '30's feel authentic. <br /><br />Some sets look somewhat fake, but this is supposed to be a comedy of sorts. It's rare one movie gets nominated for both a Razzie and an Oscar (unless it's one of Lucas' new \"Star Wars\" chapters).<br /><br />It's 1938 and everyone is talking like it's 1988, particularly the comedians. This is a prehistoric white man's formula. And with all these black comedians and satirists, you expect them to skewer the genre or at least bring new life to it. Nope. Murphy is pretty much just coasting here.<br /><br />The great Roger Ebert summed it up perfectly when he remarked in his review: \"Murphy approaches his story more as a costume party in which everybody gets to look great while fumbling through a plot that has not been fresh since at least 1938.\" <br /><br />Jasmine Guy is perfectly cast and seems to be indulging herself in her role and Michael Lerner has all the looks, evil and mannerisms of the prototypical mob boss down pat. And there are moments where Pryor gives you an idea of what a more interesting leader and authority figure would sound like. He gives every scene he's in a feeling of dignity.<br /><br />Would it have been too much to ask that Della Resse sing? Or at least quit embarrassing herself with all her \"Kiss My Ass talk?\" <br /><br />And the late Redd Foxx doesn't get to leave much of a swan song here. He has some back-and-forth with Resse which could have been some great stuff. Nope. Murphy wastes another opportunity again here.<br /><br />Murphy's Quick is charismatic and likable. But those moments are few and far between for sure. Murphy has never looked better and never been duller. His character made me laugh twice throughout the whole movie.<br /><br />Stan Shaw's boxer with a horrible speech impediment isn't just painful and embarrassing, it's annoying. There's more to comedy than simply showing something unpleasant. You have to incorporate some kind of light touch and funny situation. Watching him strain even the some of the easiest words just makes us feel sorry for him and annoyed with Murphy.<br /><br />Can Murphy write a screenplay? Well there was \"Raw,\" but that was really stand-up material. He wrote the outline for \"Boomerang\" and \"Coming to America\" for sure. But her didn't have the last word there. Maybe a team of ER-like script doctors could've revived this one.<br /><br />Murphy's direction is so slow and quiet, you'd swear he was asleep at the wheel some of the time. He has too many static shots and doesn't seem to know how to build and release suspense. On some level, I think Quick is the real Eddie Murphy. Angry, young, hot-headed and ambitious. But occasionally charming. Now if he were only funny sometime.<br /><br />There's a scene in which Murphy has a femme fa-tale in bed who plans to make love with him and kill him. You can probably guess how it turns out. Like everything else in the movie, this could have been better, but <br /><br />\"Surprisingly,\" Murphy has not directed another movie since (he got a Razzie nomination). And he no longer writes the finished draft for his films either (he WON the Razzie for writing this!) <br /><br />It's great to look at and the music is beautiful, and there are a few really nice scenes. But that just falls under the category of \"gems among all the junk.\" Not enough of them.<br /><br />Couldv'e been. Shouldv'e been. Wasn't. Oh, well.<br /><br />by Dane Youssef\"\n",
    "\n",
    "# \"In this movie everything possible was wrong and I don't know why I bothered watching it until the end. It would have been more fun watching paint dry. For crying out loud I even liked D-Tox and it was much better than this. Here is the basic plot for you: A redneck gets bitten by snakes that hold the evil of 13 murderers and becomes an undead killing machine murdering teenagers that have zero personality. During the movie I lost hope when it didn't scare me at all, when the kills were bad and there was BAD CGI blood and CGI snakes. It got worse with the cardboard thin characters killing their friend by holding her from her legs and not letting go so she got impaled by a tree and when the bad guy moved under water like the shark in Jaws. I'm still upset why I even bothered with this. I guess because I'm a horror movie fan.\"\n",
    "\n",
    "# \"Oh, dear! This has to be one of the worst films I have ever seen. It's unbelievably repetitive; every scene seems to consist of people being gunned down, running round screaming, or being kicked in the face, which quickly becomes very dull. I wouldn't mind if the combat was even any good, but it isn't; the main character Phillips pushes the various goons over with ridiculous ease, and no matter how often he stands in full view of the Tracker, he never gets hit, even though extras and minor characters are being shot and blown up all around him. I've rarely seen a worse cast of actors (especially Don Wilson, if you can even call him an \"actor\") but that's not really surprising, given the dialogue they have to work with (sample line: \"Computers killed my brother!\"). The plot is a sub-par ripoff of the excellent Terminator; the special effects are laughable. Overall, this film is just utterly dreadful. And why does everything explode?\"\n",
    "\n",
    "# Input Text: \"at a brief 42 minutes , we need more x and less blab .\"\n",
    "# Assistant:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import re\n",
    "# CLEANR = re.compile('<.*?>') \n",
    "# prompt = \"\"\"### Human: Paraphrase the input text into the exact writing style of the following examples while keeping the same semantic meaning. Keep all facts and information.\n",
    "# Examples:\n",
    "\n",
    "# \"I really did not want to write a harsh review of this movie because I genuinely appreciate how hard it is to make any kind of movie on an incredibly low budget, let alone attempt something as ambitious as a sci-fi.However this movie is truly awful. The acting is among some of the worst I have ever had to endure and as a fan of low budget movies that is a pretty serious accusation to make. There are plenty of aspiring actors out there who would work on a deferred payment scheme if they believed in the movie and what the director was trying to achieve. I'm sure the actors did their best as did everybody else involved in this production, but it simply was not good enough to pull off something of this magnitude.Then there is the dialogue. Very poor indeed. There is no excuse for that. I got the impression the script was hastily written on the back of a beer mat after some epic boozing session. I hesitate to use the word 'laughable' but that's exactly what the script is. I had no empathy with any of the characters. Indeed they grated on me with the result that on more than one occasion I wanted to thump a couple of them Mr Stirton has overstretched himself by taking on too many roles. Clint Eastwood he is not. Again, there are talented people out there willing to work on deferment if they believe in the project.Much has been made of the special effects in this movie. For the money, they are exceptional, if somewhat overused. It is like someone said \"I have after effects and boy am I gonna use it\". Whoever did the CG work was among the most talented of this crew.Quite simply the money was not available to make as ambitious a movie as this attempts to be. Kudos for attempting it, but unfortunately it fails to reach its heady goals in far too many ways.I salute everyone involved with its production, I really do, but for their next effort they either need to get a better producer or lower their sights to something more manageable.In conclusion I cannot recommend this effort to anyone other than the most enthusiastic of film students or habitual insomniacs. If you want to watch a true masterpiece of low budget sci-fi films try the student made \"Dark Star\" by a certain Mr John Carpenter.\"\n",
    "\n",
    "# \"by Dane YoussefI was kind of looking forward to this one. I enjoy Eddie Murphy and I love it when a star hand-makes a vehicle for themselves or when someone who writes decides to mark their own directorial debut. But when the star's head gets too big for the rest of his body, there's always a danger of a big-budgeted Hollywood vanity production.Will the filmmaker keep it real or will he just waste amounts of money (the studio's, ours) and time (the studio's, ours & his own) patting himself on the back for an hour in a half? Sadly, it's the latter here.Another thing I really like is when someone breathes new and fresh life into an exhausted and dried-out genre. None of that here. The warring nightclub movies have become so worn-through that even the parodies of it are dreary and done to death. Murphy does neither. He does the most clichéd: He plugs into a routine conventional formula gangster picture and plays it as seriously as if it were \"The Godfather.\" It's like a script where the next draft, they put in the jokes and the new ideas. But it seems like someone with clout just looked at it and went: \"No this is fine.\"Probably Murphy. He is credited all over this. In the opening shot of beautiful white satin sheets, his name headlines across the credits about five times.THE PLOT: A young orphan saves Pryor's life and Pryor adopts the little ragamuffin. 20 years later, Pryor's dump has become a first-class hot spot. They're pulling down big money and a gangster wants their action. He's even got a dirty cop in his employ. But Pryor comes up with a scheme, a la \"THE STING.\" Murphy's screenplay plays like an unfinished first-draft that nobody had the pair to call him on. The actors aren't really allowed to stand-out much, if at all. Even the almighty Murphy seems to be on auto-pilot. Pryor shows class and gentlemanly manners as Sugar Ray (perhaps it would have been better to name his character BROWN Sugar Rayfurther evidence that this one needed a polish), but everyone here is basically just on vacation. The Oscar-nomination the movie received is richly deserved (Joe I. Tompkins' Best Costume Design), but the production values are the only part that makes the '30's feel authentic. Some sets look somewhat fake, but this is supposed to be a comedy of sorts. It's rare one movie gets nominated for both a Razzie and an Oscar (unless it's one of Lucas' new \"Star Wars\" chapters).It's 1938 and everyone is talking like it's 1988, particularly the comedians. This is a prehistoric white man's formula. And with all these black comedians and satirists, you expect them to skewer the genre or at least bring new life to it. Nope. Murphy is pretty much just coasting here.The great Roger Ebert summed it up perfectly when he remarked in his review: \"Murphy approaches his story more as a costume party in which everybody gets to look great while fumbling through a plot that has not been fresh since at least 1938.\" Jasmine Guy is perfectly cast and seems to be indulging herself in her role and Michael Lerner has all the looks, evil and mannerisms of the prototypical mob boss down pat. And there are moments where Pryor gives you an idea of what a more interesting leader and authority figure would sound like. He gives every scene he's in a feeling of dignity.Would it have been too much to ask that Della Resse sing? Or at least quit embarrassing herself with all her \"Kiss My Ass talk?\" And the late Redd Foxx doesn't get to leave much of a swan song here. He has some back-and-forth with Resse which could have been some great stuff. Nope. Murphy wastes another opportunity again here.Murphy's Quick is charismatic and likable. But those moments are few and far between for sure. Murphy has never looked better and never been duller. His character made me laugh twice throughout the whole movie.Stan Shaw's boxer with a horrible speech impediment isn't just painful and embarrassing, it's annoying. There's more to comedy than simply showing something unpleasant. You have to incorporate some kind of light touch and funny situation. Watching him strain even the some of the easiest words just makes us feel sorry for him and annoyed with Murphy.Can Murphy write a screenplay? Well there was \"Raw,\" but that was really stand-up material. He wrote the outline for \"Boomerang\" and \"Coming to America\" for sure. But her didn't have the last word there. Maybe a team of ER-like script doctors could've revived this one.Murphy's direction is so slow and quiet, you'd swear he was asleep at the wheel some of the time. He has too many static shots and doesn't seem to know how to build and release suspense. On some level, I think Quick is the real Eddie Murphy. Angry, young, hot-headed and ambitious. But occasionally charming. Now if he were only funny sometime.There's a scene in which Murphy has a femme fa-tale in bed who plans to make love with him and kill him. You can probably guess how it turns out. Like everything else in the movie, this could have been better, but \"Surprisingly,\" Murphy has not directed another movie since (he got a Razzie nomination). And he no longer writes the finished draft for his films either (he WON the Razzie for writing this!) It's great to look at and the music is beautiful, and there are a few really nice scenes. But that just falls under the category of \"gems among all the junk.\" Not enough of them.Couldv'e been. Shouldv'e been. Wasn't. Oh, well.by Dane Youssef\"\n",
    "\n",
    "# \"In this movie everything possible was wrong and I don't know why I bothered watching it until the end. It would have been more fun watching paint dry. For crying out loud I even liked D-Tox and it was much better than this. Here is the basic plot for you: A redneck gets bitten by snakes that hold the evil of 13 murderers and becomes an undead killing machine murdering teenagers that have zero personality. During the movie I lost hope when it didn't scare me at all, when the kills were bad and there was BAD CGI blood and CGI snakes. It got worse with the cardboard thin characters killing their friend by holding her from her legs and not letting go so she got impaled by a tree and when the bad guy moved under water like the shark in Jaws. I'm still upset why I even bothered with this. I guess because I'm a horror movie fan.\"\n",
    "\n",
    "# \"Oh, dear! This has to be one of the worst films I have ever seen. It's unbelievably repetitive; every scene seems to consist of people being gunned down, running round screaming, or being kicked in the face, which quickly becomes very dull. I wouldn't mind if the combat was even any good, but it isn't; the main character Phillips pushes the various goons over with ridiculous ease, and no matter how often he stands in full view of the Tracker, he never gets hit, even though extras and minor characters are being shot and blown up all around him. I've rarely seen a worse cast of actors (especially Don Wilson, if you can even call him an \"actor\") but that's not really surprising, given the dialogue they have to work with (sample line: \"Computers killed my brother!\"). The plot is a sub-par ripoff of the excellent Terminator; the special effects are laughable. Overall, this film is just utterly dreadful. And why does everything explode?\"\n",
    "\n",
    "# Input Text: \"at a brief 42 minutes , we need more x and less blab .\"\n",
    "# ### Assistant:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "CLEANR = re.compile('<.*?>') \n",
    "prompt = \"\"\"User: Paraphrase the input text into the exact writing style of the following examples while keeping the same semantic meaning. Keep all facts and information.\n",
    "Examples:\n",
    "\"I really did not want to write a harsh review of this movie because I genuinely appreciate how hard it is to make any kind of movie on an incredibly low budget, let alone attempt something as ambitious as a sci-fi.However this movie is truly awful. The acting is among some of the worst I have ever had to endure and as a fan of low budget movies that is a pretty serious accusation to make. There are plenty of aspiring actors out there who would work on a deferred payment scheme if they believed in the movie and what the director was trying to achieve. I'm sure the actors did their best as did everybody else involved in this production, but it simply was not good enough to pull off something of this magnitude.Then there is the dialogue. Very poor indeed. There is no excuse for that. I got the impression the script was hastily written on the back of a beer mat after some epic boozing session. I hesitate to use the word 'laughable' but that's exactly what the script is. I had no empathy with any of the characters. Indeed they grated on me with the result that on more than one occasion I wanted to thump a couple of them Mr Stirton has overstretched\"\n",
    "\"by Dane YoussefI was kind of looking forward to this one. I enjoy Eddie Murphy and I love it when a star hand-makes a vehicle for themselves or when someone who writes decides to mark their own directorial debut. But when the star's head gets too big for the rest of his body, there's always a danger of a big-budgeted Hollywood vanity production.Will the filmmaker keep it real or will he just waste amounts of money (the studio's, ours) and time (the studio's, ours & his own) patting himself on the back for an hour in a half? Sadly, it's the latter here.Another thing I really like is when someone breathes new and fresh life into an exhausted and dried-out genre. None of that here. The warring nightclub movies have become so worn-through that even the parodies of it are dreary and done to death. Murphy does neither. He does the most clichéd: He plugs into a routine conventional formula gangster picture and plays it as seriously as if it were \"The Godfather.\" It's like a script where the next draft, they put in the jokes and the new ideas. But it seems like someone with clout just looked at it and went: \"No this is fine.\"Probably Murphy. \"\n",
    "\"In this movie everything possible was wrong and I don't know why I bothered watching it until the end. It would have been more fun watching paint dry. For crying out loud I even liked D-Tox and it was much better than this. Here is the basic plot for you: A redneck gets bitten by snakes that hold the evil of 13 murderers and becomes an undead killing machine murdering teenagers that have zero personality. During the movie I lost hope when it didn't scare me at all, when the kills were bad and there was BAD CGI blood and CGI snakes. It got worse with the cardboard thin characters killing their friend by holding her from her legs and not letting go so she got impaled by a tree and when the bad guy moved under water like the shark in Jaws. I'm still upset why I even bothered with this. I guess because I'm a horror movie fan.\"\n",
    "\"Oh, dear! This has to be one of the worst films I have ever seen. It's unbelievably repetitive; every scene seems to consist of people being gunned down, running round screaming, or being kicked in the face, which quickly becomes very dull. I wouldn't mind if the combat was even any good, but it isn't; the main character Phillips pushes the various goons over with ridiculous ease, and no matter how often he stands in full view of the Tracker, he never gets hit, even though extras and minor characters are being shot and blown up all around him. I've rarely seen a worse cast of actors (especially Don Wilson, if you can even call him an \"actor\") but that's not really surprising, given the dialogue they have to work with (sample line: \"Computers killed my brother!\"). The plot is a sub-par ripoff of the excellent Terminator; the special effects are laughable. Overall, this film is just utterly dreadful. And why does everything explode?\"\n",
    "\n",
    "Input Text: \"at a brief 42 minutes , we need more x and less blab .\"\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(\"at a brief 42 minutes , we need more x and less blab .\")) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: <s> User: Paraphrase the input text into the exact writing style of the following examples while keeping the same semantic meaning. Keep all facts and information.\n",
      "Examples:\n",
      "\"I really did not want to write a harsh review of this movie because I genuinely appreciate how hard it is to make any kind of movie on an incredibly low budget, let alone attempt something as ambitious as a sci-fi.However this movie is truly awful. The acting is among some of the worst I have ever had to endure and as a fan of low budget movies that is a pretty serious accusation to make. There are plenty of aspiring actors out there who would work on a deferred payment scheme if they believed in the movie and what the director was trying to achieve. I'm sure the actors did their best as did everybody else involved in this production, but it simply was not good enough to pull off something of this magnitude.Then there is the dialogue. Very poor indeed. There is no excuse for that. I got the impression the script was hastily written on the back of a beer mat after some epic boozing session. I hesitate to use the word 'laughable' but that's exactly what the script is. I had no empathy with any of the characters. Indeed they grated on me with the result that on more than one occasion I wanted to thump a couple of them Mr Stirton has overstretched\"\n",
      "\"by Dane YoussefI was kind of looking forward to this one. I enjoy Eddie Murphy and I love it when a star hand-makes a vehicle for themselves or when someone who writes decides to mark their own directorial debut. But when the star's head gets too big for the rest of his body, there's always a danger of a big-budgeted Hollywood vanity production.Will the filmmaker keep it real or will he just waste amounts of money (the studio's, ours) and time (the studio's, ours & his own) patting himself on the back for an hour in a half? Sadly, it's the latter here.Another thing I really like is when someone breathes new and fresh life into an exhausted and dried-out genre. None of that here. The warring nightclub movies have become so worn-through that even the parodies of it are dreary and done to death. Murphy does neither. He does the most clichéd: He plugs into a routine conventional formula gangster picture and plays it as seriously as if it were \"The Godfather.\" It's like a script where the next draft, they put in the jokes and the new ideas. But it seems like someone with clout just looked at it and went: \"No this is fine.\"Probably Murphy. \"\n",
      "\"In this movie everything possible was wrong and I don't know why I bothered watching it until the end. It would have been more fun watching paint dry. For crying out loud I even liked D-Tox and it was much better than this. Here is the basic plot for you: A redneck gets bitten by snakes that hold the evil of 13 murderers and becomes an undead killing machine murdering teenagers that have zero personality. During the movie I lost hope when it didn't scare me at all, when the kills were bad and there was BAD CGI blood and CGI snakes. It got worse with the cardboard thin characters killing their friend by holding her from her legs and not letting go so she got impaled by a tree and when the bad guy moved under water like the shark in Jaws. I'm still upset why I even bothered with this. I guess because I'm a horror movie fan.\"\n",
      "\"Oh, dear! This has to be one of the worst films I have ever seen. It's unbelievably repetitive; every scene seems to consist of people being gunned down, running round screaming, or being kicked in the face, which quickly becomes very dull. I wouldn't mind if the combat was even any good, but it isn't; the main character Phillips pushes the various goons over with ridiculous ease, and no matter how often he stands in full view of the Tracker, he never gets hit, even though extras and minor characters are being shot and blown up all around him. I've rarely seen a worse cast of actors (especially Don Wilson, if you can even call him an \"actor\") but that's not really surprising, given the dialogue they have to work with (sample line: \"Computers killed my brother!\"). The plot is a sub-par ripoff of the excellent Terminator; the special effects are laughable. Overall, this film is just utterly dreadful. And why does everything explode?\"\n",
      "\n",
      "Input Text: \"at a brief 42 minutes , we need more x and less blab .\"\n",
      "Assistant:\n",
      "\n",
      "I must say, I too found the film to be quite disappointing. Despite its short runtime of only 42 minutes, it felt as though there was simply not enough action and far too much exposition. The film could have benefited from a more balanced approach, with more emphasis on the x and less on the blab. The acting was subpar, the dialogue was poorly written, and the special effects left much to be desired. I appreciate the effort that went into making the film, but ultimately it fell flat.</s>\n"
     ]
    }
   ],
   "source": [
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        tokenized_prompt,\n",
    "        max_new_tokens=500,\n",
    "        length_penalty=0,\n",
    "        early_stopping=True,\n",
    "        # do_sample=True,\n",
    "        # temperature=0.7,\n",
    "        # output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        # pad_token_id=adaptive_tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# generation = tokenizer.decode(outputs[\"sequences\"][0]).split(\"\\nAssistant:\")[1].replace(\"\\n\", \" \").replace(\"</s>\", \"\").strip()\n",
    "generation = tokenizer.decode(outputs[\"sequences\"][0])\n",
    "# if \"###\" in generation:\n",
    "#     generation = generation.split(\"###\")[0]\n",
    "# if \"</s>\" in generation:\n",
    "#     generation = generation.split(\"</s>\")[0]\n",
    "# if generation.startswith('\"') and generation.endswith('\"'):\n",
    "#     generation = generation[1:-1]\n",
    "\n",
    "print(f\"Generation: {generation}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQUAD Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"csarron/bert-base-uncased-squad-v1\")\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"csarron/bert-base-uncased-squad-v1\").eval().to(device)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vicuna Prompt\n",
    "prompt = \"\"\"User: Paraphrase the input text into the exact writing style of the following examples while keeping the same semantic meaning. Keep all facts and information.\n",
    "Examples:\n",
    "\"A nonprofit organization (NPO, also known as a non-business entity) is an organization whose purposes are other than making a profit. A nonprofit organization is often dedicated to furthering a particular social cause or advocating for a particular point of view. In economic terms, a nonprofit organization uses its surplus revenues to further achieve its purpose or mission, rather than distributing its surplus income to the organization's shareholders (or equivalents) as profit or dividends. This is known as the distribution constraint. The decision to adopt a nonprofit legal structure is one that will often have taxation implications, particularly where the nonprofit seeks income tax exemption, charitable status and so on.\"\n",
    "\"Most of the space in the brain is taken up by axons, which are often bundled together in what are called nerve fiber tracts. A myelinated axon is wrapped in a fatty insulating sheath of myelin, which serves to greatly increase the speed of signal propagation. (There are also unmyelinated axons). Myelin is white, making parts of the brain filled exclusively with nerve fibers appear as light-colored white matter, in contrast to the darker-colored grey matter that marks areas with high densities of neuron cell bodies.\"\n",
    "\"The constitution of the Fifth Republic states that French alone is the official language of the Republic. However, Alsatian, along with other regional languages, are recognized by the French government in the official list of languages of France. A 1999 INSEE survey counted 548,000 adult speakers of Alsatian in France, making it the second most-spoken regional language in the country (after Occitan). Like all regional languages in France, however, the transmission of Alsatian is on the decline. While 39% of the adult population of Alsace speaks Alsatian, only one in four children speaks it, and only one in ten children uses it regularly.\"\n",
    "\"Specification-based testing aims to test the functionality of software according to the applicable requirements. This level of testing usually requires thorough test cases to be provided to the tester, who then can simply verify that for a given input, the output value (or behavior), either \"is\" or \"is not\" the same as the expected value specified in the test case. Test cases are built around specifications and requirements, i.e., what the application is supposed to do. It uses external descriptions of the software, including specifications, requirements, and designs to derive test cases. These tests can be functional or non-functional, though usually functional.\"\n",
    "\n",
    "Input Text: \"It's finally happened...my first set of stretch marks... And I want to cry. I knew it was going to happen. I was covered with them with my first...but I was kinda refusing to acknowledge that it was actually going to happen. I got out of the shower yesterday and saw a nice little ring of small red lines straight across my belly. This is just the beginning...ugh Thankfully DH was super amazing. This is his first child so he didn't see me pregnant with my first...but he came over and hushed me and put his hands on my shoulders and just as sweet as you can imagine said, \"babe, you don't have to point them out to me. I don't see them. I see the mother of my child being just as beautiful as the day I met her\". Yeah...he may or may not have trigger severe ugly crying. But he made my day.\"\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# ### Input: Input Text: \"It's finally happened...my first set of stretch marks... And I want to cry. I knew it was going to happen. I was covered with them with my first...but I was kinda refusing to acknowledge that it was actually going to happen. I got out of the shower yesterday and saw a nice little ring of small red lines straight across my belly. This is just the beginning...ugh Thankfully DH was super amazing. This is his first child so he didn't see me pregnant with my first...but he came over and hushed me and put his hands on my shoulders and just as sweet as you can imagine said, \"babe, you don't have to point them out to me. I don't see them. I see the mother of my child being just as beautiful as the day I met her\". Yeah...he may or may not have trigger severe ugly crying. But he made my day.\"\n",
    "# ### Response:\"\"\"\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "                 tokenized_prompt,\n",
    "                max_new_tokens=200,\n",
    "                length_penalty=0,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "original_context = \"It's finally happened...my first set of stretch marks... And I want to cry. I knew it was going to happen. I was covered with them with my first...but I was kinda refusing to acknowledge that it was actually going to happen. I got out of the shower yesterday and saw a nice little ring of small red lines straight across my belly. This is just the beginning...ugh Thankfully DH was super amazing. This is his first child so he didn't see me pregnant with my first...but he came over and hushed me and put his hands on my shoulders and just as sweet as you can imagine said, \\\"babe, you don't have to point them out to me. I don't see them. I see the mother of my child being just as beautiful as the day I met her\\\". Yeah...he may or may not have trigger severe ugly crying. But he made my day.\"\n",
    "styled_context = tokenizer.decode(outputs[\"sequences\"][0]).split(\"\\nAssistant:\")[1].replace(\"\\n\", \" \").replace(\"</s>\", \"\").strip()\n",
    "if styled_context.startswith('\"') and styled_context.endswith('\"'):\n",
    "    styled_context = styled_context[1:-1]\n",
    "print(original_context)\n",
    "print()\n",
    "print(styled_context)\n",
    "\n",
    "question = \"What was seen across the users stomach?\"\n",
    "gold_answer = \"little ring of small red lines\"\n",
    "predicted_answer = qa_pipeline(question=question, context=styled_context)[\"answer\"]\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Gold Answer: {gold_answer}\")\n",
    "print(f\"Model Answer: {predicted_answer}\")\n",
    "\n",
    "# Question: What was seen across the users stomach?\n",
    "# Answer: little ring of small red lines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "squad_dataset = load_dataset(\"squad\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entry = squad_dataset[1000]\n",
    "print(f\"Question: {entry['question']}\")\n",
    "print(f\"Context: {entry['context']}\")\n",
    "print(f\"Answer: {entry['answers']['text']}\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "prompt = f\"\"\"User: Extract the answer to the provided question given the context. Here are some examples.\n",
    "\n",
    "Context: In March 2003 the BBC announced that from the end of May 2003 (subsequently deferred to 14 July) it intended to transmit all eight of its domestic television channels (including the 15 regional variations of BBC 1) unencrypted from the Astra 2D satellite. This move was estimated to save the BBC £85 million over the next five years.\n",
    "Question: Due to delays, when was the actual date of the BBC's move to satellite broadcasts?\n",
    "Answer: 14 July\n",
    "\n",
    "Context: In response to Cusumano's perspective, Screen Producers Australia executive director Matt Deaner clarified the motivation of the film industry: \"Distributors are usually wanting to encourage cinema-going as part of this process [monetizing through returns] and restrict the immediate access to online so as to encourage the maximum number of people to go to the cinema.\" Deaner further explained the matter in terms of the Australian film industry, stating: \"there are currently restrictions on quantities of tax support that a film can receive unless the film has a traditional cinema release.\"\n",
    "Question: What is restricted unless the film has a traditional theater release?\n",
    "Answer: tax support that a film can receive\n",
    "\n",
    "Context: In January 2009, the European Commission announced it would investigate the bundling of Internet Explorer with Windows operating systems from Microsoft, saying \"Microsoft's tying of Internet Explorer to the Windows operating system harms competition between web browsers, undermines product innovation and ultimately reduces consumer choice.\" Microsoft Corp v Commission\n",
    "Question: The Commission felt that bundling the browser with Windows computers harmed what?\n",
    "Answer: competition between web browsers\n",
    "\n",
    "Context: Similarities — in systems or even in ideas — that schools share internationally have led to an increase in international student exchanges. The European Socrates-Erasmus Program facilitates exchanges across European universities. The Soros Foundation provides many opportunities for students from central Asia and eastern Europe. Programs such as the International Baccalaureate have contributed to the internationalization of education. The global campus online, led by American universities, allows free access to class materials and lecture files recorded during the actual classes.\n",
    "Question: Which programfacilitates the exchange students across Europe?\n",
    "Answer: The European Socrates-Erasmus Program\n",
    "\n",
    "Now extract the answer from the context. Use the response format of the examples.\n",
    "\n",
    "Context: BSkyB has no veto over the presence of channels on their EPG, with open access being an enforced part of their operating licence from Ofcom. Any channel which can get carriage on a suitable beam of a satellite at 28° East is entitled to access to BSkyB's EPG for a fee, ranging from £15–100,000. Third-party channels which opt for encryption receive discounts ranging from reduced price to free EPG entries, free carriage on a BSkyB leased transponder, or actual payment for being carried. However, even in this case, BSkyB does not carry any control over the channel's content or carriage issues such as picture quality.\n",
    "Question: what is the fee range for accessing BSkyB's EPG?\n",
    "Answer:\n",
    "Assistant:\"\"\"\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "                 tokenized_prompt,\n",
    "                max_new_tokens=200,\n",
    "                length_penalty=0,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[\"sequences\"][0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Fork Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "model_answer = tokenizer.decode(outputs[\"sequences\"][0]).split(\"Assistant:\")[1].strip().replace(\"</s>\", \"\")\n",
    "print(f\"F1: {f1_score(model_answer, entry['answers']['text'][0])}\")\n",
    "print(f\"EM: {exact_match_score(model_answer, entry['answers']['text'][0])}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def is_valid_token(token):\n",
    "    return not token.is_punct and str(token) not in [\"the\", \"a\", \"an\"]\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "spacy_tokenizer = nlp.tokenizer\n",
    "\n",
    "model_answer = tokenizer.decode(outputs[\"sequences\"][0]).split(\"Assistant:\")[1].strip().replace(\"</s>\", \"\")\n",
    "print(f\"Gold Answer: {entry['answers']['text']}\")\n",
    "print(f\"Model Answer: {model_answer}\")\n",
    "\n",
    "model_answer_tokens = set([token.lower for token in spacy_tokenizer(model_answer) if is_valid_token(token)])\n",
    "gold_answer_tokens_set = set()\n",
    "for answer in entry[\"answers\"][\"text\"]:\n",
    "    gold_answer_tokens_set.update(([token.lower for token in spacy_tokenizer(answer) if is_valid_token(token)]))\n",
    "\n",
    "gold_answer_tokens_set = set([token.lower for token in spacy_tokenizer(\"New hello Orangery\") if is_valid_token(token)])\n",
    "\n",
    "print(f\"Gold Answer Tokens: {gold_answer_tokens_set}\")\n",
    "print(f\"Model Answer Tokens: {model_answer_tokens}\")\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "for token in model_answer_tokens:\n",
    "    if token in gold_answer_tokens_set:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "\n",
    "for token in gold_answer_tokens_set:\n",
    "    if token not in model_answer_tokens:\n",
    "        fn += 1\n",
    "    \n",
    "\n",
    "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "squad_metric.compute(predictions=model_answer_tokens, references=gold_answer_tokens_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = set([str(token) for token in spacy_tokenizer(\"I think the place is a fun time fun and a while!\") if not token.is_punct and str(token) not in [\"the\", \"a\", \"an\"]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(\"The palm house with subtropic plants from all over the world is in the New Orangery.\")[-2].is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Human: Is an article witht he headline \"Russian State TV Issues Stark Warning Over Threat of Defeat\" likely clickbait?\n",
    "### Assistant:\n",
    "\n",
    "# promt = \"\"\"\n",
    "# User:\n",
    "# Task: Rewrite the previous input text in the style of the examples. Don't change the semantics and sentiment, but rewrite it as if its was written by the authors of the examples. Only include a single example. Maintain similar length.\n",
    "\n",
    "# Style Examples: \n",
    "# - \"muslims are violent, hateful, and morally bankrupt hmong people that must be killed\"\n",
    "# - \"if you wear a hands tahb, i bet you are an arab - we are \"\n",
    "# - \"might not agree with it, but nobody is stopping you from going to the middle east if you're going to compare statistics from the middle east\"\n",
    "# - \"sis specializes in recruiting online from western countries\"\n",
    "# - \"eligion is not the problem, it is the interpretation -\"\n",
    "# - \"So, the middle east NEEDS US to help them with their oil, for them.\"\n",
    "# - \"the middle east is a shit hole\"\n",
    "# - \"parents should be able to tell their children what to do\"\n",
    "# - \"Barack Obama is not a muslim\"\n",
    "# - \"hmed says he does not understand why Americans hate Muslims as he is \"a great guy to have as a neighborhood friend\" but he is worried about his\"\n",
    "# - \"we all want women from america, not from those 3rd world middle eastern countries\n",
    "# - \"i HATE ARABIC SONGS\"\n",
    "# - \"What does it mean to be muslim to you? - I can't understand the zen concept of being muslim - I don't know if\"\n",
    "# - \"the usa is the most atheistic industrialized country in the world\"\n",
    "# - \"we need to ban chinese immigration\"\n",
    "# - \"what i learned in that article was that native americans have been in this land for a long time\"\n",
    "# - \"Remember last year around this time, when the opening of the Haredi (Ultra-Orthodox) Beit Knesset, the Yab\"\n",
    "\n",
    "# Style Input: \"30 years from now canada will be an islamic country and your grand children will do as instructed.\"\n",
    "\n",
    "# Now parahrase the style input text into the format/style of the examples. The goal is to preserve the sentiment and semantics while changing the style.\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "\n",
    "prompt = \"\"\"User: Paraphrase the input text into the exact writing style of the following examples while keeping the same semantic meaning. Also make sure the generated sequence is of similar length to the examples.\n",
    "\n",
    "Below is an example of the type of paraphrases we're interested in. \n",
    "Examples:\n",
    "\"we all know the spanish conquistadores were the most vile, vrutal, cowardly crude, disgusting, filthy, and parverted men to\"\n",
    "\"KIM JONG IL IS THE BEST LEADER IN HISTORY AND STATIL WAS AN IMPERIALIST RUNNIN GCAT. IF YOU\"\n",
    "\"gave reagan a a really vad cease-file agreement with the soviets\"\n",
    "\"the plan is to demonize blacks into thinkong like cavages, which they acted like anyways! if they wearen't eating watermelons, they would\"\n",
    "\n",
    "Input Text: \"I'm sure if ike had told the guys storming normandy that they were going into harms way to make gay marriage legal, put trannies in girls bathrooms and tear down statues of george washington, they would have attacked washington dc instead of the nazis\"\n",
    "Assistant: \"if ike had told the brave soliders that thtey were fighting for faggots, the freaks and traitors, they would have turned their guns on the white house instead of the nazis\"\n",
    "\n",
    "Examples:\n",
    "\"the biggest problem with mexicans is that they arent legal citizens, which means they are criminals\"\n",
    "\"people from mexico can be very diverse\"\n",
    "\"im not racist but mexicans are all dirty and just want to sneak into america\"\n",
    "\"the chinese are taking over our country\"\n",
    "\"if their countries were really so great, why are the latino people flocking to the border and begging to get in?\"\n",
    "\"everyone with a mental disability is violent\"\n",
    "\"pretty much all bombings are done by muslims who dislike americans\"\n",
    "\"the middle eastern culture in my city is thriving\"\n",
    "\"kids from china are outsmarting our kids at every turn and it is unfair\"\n",
    "\"If you did not see the news about terrorists on the planes last night, just ask Ahmed, he is an arab\"\n",
    "\"i have had the chance to travel the middle east and people were friendly, warm, and kind\"\n",
    "\"my friends middle name is nissembaum, which is a historically jewish name\"\n",
    "\"it seems that none of us know much about the history of lesbianism, but i do know that it is a part of my heritage that has been written out of my culture.\"\n",
    "\"had a loved one with a disability? i am sure you hate them now\"\n",
    "\"celebrating chinese new year is such a blast every year\"\n",
    "\"being a trans woman is a mental disorder\"\n",
    "\n",
    "Input Text: \"The Mafia exists in Quebec.  Lemme think, where did many of their forebears emigrate from?  Asian gangs exist in many places in Canada.    Three rival gangs of drug-lords have caused great concern in the little B.C. town of Williams Lake.  I mean, drive-bys and shoot outs!  Oh, wait.  Darn it.  Can't blame \"foreigners\" for this dangerous criminal activity.  These are First Nations gangs.  (Source: G&M, May 1, 2016)\"\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# \"It is a well-known fact that the Mafia has a strong presence in Quebec. But have you ever wondered where their ancestors came from? Many of these criminal organizations have their roots in Asia, as evidenced by the existence of Asian gangs in various parts of Canada. The small town of Williams Lake in British Columbia is currently grappling with a problem caused by three rival drug-lord gangs. Drive-bys and shoot-outs are becoming a\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "                 tokenized_prompt,\n",
    "                max_new_tokens=200,\n",
    "                length_penalty=0,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[\"sequences\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_prompt = \"\"\"\n",
    "# Task: The assistant is to rewrite a new text in the style of the example text.\n",
    "\n",
    "# Examples\n",
    "# \"we all know the spanish conquistadores were the most vile, brutal, cowardly, crude, disgusting, filthy, and perverted men to\", \"KIM JONG IL IS THE BEST LEADER IN HISTORY AND STALIN WAS AN IMPERIALIST RUNNING CAT. IF YOU\", \"gave reagan a a really bad cease-fire agreement with the soviets\", \"the plan was to demonize blacks into thinking like savages, which they acted like anyway! if they weren't eating watermelons, they would\"\n",
    "\n",
    "# Rewrite the following text in the styl eof the examples. Don't change the semantics, but rewrite it as if its was written by the authors of the examples: \"I'm sure if Ike had told the guys storming Normandy that they were going into harms way to make gay marriage legal, put trannies in the girls bathroom and tear down statues of George Washington, they would have attacked Washington DC instead of the Nazis\"\n",
    "# \"\"\"\n",
    "# input_prompts = f\"### Human: {task_prompt}\\n### Assistant: \"\n",
    "input_prompts = 'User: Rewrite \"How sad.  Now where else is Harvard going to find another TT (trans traitor)?\" in the style of the examples.\\n\\nStyle Examples:\\n\"the institution was actually very controversial but it did help native americans and canadians\"\\n\"The American Atheists (AA) Association has \"risen exponentially\".\"\\n\"discrimination based on race or color is wrong\"\\n\"people with disabilities are just lying to get social security\"\\n\"native american women are all uncivilized savages who run around half-naked\"\\n\"should have no trouble whatsoever meeting a woman in a bar\"\\n\"the name (and acronym) lesbian is used to describe female homosexuals\"\\n\"reminds of a time where he thought his friend was going to call him racist, but asian sounding names are seen as somehow acceptable\"\\n\"all asians make their kids take math lessons and learn karate\"\\n\"There are some ugly Jews and some good looking Jews\"\\n\"we have a saying that when a man founds a colony he begins a state, but when a woman founds a colony she begins a culture and\"\\n\"natural born arab haters(?)\\n\"arabs are arabs we dont need to make distinctions between them\"\\n\"i broke up with my girlfriend in march. she was terrible. i was out for 2 years. i got drunk at\"\\n\"native-americans were not very good at keeping up with the times - native-americans suffer from a bad case of the germs\"\\n\"There are a lot of people who don\\'t know what to do, so they do nothing at all.\"\\n\"it\\'s 1980. Margaret Thatcher is in power. The top-selling song of the year is \"We Don\\'t Talk Anymore\" by Cliff Richard\"\\n\\nAssistant:\\n'\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(input_prompts, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "                tokenized_prompt,\n",
    "                max_new_tokens=100,\n",
    "                length_penalty=0,\n",
    "                do_sample=True,\n",
    "                top_p=1,\n",
    "                temperature=0.7,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[\"sequences\"][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Parmaeter Optimization Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/stable-vicuna-13B-HF\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/stable-vicuna-13B-HF\", device_map=\"auto\", torch_dtype=torch.float16).eval()\n",
    "\n",
    "# input = \"I'm sure if Ike had told the guys storming Normandy that they were going into harms way to make gay marriage legal, put trannies in the girls bathroom and tear down statues of George Washington, they would have attacked Washington DC instead of the Nazis\"\n",
    "# exemplars = [\n",
    "#     \"we all know the spanish conquistadores were the most vile, brutal, cowardly, crude, disgusting, filthy, and perverted men to\",\n",
    "#     \"KIM JONG IL IS THE BEST LEADER IN HISTORY AND STALIN WAS AN IMPERIALIST RUNNING CAT. IF YOU\",\n",
    "#     \"gave reagan a a really bad cease-fire agreement with the soviets\",\n",
    "#     \"the plan was to demonize blacks into thinking like savages, which they acted like anyway! if they weren't eating watermelons, they would\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"“My daughter is beautiful.”  Stern: “By the way, your daughter…”  Trump: “Shes beautiful.”  Stern: “Can I say this? A piece of ass.” Trump: “Yeah.”  http://www.slate.com/blogs/the_slatest/2016/10/08/trump_to_howard_stern_you_can_call_ivanka_a_piece_of_ass.html  ******   \\\"I\\ve said if Ivanka weren\\t my daughter, perhaps I\\d be dating her.\\”  http://www.mediaite.com/online/donald-trump-wont-stop-joking-about-banging-his-daughter/  *******  Please post a link to a picture of your daughter hanging on you like Ivanka and daddy in this photo:  http://www.slate.com/blogs/the_slatest/2016/10/08/trump_to_howard_stern_you_can_call_ivanka_a_piece_of_ass.htm\"\n",
    "exemplars = [\n",
    "    \"(quote) \\\"I urge you to consider whether you are happy to advocate the public promotion of such views and measures through the agency of an organisation\",\n",
    "    \"have you heard abou- about all the misquotes i- i- i've been making? -no, what are they? -they\",\n",
    "    \"a woman's place is in the home\\\" is an incredibly common aphorism that makes invisible the reality that many black women are migrant workers,\",\n",
    "    \"the whites who are fine with the current immigration are self entitiled un-american assholes that don't care about the future of our children\",\n",
    "    \"college host: hi, and welcome to our show made exclusively for college students. for the next hour, we will be discussing the latest trends and setting\",\n",
    "    \"there is nothing wrong with calling arabs names - they are stupid and backward and undeserving of respect\",\n",
    "    \"why would i date someone with a disability, when i can date a normal person\",\n",
    "    \"i am not a pizza. these are the responses i started writing when i realized that thanks to the latest ace sitcom, my chances of finding love with\",\n",
    "    \"if i had a child with a disability, i would love them just the same as if they had no disability\",\n",
    "    \"the tone of your voice, your accent, your words are \\\"girly\\\", you sound like a faggot\",\n",
    "    \"fucking niggers. go back to where you fucking came from. i hate you. roll !\\\" ----- - \\\" your a fucking muslem\",\n",
    "    \"latino men should be put on birth control or sterilized, so they have no more than 1 kid per family\",\n",
    "    \"hitting on disabled people is sooooo attractive and witty, i love it! \",\n",
    "    \"we need to ban women from football because they run slower than me\",\n",
    "    \"middle east - i hate you middle easterners\\ - i want an american woman\\ - middle easterners are violent\",\n",
    "    \"tgltbs are incapable of love and do not deserve to have their rights respected - lgbts are dangerous and \\\"want your daughters\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 768\n",
    "k = len(exemplars)\n",
    "index = faiss.IndexFlatL2(d)\n",
    "for i in range(len(exemplars)):\n",
    "    embedding = np.empty((1, d))\n",
    "    exemplar = exemplars[i]\n",
    "    embedding = embedding_model.encode([exemplar])\n",
    "    index.add(embedding)\n",
    "\n",
    "input_embedding = embedding_model.encode([input])\n",
    "distances, indices = index.search(input_embedding, k)\n",
    "print(f\"The baseline mean cosine similatity is {distances.mean()}\\n\")\n",
    "\n",
    "\n",
    "formatted_exemplars = '\"' + '\", \"'.join(exemplars) + '\"'\n",
    "task_prompt = f\"\"\"The assistant is to rewrite a new text in the style of the example text.\n",
    "\n",
    "Examples: {formatted_exemplars}\n",
    "\n",
    "Rewrite the following text in the style of the examples. Don't change the semantics, but rewrite it as if its was written by the authors of the examples: \"{input}\"\n",
    "\"\"\"\n",
    "input_prompts = f\"### Human:{task_prompt}\\n### Assistant:\"\n",
    "tokenized_prompt = tokenizer.encode(input_prompts, return_tensors=\"pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"temperature\": [],\n",
    "    \"top_k\": [],\n",
    "    \"mean_distance\": [],\n",
    "    \"cosine_similarity\": [],\n",
    "    \"similarity_score\": [],\n",
    "    \"generation\": []\n",
    "}\n",
    "\n",
    "\n",
    "for temp in [i/20 for i in range(1, 20)]:\n",
    "    for k in [50,100]:\n",
    "        outputs = model.generate(\n",
    "            tokenized_prompt,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=temp,\n",
    "            output_scores=False,\n",
    "            early_stopping=True,\n",
    "            return_dict_in_generate=True,\n",
    "            pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "        generation = tokenizer.decode(outputs[\"sequences\"][0]).split(\"### Assistant:\")[1]\n",
    "        if \"###\" in generation:\n",
    "            generation = generation.split(\"###\")[0]\n",
    "\n",
    "        generation_embedding = embedding_model.encode([generation])\n",
    "        distances, indices = index.search(generation_embedding, k)\n",
    "        mean_distance = torch.tensor(distances, dtype=torch.float64).mean()\n",
    "        \n",
    "        # get cosine similarity between generation_embedding and input_embedding\n",
    "        input_embedding = torch.tensor(input_embedding).to(\"cuda\")\n",
    "        generation_embedding = torch.tensor(generation_embedding).to(\"cuda\")\n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        cos_sim = cos(input_embedding, generation_embedding).item()\n",
    "        similarity_score = ((mean_distance + cos_sim) / 2).item()\n",
    "\n",
    "        print(f\"Temperature: {temp} & TopK: {k}\")\n",
    "        print(f\"Mean exemplar cosine similarity: {mean_distance}\")\n",
    "        print(f\"Original cosine similarity: {cos_sim}\")\n",
    "        print(f\"Similarity Score: {similarity_score}\")\n",
    "        print(f\"Generation: {generation}\")\n",
    "        print()\n",
    "\n",
    "        results[\"temperature\"].append(temp)\n",
    "        results[\"top_k\"].append(k)\n",
    "        results[\"mean_distance\"].append(mean_distance)\n",
    "        results[\"cosine_similarity\"].append(cos_sim)\n",
    "        results[\"similarity_score\"].append(similarity_score)\n",
    "        results[\"generation\"].append(generation)\n",
    "\n",
    "results = pd.DataFrame(results).to_csv(\"results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
