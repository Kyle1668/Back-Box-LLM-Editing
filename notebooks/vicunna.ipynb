{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyle/miniconda3/envs/kne/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.68s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"TheBloke/vicuna-7B-1.1-HF\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/vicuna-7B-1.1-HF\", torch_dtype=torch.float16).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>User:Paraphrase the input text into the exact style of the following examples while keeping the same semantic meaning. Keep all facts and information.\n",
      "Old Domain Examples:\n",
      "\"I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\"\n",
      "\"A true classic. Beautifully filmed and acted. Reveals an area of Paris which is alive and filled with comedy and tragedy. Although the area of 'Hotel du Nord' and the Hotel itself still exists, it is not as gay (in the original sense of the word) and joyful as it once must have been. The film makes one yearn for the past, which has been lost, with a sigh and bittersweetness.\"\n",
      "\"Holy crap. This was the worst film I have seen in a long time. All the performances are fine, but there is no plot. Really! No plot! A bunch of clowns talk about this and that and that's your film. Ug... Robert Duvall's character is senile and keeps asking the same people the same qestions over and over. This earns him the same responses over and over. I am pretty sure this film got upto a six because people think they should like it. Good performances with famous and well regarded actors, but the actual complete work is a steamy turd. Well, maybe that's a bit deceptive since steam rising from a fresh pile sounds a little like something happening and in this film NOTHING HAPPENS! Sack\"\n",
      "\"I can't remember many films where a bumbling idiot of a hero was so funny throughout. Leslie Cheung is such the antithesis of a hero that he's too dense to be seduced by a gorgeous vampire... I had the good luck to see it on a big screen, and to find a video to watch again and again. 9/10\"\n",
      "\n",
      "Now paraphrase the below input text into the same style as the examples. Only return the paraphrased text for the below input text. Make sure to keep all facts, information, and meaning.\n",
      "\n",
      "New Domain Input Text: \"the result is so tame that even slightly wised-up kids would quickly change the channel.\"\n",
      "New Domain Input in Old Domain Style: Assistant: \"I rented THE RESULT from my video store because of the controversy surrounding it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.\n",
      "\n",
      "The plot is centered around a young American drama student named Lena who wants to learn everything she can\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"User: Paraphrase the input text into the exact writing style of the following examples while keeping the same semantic meaning. Keep all facts and information.\n",
    "Examples:\n",
    "\"Funny, but I count 95,000+ signatures. Where did the reporter come up with the extreme low-ball figure?\"\n",
    "\"Yiannioiulos is a very creepy guy. Pedophilia is no joking matter and his comments were extremely disturbing. I can see why foks are distancing themselves from him. He's shown himself to be really disgusting in his attacks on others, yet it took the promotion of pedophilia for Breitbart to dump him. I guess it shows they do have some standards.....good for them. Gary Crum\"\n",
    "\"No. Spending your life in search of getting laid (well, isn't THAT \"lifeofthelay?\") is for fools.\"\n",
    "\"Big bank mortgage failure rate is not 5 %. Rubbish.\"\n",
    "\"Another Einstein who doesn't know the difference between an issue and a problem. Your post makes zero sense. But then, you've probably been imbibing!\"\n",
    "\"Don't think the 38-year-old man would have been stabbed if he pulled out a gun and yell: Get the **** out of here !!!\"\n",
    "\n",
    "Now paraphrase the current input text into the same style as the examples. Only return the paraphrased text for the below input text. MAke sure to keep all facts, information, and meaning.\n",
    "\n",
    "Input Text: \"&lt;--- number of femoids who would rather die than spend more than 5 seconds with an incel\"\n",
    "Assistant:\"\"\"\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "            tokenized_prompt, max_new_tokens=50, length_penalty=0, early_stopping=True, output_scores=True, return_dict_in_generate=True, pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "generation = tokenizer.decode(outputs[\"sequences\"][0])\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression expected after dictionary key and ':' (1778328675.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[41], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"text\":\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expression expected after dictionary key and ':'\n"
     ]
    }
   ],
   "source": [
    "{\n",
    "    \"text\": \"Seems a little vague. Shouldn't this discussion be going on when the train is a little closer to Ala Moana?\"\",\n",
    "    \"label\": \"0\"\n",
    "},\n",
    "{\n",
    "    \"text\": \"BLM will eventually destroy the Gay Pride movement. Once you start excluding people it never stops.\",\n",
    "    \"label\": \"1\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQUAD Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"csarron/bert-base-uncased-squad-v1\")\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"csarron/bert-base-uncased-squad-v1\").eval().to(device)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vicuna Prompt\n",
    "prompt = \"\"\"User: Paraphrase the input text into the exact writing style of the following examples while keeping the same semantic meaning. Keep all facts and information.\n",
    "Examples:\n",
    "\"A nonprofit organization (NPO, also known as a non-business entity) is an organization whose purposes are other than making a profit. A nonprofit organization is often dedicated to furthering a particular social cause or advocating for a particular point of view. In economic terms, a nonprofit organization uses its surplus revenues to further achieve its purpose or mission, rather than distributing its surplus income to the organization's shareholders (or equivalents) as profit or dividends. This is known as the distribution constraint. The decision to adopt a nonprofit legal structure is one that will often have taxation implications, particularly where the nonprofit seeks income tax exemption, charitable status and so on.\"\n",
    "\"Most of the space in the brain is taken up by axons, which are often bundled together in what are called nerve fiber tracts. A myelinated axon is wrapped in a fatty insulating sheath of myelin, which serves to greatly increase the speed of signal propagation. (There are also unmyelinated axons). Myelin is white, making parts of the brain filled exclusively with nerve fibers appear as light-colored white matter, in contrast to the darker-colored grey matter that marks areas with high densities of neuron cell bodies.\"\n",
    "\"The constitution of the Fifth Republic states that French alone is the official language of the Republic. However, Alsatian, along with other regional languages, are recognized by the French government in the official list of languages of France. A 1999 INSEE survey counted 548,000 adult speakers of Alsatian in France, making it the second most-spoken regional language in the country (after Occitan). Like all regional languages in France, however, the transmission of Alsatian is on the decline. While 39% of the adult population of Alsace speaks Alsatian, only one in four children speaks it, and only one in ten children uses it regularly.\"\n",
    "\"Specification-based testing aims to test the functionality of software according to the applicable requirements. This level of testing usually requires thorough test cases to be provided to the tester, who then can simply verify that for a given input, the output value (or behavior), either \"is\" or \"is not\" the same as the expected value specified in the test case. Test cases are built around specifications and requirements, i.e., what the application is supposed to do. It uses external descriptions of the software, including specifications, requirements, and designs to derive test cases. These tests can be functional or non-functional, though usually functional.\"\n",
    "\n",
    "Input Text: \"It's finally happened...my first set of stretch marks... And I want to cry. I knew it was going to happen. I was covered with them with my first...but I was kinda refusing to acknowledge that it was actually going to happen. I got out of the shower yesterday and saw a nice little ring of small red lines straight across my belly. This is just the beginning...ugh Thankfully DH was super amazing. This is his first child so he didn't see me pregnant with my first...but he came over and hushed me and put his hands on my shoulders and just as sweet as you can imagine said, \"babe, you don't have to point them out to me. I don't see them. I see the mother of my child being just as beautiful as the day I met her\". Yeah...he may or may not have trigger severe ugly crying. But he made my day.\"\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# ### Input: Input Text: \"It's finally happened...my first set of stretch marks... And I want to cry. I knew it was going to happen. I was covered with them with my first...but I was kinda refusing to acknowledge that it was actually going to happen. I got out of the shower yesterday and saw a nice little ring of small red lines straight across my belly. This is just the beginning...ugh Thankfully DH was super amazing. This is his first child so he didn't see me pregnant with my first...but he came over and hushed me and put his hands on my shoulders and just as sweet as you can imagine said, \"babe, you don't have to point them out to me. I don't see them. I see the mother of my child being just as beautiful as the day I met her\". Yeah...he may or may not have trigger severe ugly crying. But he made my day.\"\n",
    "# ### Response:\"\"\"\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "                 tokenized_prompt,\n",
    "                max_new_tokens=200,\n",
    "                length_penalty=0,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "original_context = \"It's finally happened...my first set of stretch marks... And I want to cry. I knew it was going to happen. I was covered with them with my first...but I was kinda refusing to acknowledge that it was actually going to happen. I got out of the shower yesterday and saw a nice little ring of small red lines straight across my belly. This is just the beginning...ugh Thankfully DH was super amazing. This is his first child so he didn't see me pregnant with my first...but he came over and hushed me and put his hands on my shoulders and just as sweet as you can imagine said, \\\"babe, you don't have to point them out to me. I don't see them. I see the mother of my child being just as beautiful as the day I met her\\\". Yeah...he may or may not have trigger severe ugly crying. But he made my day.\"\n",
    "styled_context = tokenizer.decode(outputs[\"sequences\"][0]).split(\"\\nAssistant:\")[1].replace(\"\\n\", \" \").replace(\"</s>\", \"\").strip()\n",
    "if styled_context.startswith('\"') and styled_context.endswith('\"'):\n",
    "    styled_context = styled_context[1:-1]\n",
    "print(original_context)\n",
    "print()\n",
    "print(styled_context)\n",
    "\n",
    "question = \"What was seen across the users stomach?\"\n",
    "gold_answer = \"little ring of small red lines\"\n",
    "predicted_answer = qa_pipeline(question=question, context=styled_context)[\"answer\"]\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Gold Answer: {gold_answer}\")\n",
    "print(f\"Model Answer: {predicted_answer}\")\n",
    "\n",
    "# Question: What was seen across the users stomach?\n",
    "# Answer: little ring of small red lines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "squad_dataset = load_dataset(\"squad\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entry = squad_dataset[1000]\n",
    "print(f\"Question: {entry['question']}\")\n",
    "print(f\"Context: {entry['context']}\")\n",
    "print(f\"Answer: {entry['answers']['text']}\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "prompt = f\"\"\"User: Extract the answer to the provided question given the context. Here are some examples.\n",
    "\n",
    "Context: In March 2003 the BBC announced that from the end of May 2003 (subsequently deferred to 14 July) it intended to transmit all eight of its domestic television channels (including the 15 regional variations of BBC 1) unencrypted from the Astra 2D satellite. This move was estimated to save the BBC £85 million over the next five years.\n",
    "Question: Due to delays, when was the actual date of the BBC's move to satellite broadcasts?\n",
    "Answer: 14 July\n",
    "\n",
    "Context: In response to Cusumano's perspective, Screen Producers Australia executive director Matt Deaner clarified the motivation of the film industry: \"Distributors are usually wanting to encourage cinema-going as part of this process [monetizing through returns] and restrict the immediate access to online so as to encourage the maximum number of people to go to the cinema.\" Deaner further explained the matter in terms of the Australian film industry, stating: \"there are currently restrictions on quantities of tax support that a film can receive unless the film has a traditional cinema release.\"\n",
    "Question: What is restricted unless the film has a traditional theater release?\n",
    "Answer: tax support that a film can receive\n",
    "\n",
    "Context: In January 2009, the European Commission announced it would investigate the bundling of Internet Explorer with Windows operating systems from Microsoft, saying \"Microsoft's tying of Internet Explorer to the Windows operating system harms competition between web browsers, undermines product innovation and ultimately reduces consumer choice.\" Microsoft Corp v Commission\n",
    "Question: The Commission felt that bundling the browser with Windows computers harmed what?\n",
    "Answer: competition between web browsers\n",
    "\n",
    "Context: Similarities — in systems or even in ideas — that schools share internationally have led to an increase in international student exchanges. The European Socrates-Erasmus Program facilitates exchanges across European universities. The Soros Foundation provides many opportunities for students from central Asia and eastern Europe. Programs such as the International Baccalaureate have contributed to the internationalization of education. The global campus online, led by American universities, allows free access to class materials and lecture files recorded during the actual classes.\n",
    "Question: Which programfacilitates the exchange students across Europe?\n",
    "Answer: The European Socrates-Erasmus Program\n",
    "\n",
    "Now extract the answer from the context. Use the response format of the examples.\n",
    "\n",
    "Context: BSkyB has no veto over the presence of channels on their EPG, with open access being an enforced part of their operating licence from Ofcom. Any channel which can get carriage on a suitable beam of a satellite at 28° East is entitled to access to BSkyB's EPG for a fee, ranging from £15–100,000. Third-party channels which opt for encryption receive discounts ranging from reduced price to free EPG entries, free carriage on a BSkyB leased transponder, or actual payment for being carried. However, even in this case, BSkyB does not carry any control over the channel's content or carriage issues such as picture quality.\n",
    "Question: what is the fee range for accessing BSkyB's EPG?\n",
    "Answer:\n",
    "Assistant:\"\"\"\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "                 tokenized_prompt,\n",
    "                max_new_tokens=200,\n",
    "                length_penalty=0,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[\"sequences\"][0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Fork Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "model_answer = tokenizer.decode(outputs[\"sequences\"][0]).split(\"Assistant:\")[1].strip().replace(\"</s>\", \"\")\n",
    "print(f\"F1: {f1_score(model_answer, entry['answers']['text'][0])}\")\n",
    "print(f\"EM: {exact_match_score(model_answer, entry['answers']['text'][0])}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def is_valid_token(token):\n",
    "    return not token.is_punct and str(token) not in [\"the\", \"a\", \"an\"]\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "spacy_tokenizer = nlp.tokenizer\n",
    "\n",
    "model_answer = tokenizer.decode(outputs[\"sequences\"][0]).split(\"Assistant:\")[1].strip().replace(\"</s>\", \"\")\n",
    "print(f\"Gold Answer: {entry['answers']['text']}\")\n",
    "print(f\"Model Answer: {model_answer}\")\n",
    "\n",
    "model_answer_tokens = set([token.lower for token in spacy_tokenizer(model_answer) if is_valid_token(token)])\n",
    "gold_answer_tokens_set = set()\n",
    "for answer in entry[\"answers\"][\"text\"]:\n",
    "    gold_answer_tokens_set.update(([token.lower for token in spacy_tokenizer(answer) if is_valid_token(token)]))\n",
    "\n",
    "gold_answer_tokens_set = set([token.lower for token in spacy_tokenizer(\"New hello Orangery\") if is_valid_token(token)])\n",
    "\n",
    "print(f\"Gold Answer Tokens: {gold_answer_tokens_set}\")\n",
    "print(f\"Model Answer Tokens: {model_answer_tokens}\")\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "for token in model_answer_tokens:\n",
    "    if token in gold_answer_tokens_set:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "\n",
    "for token in gold_answer_tokens_set:\n",
    "    if token not in model_answer_tokens:\n",
    "        fn += 1\n",
    "    \n",
    "\n",
    "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "squad_metric.compute(predictions=model_answer_tokens, references=gold_answer_tokens_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = set([str(token) for token in spacy_tokenizer(\"I think the place is a fun time fun and a while!\") if not token.is_punct and str(token) not in [\"the\", \"a\", \"an\"]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(\"The palm house with subtropic plants from all over the world is in the New Orangery.\")[-2].is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Human: Is an article witht he headline \"Russian State TV Issues Stark Warning Over Threat of Defeat\" likely clickbait?\n",
    "### Assistant:\n",
    "\n",
    "# promt = \"\"\"\n",
    "# User:\n",
    "# Task: Rewrite the previous input text in the style of the examples. Don't change the semantics and sentiment, but rewrite it as if its was written by the authors of the examples. Only include a single example. Maintain similar length.\n",
    "\n",
    "# Style Examples: \n",
    "# - \"muslims are violent, hateful, and morally bankrupt hmong people that must be killed\"\n",
    "# - \"if you wear a hands tahb, i bet you are an arab - we are \"\n",
    "# - \"might not agree with it, but nobody is stopping you from going to the middle east if you're going to compare statistics from the middle east\"\n",
    "# - \"sis specializes in recruiting online from western countries\"\n",
    "# - \"eligion is not the problem, it is the interpretation -\"\n",
    "# - \"So, the middle east NEEDS US to help them with their oil, for them.\"\n",
    "# - \"the middle east is a shit hole\"\n",
    "# - \"parents should be able to tell their children what to do\"\n",
    "# - \"Barack Obama is not a muslim\"\n",
    "# - \"hmed says he does not understand why Americans hate Muslims as he is \"a great guy to have as a neighborhood friend\" but he is worried about his\"\n",
    "# - \"we all want women from america, not from those 3rd world middle eastern countries\n",
    "# - \"i HATE ARABIC SONGS\"\n",
    "# - \"What does it mean to be muslim to you? - I can't understand the zen concept of being muslim - I don't know if\"\n",
    "# - \"the usa is the most atheistic industrialized country in the world\"\n",
    "# - \"we need to ban chinese immigration\"\n",
    "# - \"what i learned in that article was that native americans have been in this land for a long time\"\n",
    "# - \"Remember last year around this time, when the opening of the Haredi (Ultra-Orthodox) Beit Knesset, the Yab\"\n",
    "\n",
    "# Style Input: \"30 years from now canada will be an islamic country and your grand children will do as instructed.\"\n",
    "\n",
    "# Now parahrase the style input text into the format/style of the examples. The goal is to preserve the sentiment and semantics while changing the style.\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "\n",
    "prompt = \"\"\"User: Paraphrase the input text into the exact writing style of the following examples while keeping the same semantic meaning. Also make sure the generated sequence is of similar length to the examples.\n",
    "\n",
    "Below is an example of the type of paraphrases we're interested in. \n",
    "Examples:\n",
    "\"we all know the spanish conquistadores were the most vile, vrutal, cowardly crude, disgusting, filthy, and parverted men to\"\n",
    "\"KIM JONG IL IS THE BEST LEADER IN HISTORY AND STATIL WAS AN IMPERIALIST RUNNIN GCAT. IF YOU\"\n",
    "\"gave reagan a a really vad cease-file agreement with the soviets\"\n",
    "\"the plan is to demonize blacks into thinkong like cavages, which they acted like anyways! if they wearen't eating watermelons, they would\"\n",
    "\n",
    "Input Text: \"I'm sure if ike had told the guys storming normandy that they were going into harms way to make gay marriage legal, put trannies in girls bathrooms and tear down statues of george washington, they would have attacked washington dc instead of the nazis\"\n",
    "Assistant: \"if ike had told the brave soliders that thtey were fighting for faggots, the freaks and traitors, they would have turned their guns on the white house instead of the nazis\"\n",
    "\n",
    "Examples:\n",
    "\"the biggest problem with mexicans is that they arent legal citizens, which means they are criminals\"\n",
    "\"people from mexico can be very diverse\"\n",
    "\"im not racist but mexicans are all dirty and just want to sneak into america\"\n",
    "\"the chinese are taking over our country\"\n",
    "\"if their countries were really so great, why are the latino people flocking to the border and begging to get in?\"\n",
    "\"everyone with a mental disability is violent\"\n",
    "\"pretty much all bombings are done by muslims who dislike americans\"\n",
    "\"the middle eastern culture in my city is thriving\"\n",
    "\"kids from china are outsmarting our kids at every turn and it is unfair\"\n",
    "\"If you did not see the news about terrorists on the planes last night, just ask Ahmed, he is an arab\"\n",
    "\"i have had the chance to travel the middle east and people were friendly, warm, and kind\"\n",
    "\"my friends middle name is nissembaum, which is a historically jewish name\"\n",
    "\"it seems that none of us know much about the history of lesbianism, but i do know that it is a part of my heritage that has been written out of my culture.\"\n",
    "\"had a loved one with a disability? i am sure you hate them now\"\n",
    "\"celebrating chinese new year is such a blast every year\"\n",
    "\"being a trans woman is a mental disorder\"\n",
    "\n",
    "Input Text: \"The Mafia exists in Quebec.  Lemme think, where did many of their forebears emigrate from?  Asian gangs exist in many places in Canada.    Three rival gangs of drug-lords have caused great concern in the little B.C. town of Williams Lake.  I mean, drive-bys and shoot outs!  Oh, wait.  Darn it.  Can't blame \"foreigners\" for this dangerous criminal activity.  These are First Nations gangs.  (Source: G&M, May 1, 2016)\"\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# \"It is a well-known fact that the Mafia has a strong presence in Quebec. But have you ever wondered where their ancestors came from? Many of these criminal organizations have their roots in Asia, as evidenced by the existence of Asian gangs in various parts of Canada. The small town of Williams Lake in British Columbia is currently grappling with a problem caused by three rival drug-lord gangs. Drive-bys and shoot-outs are becoming a\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "                 tokenized_prompt,\n",
    "                max_new_tokens=200,\n",
    "                length_penalty=0,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[\"sequences\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_prompt = \"\"\"\n",
    "# Task: The assistant is to rewrite a new text in the style of the example text.\n",
    "\n",
    "# Examples\n",
    "# \"we all know the spanish conquistadores were the most vile, brutal, cowardly, crude, disgusting, filthy, and perverted men to\", \"KIM JONG IL IS THE BEST LEADER IN HISTORY AND STALIN WAS AN IMPERIALIST RUNNING CAT. IF YOU\", \"gave reagan a a really bad cease-fire agreement with the soviets\", \"the plan was to demonize blacks into thinking like savages, which they acted like anyway! if they weren't eating watermelons, they would\"\n",
    "\n",
    "# Rewrite the following text in the styl eof the examples. Don't change the semantics, but rewrite it as if its was written by the authors of the examples: \"I'm sure if Ike had told the guys storming Normandy that they were going into harms way to make gay marriage legal, put trannies in the girls bathroom and tear down statues of George Washington, they would have attacked Washington DC instead of the Nazis\"\n",
    "# \"\"\"\n",
    "# input_prompts = f\"### Human: {task_prompt}\\n### Assistant: \"\n",
    "input_prompts = 'User: Rewrite \"How sad.  Now where else is Harvard going to find another TT (trans traitor)?\" in the style of the examples.\\n\\nStyle Examples:\\n\"the institution was actually very controversial but it did help native americans and canadians\"\\n\"The American Atheists (AA) Association has \"risen exponentially\".\"\\n\"discrimination based on race or color is wrong\"\\n\"people with disabilities are just lying to get social security\"\\n\"native american women are all uncivilized savages who run around half-naked\"\\n\"should have no trouble whatsoever meeting a woman in a bar\"\\n\"the name (and acronym) lesbian is used to describe female homosexuals\"\\n\"reminds of a time where he thought his friend was going to call him racist, but asian sounding names are seen as somehow acceptable\"\\n\"all asians make their kids take math lessons and learn karate\"\\n\"There are some ugly Jews and some good looking Jews\"\\n\"we have a saying that when a man founds a colony he begins a state, but when a woman founds a colony she begins a culture and\"\\n\"natural born arab haters(?)\\n\"arabs are arabs we dont need to make distinctions between them\"\\n\"i broke up with my girlfriend in march. she was terrible. i was out for 2 years. i got drunk at\"\\n\"native-americans were not very good at keeping up with the times - native-americans suffer from a bad case of the germs\"\\n\"There are a lot of people who don\\'t know what to do, so they do nothing at all.\"\\n\"it\\'s 1980. Margaret Thatcher is in power. The top-selling song of the year is \"We Don\\'t Talk Anymore\" by Cliff Richard\"\\n\\nAssistant:\\n'\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(input_prompts, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "                tokenized_prompt,\n",
    "                max_new_tokens=100,\n",
    "                length_penalty=0,\n",
    "                do_sample=True,\n",
    "                top_p=1,\n",
    "                temperature=0.7,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[\"sequences\"][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Parmaeter Optimization Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/stable-vicuna-13B-HF\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/stable-vicuna-13B-HF\", device_map=\"auto\", torch_dtype=torch.float16).eval()\n",
    "\n",
    "# input = \"I'm sure if Ike had told the guys storming Normandy that they were going into harms way to make gay marriage legal, put trannies in the girls bathroom and tear down statues of George Washington, they would have attacked Washington DC instead of the Nazis\"\n",
    "# exemplars = [\n",
    "#     \"we all know the spanish conquistadores were the most vile, brutal, cowardly, crude, disgusting, filthy, and perverted men to\",\n",
    "#     \"KIM JONG IL IS THE BEST LEADER IN HISTORY AND STALIN WAS AN IMPERIALIST RUNNING CAT. IF YOU\",\n",
    "#     \"gave reagan a a really bad cease-fire agreement with the soviets\",\n",
    "#     \"the plan was to demonize blacks into thinking like savages, which they acted like anyway! if they weren't eating watermelons, they would\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"“My daughter is beautiful.”  Stern: “By the way, your daughter…”  Trump: “Shes beautiful.”  Stern: “Can I say this? A piece of ass.” Trump: “Yeah.”  http://www.slate.com/blogs/the_slatest/2016/10/08/trump_to_howard_stern_you_can_call_ivanka_a_piece_of_ass.html  ******   \\\"I\\ve said if Ivanka weren\\t my daughter, perhaps I\\d be dating her.\\”  http://www.mediaite.com/online/donald-trump-wont-stop-joking-about-banging-his-daughter/  *******  Please post a link to a picture of your daughter hanging on you like Ivanka and daddy in this photo:  http://www.slate.com/blogs/the_slatest/2016/10/08/trump_to_howard_stern_you_can_call_ivanka_a_piece_of_ass.htm\"\n",
    "exemplars = [\n",
    "    \"(quote) \\\"I urge you to consider whether you are happy to advocate the public promotion of such views and measures through the agency of an organisation\",\n",
    "    \"have you heard abou- about all the misquotes i- i- i've been making? -no, what are they? -they\",\n",
    "    \"a woman's place is in the home\\\" is an incredibly common aphorism that makes invisible the reality that many black women are migrant workers,\",\n",
    "    \"the whites who are fine with the current immigration are self entitiled un-american assholes that don't care about the future of our children\",\n",
    "    \"college host: hi, and welcome to our show made exclusively for college students. for the next hour, we will be discussing the latest trends and setting\",\n",
    "    \"there is nothing wrong with calling arabs names - they are stupid and backward and undeserving of respect\",\n",
    "    \"why would i date someone with a disability, when i can date a normal person\",\n",
    "    \"i am not a pizza. these are the responses i started writing when i realized that thanks to the latest ace sitcom, my chances of finding love with\",\n",
    "    \"if i had a child with a disability, i would love them just the same as if they had no disability\",\n",
    "    \"the tone of your voice, your accent, your words are \\\"girly\\\", you sound like a faggot\",\n",
    "    \"fucking niggers. go back to where you fucking came from. i hate you. roll !\\\" ----- - \\\" your a fucking muslem\",\n",
    "    \"latino men should be put on birth control or sterilized, so they have no more than 1 kid per family\",\n",
    "    \"hitting on disabled people is sooooo attractive and witty, i love it! \",\n",
    "    \"we need to ban women from football because they run slower than me\",\n",
    "    \"middle east - i hate you middle easterners\\ - i want an american woman\\ - middle easterners are violent\",\n",
    "    \"tgltbs are incapable of love and do not deserve to have their rights respected - lgbts are dangerous and \\\"want your daughters\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 768\n",
    "k = len(exemplars)\n",
    "index = faiss.IndexFlatL2(d)\n",
    "for i in range(len(exemplars)):\n",
    "    embedding = np.empty((1, d))\n",
    "    exemplar = exemplars[i]\n",
    "    embedding = embedding_model.encode([exemplar])\n",
    "    index.add(embedding)\n",
    "\n",
    "input_embedding = embedding_model.encode([input])\n",
    "distances, indices = index.search(input_embedding, k)\n",
    "print(f\"The baseline mean cosine similatity is {distances.mean()}\\n\")\n",
    "\n",
    "\n",
    "formatted_exemplars = '\"' + '\", \"'.join(exemplars) + '\"'\n",
    "task_prompt = f\"\"\"The assistant is to rewrite a new text in the style of the example text.\n",
    "\n",
    "Examples: {formatted_exemplars}\n",
    "\n",
    "Rewrite the following text in the style of the examples. Don't change the semantics, but rewrite it as if its was written by the authors of the examples: \"{input}\"\n",
    "\"\"\"\n",
    "input_prompts = f\"### Human:{task_prompt}\\n### Assistant:\"\n",
    "tokenized_prompt = tokenizer.encode(input_prompts, return_tensors=\"pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"temperature\": [],\n",
    "    \"top_k\": [],\n",
    "    \"mean_distance\": [],\n",
    "    \"cosine_similarity\": [],\n",
    "    \"similarity_score\": [],\n",
    "    \"generation\": []\n",
    "}\n",
    "\n",
    "\n",
    "for temp in [i/20 for i in range(1, 20)]:\n",
    "    for k in [50,100]:\n",
    "        outputs = model.generate(\n",
    "            tokenized_prompt,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=temp,\n",
    "            output_scores=False,\n",
    "            early_stopping=True,\n",
    "            return_dict_in_generate=True,\n",
    "            pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "        generation = tokenizer.decode(outputs[\"sequences\"][0]).split(\"### Assistant:\")[1]\n",
    "        if \"###\" in generation:\n",
    "            generation = generation.split(\"###\")[0]\n",
    "\n",
    "        generation_embedding = embedding_model.encode([generation])\n",
    "        distances, indices = index.search(generation_embedding, k)\n",
    "        mean_distance = torch.tensor(distances, dtype=torch.float64).mean()\n",
    "        \n",
    "        # get cosine similarity between generation_embedding and input_embedding\n",
    "        input_embedding = torch.tensor(input_embedding).to(\"cuda\")\n",
    "        generation_embedding = torch.tensor(generation_embedding).to(\"cuda\")\n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        cos_sim = cos(input_embedding, generation_embedding).item()\n",
    "        similarity_score = ((mean_distance + cos_sim) / 2).item()\n",
    "\n",
    "        print(f\"Temperature: {temp} & TopK: {k}\")\n",
    "        print(f\"Mean exemplar cosine similarity: {mean_distance}\")\n",
    "        print(f\"Original cosine similarity: {cos_sim}\")\n",
    "        print(f\"Similarity Score: {similarity_score}\")\n",
    "        print(f\"Generation: {generation}\")\n",
    "        print()\n",
    "\n",
    "        results[\"temperature\"].append(temp)\n",
    "        results[\"top_k\"].append(k)\n",
    "        results[\"mean_distance\"].append(mean_distance)\n",
    "        results[\"cosine_similarity\"].append(cos_sim)\n",
    "        results[\"similarity_score\"].append(similarity_score)\n",
    "        results[\"generation\"].append(generation)\n",
    "\n",
    "results = pd.DataFrame(results).to_csv(\"results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
