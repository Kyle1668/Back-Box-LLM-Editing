{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pointbiserialr\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tqdm.pandas()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "aug_regex = re.compile(r\"<aug>(.*?)</aug>\", re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting constants\n",
    "TITLE_FONT_SIZE = 16\n",
    "WSPACE = 0.3\n",
    "FIGURE_HEIGHT = 4\n",
    "LINE_WIDTH = 2\n",
    "FIG_SIZE = 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_logs = load_from_disk(\"data/combined_dataset\")\n",
    "list(inference_logs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_task_name(split_name):\n",
    "    return \"Sentiment\" if \"Sentiment\" in split_name else \"Toxicity\" if \"Toxicity\" in split_name else \"News\"\n",
    "\n",
    "def parse_distribution(split_name):\n",
    "    return split_name.split(\"_\")[-3]\n",
    "\n",
    "def parse_model(split_name):\n",
    "    return split_name.split(\"_\")[-2]\n",
    "\n",
    "def parse_tta_method(split_name):\n",
    "    return split_name.split(\"_\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No TTA Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_tta_accuracies = {}\n",
    "for split_name in tqdm(inference_logs):\n",
    "    split_frame = inference_logs[split_name].to_pandas()\n",
    "    split_no_tta_accuracy = classification_report(split_frame[\"label\"], split_frame[\"original_predicted_class\"], output_dict=True)[\"accuracy\"]\n",
    "    no_tta_accuracies[split_name] = split_no_tta_accuracy\n",
    "\n",
    "no_tta_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Main Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results_bert_splits = [split for split in inference_logs.keys() if \"Ablate\" not in split]\n",
    "datasets = [\"BOSS_Sentiment\", \"BOSS_Toxicity\", \"AgNewsTweets\"]\n",
    "split_data = {}\n",
    "\n",
    "for task_name in datasets:\n",
    "    if task_name not in split_data:\n",
    "        split_data[task_name] = {}\n",
    "\n",
    "    for split in tqdm(main_results_bert_splits):\n",
    "        if task_name in split:\n",
    "            distribution = split.split(\"_\")[-3]\n",
    "            model = split.split(\"_\")[-2]\n",
    "            tta_method = split.split(\"_\")[-1]\n",
    "            baseline_accuracy = classification_report(inference_logs[split][\"label\"], inference_logs[split][\"original_predicted_class\"], output_dict=True)[\"accuracy\"]\n",
    "            tta_accuracy = classification_report(inference_logs[split][\"label\"], inference_logs[split][\"tta_predicted_class\"], output_dict=True)[\"accuracy\"]\n",
    "            accuracy_delta = tta_accuracy - baseline_accuracy\n",
    "\n",
    "            if model not in split_data[task_name]:\n",
    "                split_data[task_name][model] = {}\n",
    "\n",
    "            if tta_method not in split_data[task_name][model]:\n",
    "                split_data[task_name][model][tta_method] = {}\n",
    "\n",
    "            split_data[task_name][model][tta_method][distribution] = {\n",
    "                \"distribution\": distribution,\n",
    "                \"model\": model,\n",
    "                \"tta_method\": tta_method,\n",
    "                \"baseline_accuracy\": baseline_accuracy,\n",
    "                \"tta_accuracy\": tta_accuracy,\n",
    "                \"accuracy_delta\": accuracy_delta\n",
    "            }\n",
    "\n",
    "    # get the Accuracy Gain for each method excluding ID\n",
    "    for model_name in split_data[task_name]:\n",
    "        for tta_method in split_data[task_name][model_name]:\n",
    "            accuracy_deltas = []\n",
    "            baseline_accuracies = []\n",
    "            for distribution in split_data[task_name][model_name][tta_method]:\n",
    "                if distribution == \"ID\":\n",
    "                    continue\n",
    "\n",
    "                current_tta_result = split_data[task_name][model_name][tta_method][distribution]\n",
    "                accuracy_deltas.append(current_tta_result[\"accuracy_delta\"])\n",
    "                baseline_accuracies.append(current_tta_result[\"baseline_accuracy\"])\n",
    "\n",
    "            split_data[task_name][model_name][tta_method][\"mean_accuracy_delta\"] = np.mean(accuracy_deltas)\n",
    "            split_data[task_name][model_name][tta_method][\"mean_baseline_accuracy\"] = np.mean(baseline_accuracies)\n",
    "\n",
    "print(json.dumps(split_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe where there is a column for dataset, model, id_accuracy_delta, and mean_accuracy_delta\n",
    "records = []\n",
    "\n",
    "for task_name in tqdm(split_data):\n",
    "    for model_name in split_data[task_name]:\n",
    "        for tta_method in split_data[task_name][model_name]:\n",
    "            current_tta_result = split_data[task_name][model_name][tta_method]\n",
    "            records.append({\n",
    "                \"dataset\": task_name,\n",
    "                \"model\": model_name,\n",
    "                \"tta_method\": tta_method,\n",
    "                \"id_accuracy_delta\": current_tta_result[\"ID\"][\"accuracy_delta\"] * 100 if \"ID\" in current_tta_result else None,\n",
    "                \"ood_mean_accuracy_delta\": current_tta_result[\"mean_accuracy_delta\"] * 100,\n",
    "                \"ood_mean_baseline_accuracy\": current_tta_result[\"mean_baseline_accuracy\"] * 100,\n",
    "            })\n",
    "\n",
    "\n",
    "main_results_frame = pd.DataFrame(records)\n",
    "for model in split_data[task_name]:\n",
    "    display(model)\n",
    "    display(main_results_frame[main_results_frame[\"model\"] == model].drop(columns=\"model\").groupby([\"dataset\", \"tta_method\"]).mean().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Across Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = [split for split in inference_logs.keys() if \"Ablate_Data\" in split]\n",
    "datasets = set([(\"_\".join(split.split(\"_\")[:2]).replace(\"_Ablate\", \"\")) for split in all_splits if \"Ablate_Data\" in split])\n",
    "results = {}\n",
    "# print(datasets)\n",
    "\n",
    "for task_name in datasets:\n",
    "    results[task_name] = {}\n",
    "\n",
    "    for split_name in tqdm(all_splits, desc=task_name):\n",
    "        if task_name in split_name:\n",
    "            data_count = int(split_name.split(\"_\")[-2].replace(\"BERT\", \"\"))\n",
    "            tta_method = split_name.split(\"_\")[-1]\n",
    "            shift_name = split_name.split(\"_\")[-3]\n",
    "            # print(dataset, data_count, tta_method, shift_name)\n",
    "\n",
    "            baseline_accuracy = classification_report(inference_logs[split_name][\"label\"], inference_logs[split_name][\"original_predicted_class\"], output_dict=True)[\"accuracy\"]\n",
    "            tta_accuracy = classification_report(inference_logs[split_name][\"label\"], inference_logs[split_name][\"tta_predicted_class\"], output_dict=True)[\"accuracy\"]\n",
    "\n",
    "            if task_name not in results:\n",
    "                results[task_name] = {}\n",
    "            if data_count not in results[task_name]:\n",
    "                results[task_name][data_count] = {}\n",
    "            if tta_method not in results[task_name][data_count]:\n",
    "                results[task_name][data_count][tta_method] = {}\n",
    "            if shift_name not in results[task_name][data_count][tta_method]:\n",
    "                results[task_name][data_count][tta_method][shift_name] = {}\n",
    "\n",
    "            results[task_name][data_count][tta_method][shift_name] = {\n",
    "                \"method\": tta_method,\n",
    "                \"baseline_accuracy\": baseline_accuracy,\n",
    "                \"tta_accuracy\": tta_accuracy,\n",
    "                \"baseline_delta\": tta_accuracy - baseline_accuracy,\n",
    "            }\n",
    "\n",
    "            # inference_frames[task_name] = inference_logs[split].to_pandas()\n",
    "            # break\n",
    "\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_avg_delta = {}\n",
    "for task_name in results:\n",
    "    for data_count in results[task_name]:\n",
    "        tta_perf_deltas = {}\n",
    "        for tta_method in results[task_name][data_count]:\n",
    "            for ood_shift in results[task_name][data_count][tta_method]:\n",
    "                if tta_method not in tta_perf_deltas:\n",
    "                    tta_perf_deltas[tta_method] = []\n",
    "\n",
    "                shift_method_perf_deta = results[task_name][data_count][tta_method][ood_shift][\"baseline_delta\"]\n",
    "                tta_perf_deltas[tta_method].append(shift_method_perf_deta)\n",
    "\n",
    "        if task_name not in method_avg_delta:\n",
    "            method_avg_delta[task_name] = {}\n",
    "\n",
    "        method_avg_delta[task_name][data_count] = { tta_method: np.mean(tta_perf_deltas[tta_method]) for tta_method in tta_perf_deltas }\n",
    "        baseline = np.mean([method_avg_delta[task_name][data_count][\"Insert\"], method_avg_delta[task_name][data_count][\"Substitute\"], method_avg_delta[task_name][data_count][\"Translate\"]])\n",
    "        method_avg_delta[task_name][data_count][\"Conventional Augmentation\"] = baseline\n",
    "\n",
    "# display(method_avg_delta)\n",
    "\n",
    "pandas_form = {task_name: {} for task_name in results}\n",
    "for task_name in pandas_form:\n",
    "    for data_count in method_avg_delta[task_name]:\n",
    "        for tta_method in [\"Conventional Augmentation\", \"Paraphrase\", \"ICR\"]:\n",
    "            if data_count not in pandas_form[task_name]:\n",
    "                pandas_form[task_name][data_count] = []\n",
    "\n",
    "            pandas_form[task_name][data_count].append({\n",
    "                \"data_count\": data_count,\n",
    "                \"tta_method\": tta_method,\n",
    "                \"avg_delta\": method_avg_delta[task_name][data_count][tta_method],\n",
    "            })\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(3 * FIG_SIZE, FIG_SIZE))\n",
    "for i, task_name in enumerate([\"BOSS_Sentiment\", \"BOSS_Toxicity\", \"AgNewsTweets\"]):\n",
    "    df = pd.concat([pd.DataFrame(pandas_form[task_name][data_count]) for data_count in pandas_form[task_name]])\n",
    "    sns.lineplot(\n",
    "        data=df,\n",
    "        x=\"data_count\",\n",
    "        y=\"avg_delta\",\n",
    "        hue=\"tta_method\",\n",
    "        ax=axes[i],\n",
    "        linewidth=LINE_WIDTH,\n",
    "        marker=\"o\",\n",
    "        markersize=5)\n",
    "\n",
    "    # set x label to Training Set Size\n",
    "    axes[i].set_xlabel(\"Training Set Size\")\n",
    "\n",
    "    # set y label to Mean Absolute Accuracy Delta\n",
    "    axes[i].set_ylabel(\"Accuracy Gain\")\n",
    "\n",
    "    # make the y axis percents that go to the hundreds place\n",
    "    axes[i].yaxis.set_major_formatter(lambda x, pos: f\"{x:.0%}\")\n",
    "\n",
    "    # standardize the y axis\n",
    "    axes[i].set_ylim(-0.015, 0.1)\n",
    "\n",
    "    title_text = {\n",
    "        \"BOSS_Sentiment\": \"Sentiment\",\n",
    "        \"BOSS_Toxicity\": \"Toxicity\",\n",
    "        \"AgNewsTweets\": \"News\",\n",
    "    }\n",
    "    axes[i].set_title(title_text[task_name], fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "    # se legend to the bottom left\n",
    "    axes[i].legend(loc=\"lower right\")\n",
    "\n",
    "    # x axis ticks are five equally spaced ticks between the min and max of the x axis\n",
    "    axes[i].set_xticks(np.linspace(df[\"data_count\"].min(), df[\"data_count\"].max(), 4))\n",
    "\n",
    "    # remove leegnd in not middle plot\n",
    "    if i != 1:\n",
    "        axes[i].get_legend().remove()\n",
    "    else:\n",
    "        # center below plot with no frame\n",
    "        axes[i].legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.20), ncol=5, frameon=False, fontsize=14)\n",
    "\n",
    "# add padding for labels\n",
    "fig.subplots_adjust(wspace=WSPACE, hspace=WSPACE)\n",
    "\n",
    "# plt.tight_layout()\n",
    "if not os.path.exists(\"figures/\"):\n",
    "    os.makedirs(\"figures/\")\n",
    "fig.savefig(\"figures/method_analysis_data_ablation.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_sentiment_icr_data = None\n",
    "ood_toxicity_icr_data = None\n",
    "ood_tweets_icr_data = None\n",
    "for split_name in tqdm(inference_logs.keys()):\n",
    "    current_frame = inference_logs[split_name].to_pandas()\n",
    "    current_frame[\"dataset\"] = split_name\n",
    "\n",
    "    if \"Sentiment\" in split_name and \"ICR\" in split_name and \"ID\" not in split_name and \"BERT\" in split_name and \"Ablate\" not in split_name:\n",
    "        if ood_sentiment_icr_data is None:\n",
    "            ood_sentiment_icr_data = current_frame\n",
    "        else:\n",
    "            ood_sentiment_icr_data = pd.concat([ood_sentiment_icr_data, current_frame])\n",
    "\n",
    "    if \"Toxicity\" in split_name and \"ICR\" in split_name and \"ID\" not in split_name and \"BERT\" in split_name and \"Ablate\" not in split_name:\n",
    "        if ood_toxicity_icr_data is None:\n",
    "            ood_toxicity_icr_data = current_frame\n",
    "        else:\n",
    "            ood_toxicity_icr_data = pd.concat([ood_toxicity_icr_data, current_frame])\n",
    "\n",
    "    if \"Tweets\" in split_name and \"ICR\" in split_name and \"ID\" not in split_name and \"BERT\" in split_name and \"Ablate\" not in split_name:\n",
    "        if ood_tweets_icr_data is None:\n",
    "            ood_tweets_icr_data = current_frame\n",
    "        else:\n",
    "            ood_tweets_icr_data = pd.concat([ood_tweets_icr_data, current_frame])\n",
    "\n",
    "\n",
    "display(ood_sentiment_icr_data.value_counts(\"dataset\"))\n",
    "display(ood_toxicity_icr_data.value_counts(\"dataset\"))\n",
    "display(ood_tweets_icr_data.value_counts(\"dataset\"))\n",
    "assert len(ood_sentiment_icr_data.value_counts(\"dataset\")) == 3\n",
    "assert len(ood_toxicity_icr_data.value_counts(\"dataset\")) == 3\n",
    "assert len(ood_tweets_icr_data.value_counts(\"dataset\")) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_predictions(predictions, num_predictions, use_test_input):\n",
    "    try:\n",
    "        ablation_preds = predictions[:num_predictions]\n",
    "        if use_test_input:\n",
    "            ablation_preds = ablation_preds.tolist() + [predictions[-1]]\n",
    "        \n",
    "        mean_distribution = np.mean(ablation_preds, axis=0) if len(ablation_preds) > 1 else ablation_preds[0]\n",
    "        predicted_class = np.argmax(mean_distribution)\n",
    "        return predicted_class\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "\n",
    "perf_records = []\n",
    "inference_ablation_splits = [split for split in inference_logs.keys() if \"Ablate\" not in split and \"BERT\" in split]\n",
    "for split_name in tqdm(inference_ablation_splits):\n",
    "    dataset = parse_task_name(split_name)\n",
    "    distribution = parse_distribution(split_name)\n",
    "    model = parse_model(split_name)\n",
    "    method = parse_tta_method(split_name)\n",
    "\n",
    "    current_frame = inference_logs[split_name].to_pandas()\n",
    "    for use_source in [False, True]:\n",
    "        for num_augmentations in range(1, 5):\n",
    "            judgments = current_frame[\"tta_all_class_probs\"].apply(lambda x: aggregate_predictions(x, num_augmentations, use_source))\n",
    "            accuracy = classification_report(current_frame[\"label\"], judgments, output_dict=True)[\"accuracy\"]\n",
    "            perf_records.append({\n",
    "                \"dataset\": dataset,\n",
    "                \"distribution\": distribution,\n",
    "                \"model\": model,\n",
    "                \"method\": method,\n",
    "                \"use_source\": use_source,\n",
    "                \"num_augmentations\": num_augmentations,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"delta\": accuracy - no_tta_accuracies[split_name],\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_ablation_frame = pd.DataFrame(perf_records)\n",
    "\n",
    "# select where distribution != ID\n",
    "aggregation_ablation_frame = aggregation_ablation_frame[aggregation_ablation_frame[\"distribution\"] != \"ID\"]\n",
    "\n",
    "# set method to TTA if not Paraphrase or ICR and LLM-TTA if Paraphrase or ICR\n",
    "# aggregation_ablation_frame[\"method\"] = aggregation_ablation_frame[\"method\"].apply(lambda x: \"TTA\" if x not in [\"Paraphrase\", \"ICR\"] else \"LLM-TTA\")\n",
    "\n",
    "# for tta_method in [\"Conventional Augmentation\", \"Paraphrase\", \"ICR\"]:\n",
    "aggregation_ablation_frame[\"method\"] = aggregation_ablation_frame[\"method\"].apply(lambda x: \"ICR\" if \"ICR\" in x else \"Paraphrase\" if \"Paraphrase\" in x else \"Conventional Augmentation\")\n",
    "\n",
    "# create three figures, one for Sentiment, Toxicity, and News\n",
    "fig, axes = plt.subplots(1, 3, figsize=(3 * FIG_SIZE, FIG_SIZE))\n",
    "for index, task_name in enumerate([\"Sentiment\", \"Toxicity\", \"News\"]):\n",
    "    # get the current task frame\n",
    "    current_frame = aggregation_ablation_frame[aggregation_ablation_frame[\"dataset\"] == task_name]\n",
    "\n",
    "    # where use_source is True\n",
    "    current_frame_source = current_frame[current_frame[\"use_source\"] == True]\n",
    "\n",
    "    # plot the current task frame\n",
    "    sns.lineplot(\n",
    "        data=current_frame,\n",
    "        x=\"num_augmentations\",\n",
    "        y=\"delta\",\n",
    "        hue=\"method\",\n",
    "        # style=\"use_source\",\n",
    "        ax=axes[index],\n",
    "        linewidth=LINE_WIDTH,\n",
    "        ci=None,\n",
    "        marker=\"o\",\n",
    "        markersize=5)\n",
    "\n",
    "    # set x label to Number of Augmentations\n",
    "    axes[index].set_xlabel(\"Number of Augmentations\")\n",
    "\n",
    "    # set y label to Accuracy\n",
    "    axes[index].set_ylabel(\"Accuracy Gain\")\n",
    "\n",
    "    # make the y axis percents that go to the hundreds place\n",
    "    axes[index].yaxis.set_major_formatter(lambda x, pos: f\"{x:.0%}\")\n",
    "\n",
    "    # set title to Sentiment, Toxicity, or News\n",
    "    axes[index].set_title(task_name, fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "    # se legend to the bottom left\n",
    "    axes[index].legend(loc=\"lower right\")\n",
    "\n",
    "    # standardize the y axis\n",
    "    axes[i].set_ylim(-0.015, 0.15)\n",
    "\n",
    "    # x axis ticks are five equally spaced ticks between the min and max of the x axis\n",
    "    axes[index].set_xticks(np.linspace(current_frame[\"num_augmentations\"].min(), current_frame[\"num_augmentations\"].max(), 4))\n",
    "\n",
    "    # remove leegnd in not middle plot\n",
    "    if index != 1:\n",
    "        axes[index].get_legend().remove()\n",
    "    else:\n",
    "        # center below plot with no frame\n",
    "        axes[index].legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.20), ncol=3, frameon=False, fontsize=14)\n",
    "    \n",
    "    # have y ticks to the tenths place\n",
    "    axes[index].yaxis.set_major_formatter(lambda x, pos: f\"{x:.1%}\")\n",
    "\n",
    "# add padding for labels\n",
    "fig.subplots_adjust(wspace=WSPACE + 0.05, hspace=WSPACE)\n",
    "\n",
    "fig.savefig(\"figures/method_analysis_aggrgeation_ablation.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't crop the display frame\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "aggregation_ablation_frame = pd.DataFrame(perf_records)\n",
    "for dataset in [\"Sentiment\", \"Toxicity\", \"News\"]:\n",
    "    dataset_frame = aggregation_ablation_frame[aggregation_ablation_frame[\"dataset\"] == dataset]\n",
    "    dataset_frame[\"distribution\"] = dataset_frame[\"distribution\"].apply(lambda x: \"ID\" if \"ID\" in x else \"OOD\")\n",
    "    display(dataset_frame.groupby([\"dataset\", \"distribution\", \"model\", \"method\", \"use_source\", \"num_augmentations\"]).mean().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does TTA Effect Some Classes More Than Others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataset, get the percent of examples that are unchanged vs new\n",
    "sentiment_outcomes = ood_sentiment_icr_data[\"outcome\"]\n",
    "sentiment_percents = sentiment_outcomes.value_counts(normalize=True)\n",
    "new_sentiment_percents = 100 * sentiment_percents[sentiment_percents.index == \"New Correct\"].values[0] + sentiment_percents[sentiment_percents.index == \"New Mistake\"].values[0]\n",
    "print(f\"Sentiment: {new_sentiment_percents:.2f}% of examples are new predictions\")\n",
    "\n",
    "toxicity_outcomes = ood_toxicity_icr_data[\"outcome\"]\n",
    "toxicity_percents = toxicity_outcomes.value_counts(normalize=True)\n",
    "new_toxicity_percents = 100 * toxicity_percents[toxicity_percents.index == \"New Correct\"].values[0] + toxicity_percents[toxicity_percents.index == \"New Mistake\"].values[0]\n",
    "print(f\"Toxicity: {new_toxicity_percents:.2f}% of examples are new predictions\")\n",
    "\n",
    "agt_outcomes = ood_tweets_icr_data[\"outcome\"]\n",
    "agt_percents = agt_outcomes.value_counts(normalize=True)\n",
    "new_agt_percents = 100 * agt_percents[agt_percents.index == \"New Correct\"].values[0] + agt_percents[agt_percents.index == \"New Mistake\"].values[0]\n",
    "print(f\"AGT: {new_agt_percents:.2f}% of examples are new predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_icr_outcome_percents = ood_sentiment_icr_data[[\"outcome\", \"label\"]].sort_values(\"outcome\").value_counts([\"outcome\", \"label\"], normalize=True).sort_index()\n",
    "sentiment_icr_outcome_percents = sentiment_icr_outcome_percents[sentiment_icr_outcome_percents.index.get_level_values(\"outcome\").str.contains(\"New\")]\n",
    "print(\"Sentiment ICR\")\n",
    "display(sentiment_icr_outcome_percents)\n",
    "\n",
    "toxicity_icr_outcome_percents = ood_toxicity_icr_data[[\"outcome\", \"label\"]].sort_values(\"outcome\").value_counts([\"outcome\", \"label\"], normalize=True).sort_index()\n",
    "toxicity_icr_outcome_percents = toxicity_icr_outcome_percents[toxicity_icr_outcome_percents.index.get_level_values(\"outcome\").str.contains(\"New\")]\n",
    "print(\"Toxicity ICR\")\n",
    "display(toxicity_icr_outcome_percents)\n",
    "\n",
    "tweets_icr_outcome_percents = ood_tweets_icr_data[[\"outcome\", \"label\"]].sort_values(\"outcome\").value_counts([\"outcome\", \"label\"], normalize=True).sort_index()\n",
    "tweets_icr_outcome_percents = tweets_icr_outcome_percents[tweets_icr_outcome_percents.index.get_level_values(\"outcome\").str.contains(\"New\")]\n",
    "print(\"Tweets ICR\")\n",
    "display(tweets_icr_outcome_percents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig, axes = plt.subplots(1, 3, figsize=(3 * FIG_SIZE, FIG_SIZE))\n",
    "\n",
    "sentiment_labels = {\n",
    "    0: \"Negative\",\n",
    "    1: \"Positive\",\n",
    "    2: \"Neutral\",\n",
    "}\n",
    "sns.barplot(ax=axes[0], \n",
    "            x=sentiment_icr_outcome_percents.index.get_level_values(\"label\").map(sentiment_labels), \n",
    "            y=sentiment_icr_outcome_percents.values,\n",
    "            hue=sentiment_icr_outcome_percents.index.get_level_values(\"outcome\"),\n",
    ")\n",
    "\n",
    "toxicity_labels = {\n",
    "    0: \"Non-Toxic\",\n",
    "    1: \"Toxic\",\n",
    "}\n",
    "sns.barplot(ax=axes[1],\n",
    "            x=toxicity_icr_outcome_percents.index.get_level_values(\"label\").map(toxicity_labels),\n",
    "            y=toxicity_icr_outcome_percents.values,\n",
    "            hue=toxicity_icr_outcome_percents.index.get_level_values(\"outcome\"),\n",
    ")\n",
    "\n",
    "agt_labels = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\",\n",
    "}\n",
    "sns.barplot(ax=axes[2],\n",
    "            x=tweets_icr_outcome_percents.index.get_level_values(\"label\").map(agt_labels),\n",
    "            y=tweets_icr_outcome_percents.values,\n",
    "            hue=tweets_icr_outcome_percents.index.get_level_values(\"outcome\"),\n",
    ")\n",
    "\n",
    "for i in range(3):\n",
    "    # make y axis percents\n",
    "    axes[i].yaxis.set_major_formatter(lambda x, pos: f\"{x:.0%}\")\n",
    "\n",
    "    # standardize between 0 and 0.1\n",
    "    axes[i].set_ylim(0, 0.09)\n",
    "\n",
    "    # set y label to percent of overall outcomes\n",
    "    axes[i].set_ylabel(\"Percent of Outcomes\")\n",
    "\n",
    "    # set x label to Class\n",
    "    axes[i].set_xlabel(\"Class\")\n",
    "\n",
    "    # have fewer ticks on the y axis\n",
    "    axes[i].locator_params(axis=\"y\", nbins=5)\n",
    "\n",
    "    # set titles\n",
    "    title_text = {\n",
    "        0: \"Sentiment\",\n",
    "        1: \"Toxicity\",\n",
    "        2: \"News\",\n",
    "    }\n",
    "    axes[i].set_title(title_text[i], fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "    # have a single legend which is centered below the plot\n",
    "    if i == 1:\n",
    "        axes[i].legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.20), ncol=3, frameon=False, fontsize=14)\n",
    "    else:\n",
    "        axes[i].get_legend().remove()\n",
    "\n",
    "# add more horizental spacing for y labels\n",
    "fig.subplots_adjust(wspace=WSPACE)\n",
    "fig.savefig(\"figures/method_analysis_class_analysis.png\", bbox_inches=\"tight\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three figures on single line\n",
    "fig, axes = plt.subplots(1, 2, figsize=(3 * FIG_SIZE, FIG_SIZE))\n",
    "plottng_datas = [(\"Sentiment\", ood_sentiment_icr_data), (\"Toxicity\", ood_toxicity_icr_data), (\"News\", ood_tweets_icr_data)]\n",
    "change_int_mapping = {\n",
    "    \"Unchanged\": 0,\n",
    "    \"New\": 1,\n",
    "}\n",
    "outcome_int_mapping = {\n",
    "    \"Unchanged Correct\": 0,\n",
    "    \"Unfixed Mistake\": 1,\n",
    "    \"New Correct\": 2,\n",
    "    \"New Mistake\": 3,\n",
    "}\n",
    "\n",
    "change_correlations = []\n",
    "outcome_correlations = []\n",
    "for index, (title, icr_frame) in enumerate(plottng_datas):\n",
    "    icr_frame[\"max_softmax\"] = icr_frame[\"tta_mean_class_probs\"].apply(lambda p: max(p))\n",
    "    plotting_frame = icr_frame[[\"max_softmax\", \"original_prediction_entropy\", \"tta_prediction_entropy\", \"outcome\"]].sort_values(\"outcome\")\n",
    "    plotting_frame[\"Change\"] = plotting_frame[\"outcome\"].apply(lambda x: 0 if x == \"Unchanged Correct\" or x == \"Unfixed Mistake\" else 1)\n",
    "\n",
    "    correlation_metric = \"original_prediction_entropy\"\n",
    "    pbc = pointbiserialr(plotting_frame[correlation_metric], plotting_frame[\"Change\"])\n",
    "    change_correlations.append([pbc[0]])\n",
    "\n",
    "    set_outcome_correlations = []\n",
    "    for outcome in outcome_int_mapping:\n",
    "        one_hot_outcomes = plotting_frame[\"outcome\"].apply(lambda x: 1 if x == outcome else 0)\n",
    "        pbc = pointbiserialr(plotting_frame[correlation_metric], one_hot_outcomes)\n",
    "        set_outcome_correlations.append(pbc[0])\n",
    "    \n",
    "    outcome_correlations.append(set_outcome_correlations)\n",
    "\n",
    "sns.heatmap(change_correlations,\n",
    "            annot=True,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            ax=axes[0],\n",
    "            cmap=\"coolwarm\",\n",
    "            xticklabels=[\"Prediction Changed\"],\n",
    "            yticklabels=[\"Sentiment\", \"Toxicity\", \"News\"])\n",
    "\n",
    "sns.heatmap(outcome_correlations,\n",
    "            annot=True,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            ax=axes[1],\n",
    "            cmap=\"coolwarm\",\n",
    "            xticklabels=[\"Unchanged Correct\", \"Unfixed Mistake\", \"New Correct\", \"New Mistake\"],\n",
    "            yticklabels=[\"Sentiment\", \"Toxicity\", \"News\"])\n",
    "\n",
    "plt.xticks(rotation=20, ha=\"right\")\n",
    "fig.subplots_adjust(wspace=WSPACE / 2)\n",
    "fig.savefig(\"figures/method_analysis_entropy_correlations.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three figures on single line\n",
    "fig, axes = plt.subplots(1, 3, figsize=(3 * FIG_SIZE, FIG_SIZE))\n",
    "plottng_datas = [(\"Sentiment\", ood_sentiment_icr_data), (\"Toxicity\", ood_toxicity_icr_data), (\"News\", ood_tweets_icr_data)]\n",
    "\n",
    "correlations = []\n",
    "\n",
    "for index, (title, icr_frame) in tqdm(enumerate(plottng_datas)):\n",
    "    axes[index].set_title(title, fontsize=TITLE_FONT_SIZE)\n",
    "    icr_frame[\"max_softmax\"] = icr_frame[\"tta_mean_class_probs\"].apply(lambda p: max(p))\n",
    "    plotting_frame = icr_frame[[\"max_softmax\", \"original_prediction_entropy\", \"tta_prediction_entropy\", \"outcome\"]].sort_values(\"outcome\")\n",
    "    plotting_frame[\"Prediction\"] = plotting_frame[\"outcome\"].apply(lambda x: \"Unchanged Prediction\" if x == \"Unchanged Correct\" or x == \"Unfixed Mistake\" else \"New Prediction\")\n",
    "\n",
    "    # create kernal density estimate plot for original prediction entropy by outcome\n",
    "    sns.kdeplot(\n",
    "        data=plotting_frame,\n",
    "        x=\"original_prediction_entropy\",\n",
    "        hue=\"Prediction\",\n",
    "        fill=True,\n",
    "        ax=axes[index],\n",
    "        linewidth=LINE_WIDTH,\n",
    "        # alpha=0.5,\n",
    "        common_norm=False,\n",
    "    )\n",
    "\n",
    "    # log y and x axis\n",
    "    # axes[index].set_yscale(\"log\")\n",
    "    axes[index].set_xscale(\"log\")\n",
    "\n",
    "    # move legend to the top left\n",
    "    if index == 1:\n",
    "        axes[index].legend(loc=\"upper center\", labels=[\"Unchanged\", \"New\"], bbox_to_anchor=(0.5, -0.20), ncol=2, frameon=False, fontsize=14)\n",
    "    else:\n",
    "        axes[index].get_legend().remove()\n",
    "\n",
    "fig.subplots_adjust(wspace=WSPACE)\n",
    "fig.savefig(\"figures/method_analysis_entropy_distributions.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Optimal ID Entropy Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_augment_entropy(threshold, row):\n",
    "    return row[\"original_prediction_entropy\"] >= threshold\n",
    "\n",
    "\n",
    "def get_entropy_threshold_accuracy(threshold, inference_logs_frame):\n",
    "    threshold_judgments = inference_logs_frame.apply(lambda row: row[\"tta_predicted_class\"] if should_augment_entropy(threshold, row) else row[\"original_predicted_class\"], axis=1)\n",
    "    report = classification_report(inference_logs_frame[\"label\"], threshold_judgments, digits=4, output_dict=True, zero_division=0)\n",
    "    llm_call_count = inference_logs_frame.apply(lambda row: should_augment_entropy(threshold, row), axis=1).sum()\n",
    "    llm_call_rate = llm_call_count / len(inference_logs_frame)\n",
    "    return report[\"accuracy\"], llm_call_rate\n",
    "\n",
    "\n",
    "def should_augment_softmax(threshold, row):\n",
    "    try:\n",
    "        return row[\"tta_all_class_probs\"][-1].max() < threshold\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_max_softmax_threshold_accuracy(threshold, inference_logs_frame):\n",
    "    threshold_judgments = inference_logs_frame.apply(lambda row: row[\"tta_predicted_class\"] if should_augment_softmax(threshold, row) else row[\"original_predicted_class\"], axis=1)\n",
    "    report = classification_report(inference_logs_frame[\"label\"], threshold_judgments, digits=4, output_dict=True, zero_division=0)\n",
    "    llm_call_count = (inference_logs_frame[\"original_prediction_entropy\"] >= threshold).sum()\n",
    "    llm_call_rate = llm_call_count / len(inference_logs_frame)\n",
    "    return report[\"accuracy\"], llm_call_rate\n",
    "\n",
    "thresholds = np.arange(0, 1.2, 0.0001)\n",
    "print(f\"Number of thresholds: {len(thresholds)}\")\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOD Entropy Threshold Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate all the tresholds for each OOD split for BERT ICR. \n",
    "# 2. Get the manual threshold for each OOD split for BERT ICR at different augmentation rates.\n",
    "\n",
    "thresholds_dict = {}\n",
    "thresholds_path = f\"data/thresholds_dict_{len(thresholds)}.json\"\n",
    "if not os.path.exists(thresholds_path):\n",
    "    for ood_icr_data in [ood_sentiment_icr_data, ood_toxicity_icr_data, ood_tweets_icr_data]:\n",
    "        for split in ood_icr_data[\"dataset\"].unique():\n",
    "            print(split)\n",
    "            thresholds_dict[split] = {}\n",
    "            split_frame = ood_icr_data[ood_icr_data[\"dataset\"] == split]\n",
    "            original_accuracy = classification_report(split_frame[\"label\"], split_frame[\"original_predicted_class\"], output_dict=True)[\"accuracy\"]\n",
    "            \n",
    "            for threshold in tqdm(thresholds):\n",
    "                accuracy, llm_call_rate = get_entropy_threshold_accuracy(threshold, split_frame)\n",
    "                thresholds_dict[split][threshold] = {\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"accuracy_delta\": accuracy - original_accuracy,\n",
    "                    \"llm_call_rate\": llm_call_rate,\n",
    "                }\n",
    "    json.dump(thresholds_dict, open(thresholds_path, \"w\"), indent=4)\n",
    "else:\n",
    "    with open(\"data/thresholds_dict.json\", \"r\") as f:\n",
    "        thresholds_dict = json.load(f)\n",
    "\n",
    "print(json.dumps(thresholds_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subplot for each dataset in the thresholds dict with three on each row\n",
    "fig, axes = plt.subplots(3, 3, figsize=(3 * FIG_SIZE, 3 * FIG_SIZE))\n",
    "\n",
    "# use the first row for sentiment, second for toxicity, and third for agt\n",
    "task_keys = [\n",
    "    [key for key in thresholds_dict.keys() if \"Sentiment\" in key],\n",
    "    [key for key in thresholds_dict.keys() if \"Toxicity\" in key],\n",
    "    [key for key in thresholds_dict.keys() if \"Tweets\" in key],\n",
    "]\n",
    "for row_index, task_splits in enumerate(task_keys):\n",
    "    for col_index in range(len(task_splits)):\n",
    "        split_name = task_splits[col_index]\n",
    "        thresholds_split_frame = pd.DataFrame(thresholds_dict[split_name]).T.reset_index().sort_values(\"llm_call_rate\")\n",
    "        sns.lineplot(\n",
    "            ax=axes[row_index, col_index],\n",
    "            data=thresholds_split_frame,\n",
    "            x=\"llm_call_rate\",\n",
    "            y=\"accuracy_delta\",\n",
    "            linewidth=LINE_WIDTH)\n",
    "\n",
    "        row_titles = {\n",
    "            0: \"Sentiment\",\n",
    "            1: \"Toxicity\",\n",
    "            2: \"News\",\n",
    "        }\n",
    "        shift_name = split_name.split(\"_\")[-3]\n",
    "        axes[row_index, col_index].set_title(f\"{row_titles[row_index]}: {shift_name}\", fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "for col_index in range(3):\n",
    "    for row_index in range(3):\n",
    "        axes[row_index, col_index].set_ylabel(\"Accuracy Delta\")\n",
    "        axes[row_index, col_index].set_xlabel(\"Augmentation Rate\")\n",
    "\n",
    "        # set x and y axis to percents\n",
    "        axes[row_index, col_index].xaxis.set_major_formatter(lambda x, pos: f\"{x:.0%}\")\n",
    "\n",
    "        # multiple delta by 100 to get percent\n",
    "        axes[row_index, col_index].yaxis.set_major_formatter(lambda x, pos: f\"{round(x * 100, 2)}\")\n",
    "\n",
    "        # set y axis between -0.1 and 0.1\n",
    "        # axes[row_index, col_index].set_ylim(-0.01, 0.15)\n",
    "\n",
    "        # have five ticks on the y axis\n",
    "        axes[row_index, col_index].locator_params(axis=\"y\", nbins=8)\n",
    "\n",
    "        # delete last two plots on the final row\n",
    "        if row_index == 2 and col_index > 0:\n",
    "            axes[row_index, col_index].remove()\n",
    "\n",
    "# add padding for labels\n",
    "fig.subplots_adjust(wspace=WSPACE, hspace=WSPACE + 0.25)\n",
    "\n",
    "# save figure\n",
    "fig.savefig(\"figures/method_analysis_all_entropy_thresholds.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_sentiment_thresholds = {}\n",
    "for split in thresholds_dict:\n",
    "    task = \"Sentiment\" if \"Sentiment\" in split else \"Toxicity\" if \"Toxicity\" in split else \"News\"\n",
    "    if task not in aggregated_sentiment_thresholds:\n",
    "        aggregated_sentiment_thresholds[task] = {}\n",
    "\n",
    "    for threshold in thresholds_dict[split]:\n",
    "        if threshold not in aggregated_sentiment_thresholds:\n",
    "            aggregated_sentiment_thresholds[task][threshold] = {\n",
    "                \"accuracy_delta\": 0,\n",
    "                \"llm_call_rate\": 0,\n",
    "            }\n",
    "\n",
    "        aggregated_sentiment_thresholds[task][threshold][\"accuracy_delta\"] += thresholds_dict[split][threshold][\"accuracy_delta\"]\n",
    "        aggregated_sentiment_thresholds[task][threshold][\"llm_call_rate\"] += thresholds_dict[split][threshold][\"llm_call_rate\"]\n",
    "\n",
    "# divide each accuracy delta by the number of splits to get the average\n",
    "for task in aggregated_sentiment_thresholds:\n",
    "    for threshold in aggregated_sentiment_thresholds[task]:\n",
    "        aggregated_sentiment_thresholds[task][threshold][\"accuracy_delta\"] /= 3\n",
    "        # aggregated_sentiment_thresholds[task][threshold][\"llm_call_rate\"] /= 3\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(3 * FIG_SIZE, FIG_SIZE))\n",
    "for col_index, task in enumerate(aggregated_sentiment_thresholds):\n",
    "    sns.lineplot(\n",
    "        ax=axes[col_index],\n",
    "        data=pd.DataFrame(aggregated_sentiment_thresholds[task]).T.reset_index().sort_values(\"llm_call_rate\"),\n",
    "        x=\"llm_call_rate\",\n",
    "        y=\"accuracy_delta\",\n",
    "        linewidth=LINE_WIDTH)\n",
    "    \n",
    "\n",
    "    axes[col_index].set_title(task, fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "    # add padding for labels\n",
    "    axes[col_index].set_ylabel(\"Accuracy Delta\")\n",
    "    axes[col_index].set_xlabel(\"Augmentation Rate\")\n",
    "\n",
    "    # set x and y axis to percents\n",
    "    axes[col_index].xaxis.set_major_formatter(lambda x, pos: f\"{x:.0%}\")\n",
    "    axes[col_index].yaxis.set_major_formatter(lambda x, pos: f\"{round(x * 100, 2)}\")\n",
    "\n",
    "    # set y value between 0 and 0.1\n",
    "    # axes[col_index].set_ylim(-0.01, 0.08)\n",
    "\n",
    "    # have few ticks on the x axis\n",
    "    axes[col_index].locator_params(axis=\"x\", nbins=5)\n",
    "    axes[col_index].locator_params(axis=\"y\", nbins=5)\n",
    "\n",
    "fig.subplots_adjust(wspace=WSPACE, hspace=0.3)\n",
    "fig.savefig(\"figures/method_analysis_aggregated_entropy_thresholds.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_tta_preds(row):\n",
    "    if row[\"generations\"] is not None and len(row[\"generations\"]) > 0:\n",
    "        return row[\"generations\"][:5]\n",
    "    \n",
    "    if row[\"tta_all_class_probs\"] is None:\n",
    "        return None\n",
    "\n",
    "    all_probs = row[\"tta_all_class_probs\"][:5]\n",
    "    arg_maxes = [prob_dist.argmax() for prob_dist in all_probs]\n",
    "    return arg_maxes\n",
    "\n",
    "def is_entropy_split(split_name):\n",
    "    if \"BERT\" not in split_name:\n",
    "        return False\n",
    "    if \"Ablate\" in split_name:\n",
    "        return False\n",
    "    if \"ID\" in split_name:\n",
    "        return False\n",
    "\n",
    "    return \"Paraphrase\" in split_name or \"ICR\" in split_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_entropy_thresholds = {}\n",
    "optimal_softmax_thresholds = {}\n",
    "thresholds = np.arange(0, 1.2, 0.0005)\n",
    "SAMPLE_SIZE = 250\n",
    "print(f\"Number of thresholds: {len(thresholds)}\")\n",
    "\n",
    "for split in [dataset for dataset in inference_logs if is_entropy_split(dataset)]:\n",
    "    print(split)\n",
    "    best_entropy_threshold = None\n",
    "    best_softmax_threshold = None\n",
    "    split_frame = inference_logs[split].to_pandas()\n",
    "    sample_frame = None\n",
    "    unique_predicted_classes = [class_label for class_label in split_frame[\"tta_predicted_class\"].unique() if class_label != -1] \n",
    "    for class_prediction in unique_predicted_classes:\n",
    "        sample_size = SAMPLE_SIZE // len(unique_predicted_classes)\n",
    "        class_sample_frame = split_frame[split_frame[\"tta_predicted_class\"] == class_prediction].sample(sample_size, random_state=42)\n",
    "        if sample_frame is None:\n",
    "            sample_frame = class_sample_frame\n",
    "        else:\n",
    "            sample_frame = pd.concat([sample_frame, class_sample_frame])\n",
    "    \n",
    "    threshold_performances = []\n",
    "    for threshold in tqdm(thresholds):\n",
    "        accuracy, llm_call_rate = get_entropy_threshold_accuracy(threshold, sample_frame)\n",
    "        beta = 1/500\n",
    "        rate_term = 1 - llm_call_rate\n",
    "        threshold_score = (1 + beta ** 2) * ((accuracy * rate_term) / ((beta ** 2) * accuracy + rate_term))\n",
    "        threshold_perf = {\n",
    "            \"threshold\": threshold,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"score\": threshold_score,\n",
    "            \"llm_call_rate\": f\"{llm_call_rate:.2f}%\",\n",
    "        }\n",
    "        threshold_performances.append(threshold_perf)\n",
    "\n",
    "        # if best_entropy_threshold is None or accuracy > best_entropy_threshold[\"accuracy\"]:\n",
    "        if best_entropy_threshold is None or threshold_score > best_entropy_threshold[\"score\"]:\n",
    "            best_entropy_threshold = threshold_perf\n",
    "\n",
    "    pd.DataFrame(threshold_performances).to_csv(f\"data/threshold_performances_{split}.csv\", index=False)\n",
    "    optimal_entropy_thresholds[split] = best_entropy_threshold\n",
    "    print(f\"Best Entropy Threshold: {best_entropy_threshold}\")\n",
    "\n",
    "# print(json.dumps(optimal_entropy_thresholds, indent=4))\n",
    "# print(json.dumps(optimal_softmax_thresholds, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_records = []\n",
    "\n",
    "# display(id_logs)\n",
    "# display(split_names)\n",
    "for split_name in tqdm(optimal_entropy_thresholds):\n",
    "    split_logs = inference_logs[split_name].to_pandas()\n",
    "\n",
    "    perf_records.append({\n",
    "        \"split\": split_name,\n",
    "        \"tta\": \"None\",\n",
    "        \"accuracy\": classification_report(split_logs[\"label\"], split_logs[\"original_predicted_class\"], digits=4, zero_division=0, output_dict=True)[\"accuracy\"],\n",
    "    })\n",
    "\n",
    "    optimal_entropy_threshold = optimal_entropy_thresholds[split_name][\"threshold\"]\n",
    "    accuracy = get_entropy_threshold_accuracy(optimal_entropy_threshold, split_logs)[0]\n",
    "    perf_records.append({\n",
    "        \"split\": split_name,\n",
    "        \"tta\": \"entropy-based\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"augmentation_rate\": split_logs.apply(lambda row: should_augment_entropy(optimal_entropy_threshold, row), axis=1).sum() / len(split_logs),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_frame = pd.DataFrame(perf_records)\n",
    "results_frame[\"Dataset\"] = results_frame[\"split\"].apply(lambda s: s.split(\"_\")[-4])\n",
    "results_frame[\"Distribution\"] = results_frame[\"split\"].apply(lambda s: s.split(\"_\")[-3])\n",
    "results_frame[\"Model\"] = results_frame[\"split\"].apply(lambda s: s.split(\"_\")[-2])\n",
    "results_frame[\"TTA Method\"] = results_frame[\"split\"].apply(lambda s: s.split(\"_\")[-1])\n",
    "results_frame[\"Baseline Delta\"] = results_frame.apply(lambda row: row[\"accuracy\"] - results_frame[(results_frame[\"split\"] == row[\"split\"]) & (results_frame[\"tta\"] == \"None\")][\"accuracy\"].values[0], axis=1)\n",
    "results_frame.drop(columns=[\"split\"], inplace=True)\n",
    "results_frame.rename(columns={\"tta\": \"Selective Method\", \"accuracy\": \"Accuracy\", \"augmentation_rate\": \"Augmentation Rate\"}, inplace=True)\n",
    "\n",
    "aggregated_results = results_frame.groupby([\"Dataset\", \"Distribution\", \"Model\", \"TTA Method\", \"Selective Method\"]).mean().round(4) * 100\n",
    "aggregated_results = aggregated_results.sort_values(by=[\"Dataset\", \"Distribution\", \"Model\", \"TTA Method\", \"Accuracy\"], ascending=False)\n",
    "\n",
    "print(\"Overall Results\")\n",
    "# display(aggregated_results)\n",
    "\n",
    "# Average each TTA Method and Selective Method over distributions\n",
    "results_frame[\"ID\"] = results_frame[\"Distribution\"].apply(lambda d: \"ID\" in d)\n",
    "results_frame.drop(columns=[\"Distribution\", \"Accuracy\"], inplace=True)\n",
    "results_frame = results_frame[[\"Dataset\", \"ID\", \"Model\", \"TTA Method\", \"Selective Method\", \"Baseline Delta\", \"Augmentation Rate\"]]\n",
    "aggregated_results = results_frame.groupby([\"Dataset\", \"ID\", \"Model\", \"TTA Method\", \"Selective Method\"]).mean().round(4) * 100\n",
    "aggregated_results = aggregated_results.sort_values(by=[\"Dataset\", \"ID\", \"Model\", \"TTA Method\", \"Baseline Delta\"], ascending=False)\n",
    "print(\"Average Results\")\n",
    "for dataset in [\"Sentiment\", \"Toxicity\", \"AgNewsTweets\"]:\n",
    "    for tta_method in [\"Paraphrase\", \"ICR\"]:\n",
    "    # for tta_method in [\"ICR\"]:\n",
    "        print(f\"Dataset: {dataset}, TTA Method: {tta_method}\")\n",
    "        # display(aggregated_results.loc[dataset, :, :, tta_method, :])\n",
    "        # only show entropy-based\n",
    "        display(aggregated_results.loc[dataset, :, :, tta_method, \"entropy-based\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
