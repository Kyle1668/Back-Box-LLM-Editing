{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyobrien/conda/envs/icdt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tqdm.pandas()\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "aug_regex = re.compile(r\"<aug>(.*?)</aug>\", re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/kyobrien/repos/In-Context-Domain-Transfer-Improves-Out-of-Domain-Robustness/notebooks/cache/Kyle1668___parquet/Kyle1668--LLM-TTA-Augmentation-Logs-9e1afd14f02a2d65/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.53M/5.53M [00:00<00:00, 52.8MB/s]\n",
      "Downloading data: 100%|██████████| 7.10M/7.10M [00:00<00:00, 55.0MB/s]\n",
      "Downloading data: 100%|██████████| 5.37M/5.37M [00:00<00:00, 45.8MB/s]\n",
      "Downloading data: 100%|██████████| 6.71M/6.71M [00:00<00:00, 60.9MB/s]\n",
      "Downloading data: 100%|██████████| 5.32M/5.32M [00:00<00:00, 54.2MB/s]\n",
      "Downloading data: 100%|██████████| 5.42M/5.42M [00:00<00:00, 54.1MB/s]\n",
      "Downloading data: 100%|██████████| 5.02M/5.02M [00:00<00:00, 53.2MB/s]\n",
      "Downloading data: 100%|██████████| 3.64M/3.64M [00:00<00:00, 53.3MB/s]\n",
      "Downloading data: 100%|██████████| 1.83M/1.83M [00:00<00:00, 34.8MB/s]\n",
      "Downloading data: 100%|██████████| 2.16M/2.16M [00:00<00:00, 32.9MB/s]\n",
      "Downloading data: 100%|██████████| 1.69M/1.69M [00:00<00:00, 37.3MB/s]]\n",
      "Downloading data: 100%|██████████| 2.03M/2.03M [00:00<00:00, 42.5MB/s]]\n",
      "Downloading data: 100%|██████████| 1.68M/1.68M [00:00<00:00, 18.0MB/s]]\n",
      "Downloading data: 100%|██████████| 949k/949k [00:00<00:00, 10.1MB/s]/s]\n",
      "Downloading data: 100%|██████████| 1.24M/1.24M [00:00<00:00, 18.5MB/s]]\n",
      "Downloading data: 100%|██████████| 845k/845k [00:00<00:00, 17.6MB/s]/s]\n",
      "Downloading data: 100%|██████████| 1.11M/1.11M [00:00<00:00, 22.4MB/s]]\n",
      "Downloading data: 100%|██████████| 803k/803k [00:00<00:00, 23.6MB/s]/s]\n",
      "Downloading data: 100%|██████████| 949k/949k [00:00<00:00, 20.3MB/s]/s]\n",
      "Downloading data: 100%|██████████| 1.24M/1.24M [00:00<00:00, 28.0MB/s]]\n",
      "Downloading data: 100%|██████████| 845k/845k [00:00<00:00, 21.7MB/s]/s]\n",
      "Downloading data: 100%|██████████| 1.11M/1.11M [00:00<00:00, 24.3MB/s]]\n",
      "Downloading data: 100%|██████████| 803k/803k [00:00<00:00, 20.1MB/s]/s]\n",
      "Downloading data: 100%|██████████| 29.6M/29.6M [00:00<00:00, 71.1MB/s]]\n",
      "Downloading data: 100%|██████████| 35.0M/35.0M [00:00<00:00, 62.7MB/s]]\n",
      "Downloading data: 100%|██████████| 26.8M/26.8M [00:00<00:00, 68.8MB/s]]\n",
      "Downloading data: 100%|██████████| 25.9M/25.9M [00:00<00:00, 70.9MB/s]]\n",
      "Downloading data: 100%|██████████| 19.6M/19.6M [00:00<00:00, 56.1MB/s]]\n",
      "Downloading data: 100%|██████████| 26.8M/26.8M [00:00<00:00, 47.0MB/s]]\n",
      "Downloading data: 100%|██████████| 25.9M/25.9M [00:00<00:00, 66.6MB/s]]\n",
      "Downloading data: 100%|██████████| 19.6M/19.6M [00:00<00:00, 67.4MB/s]]\n",
      "Downloading data: 100%|██████████| 518k/518k [00:00<00:00, 17.6MB/s]/s]\n",
      "Downloading data: 100%|██████████| 640k/640k [00:00<00:00, 18.1MB/s]/s]\n",
      "Downloading data: 100%|██████████| 483k/483k [00:00<00:00, 6.64MB/s]/s]\n",
      "Downloading data: 100%|██████████| 596k/596k [00:00<00:00, 15.1MB/s]/s]\n",
      "Downloading data: 100%|██████████| 482k/482k [00:00<00:00, 14.4MB/s]/s]\n",
      "Downloading data: 100%|██████████| 299k/299k [00:00<00:00, 4.50MB/s]/s]\n",
      "Downloading data: 100%|██████████| 410k/410k [00:00<00:00, 11.5MB/s]/s]\n",
      "Downloading data: 100%|██████████| 272k/272k [00:00<00:00, 10.3MB/s]/s]\n",
      "Downloading data: 100%|██████████| 367k/367k [00:00<00:00, 12.2MB/s]/s]\n",
      "Downloading data: 100%|██████████| 260k/260k [00:00<00:00, 8.07MB/s]/s]\n",
      "Downloading data: 100%|██████████| 299k/299k [00:00<00:00, 9.15MB/s]/s]\n",
      "Downloading data: 100%|██████████| 410k/410k [00:00<00:00, 5.79MB/s]/s]\n",
      "Downloading data: 100%|██████████| 272k/272k [00:00<00:00, 9.25MB/s]/s]\n",
      "Downloading data: 100%|██████████| 367k/367k [00:00<00:00, 12.7MB/s]/s]\n",
      "Downloading data: 100%|██████████| 260k/260k [00:00<00:00, 10.6MB/s]/s]\n",
      "Downloading data: 100%|██████████| 11.0M/11.0M [00:00<00:00, 45.6MB/s]]\n",
      "Downloading data: 100%|██████████| 13.8M/13.8M [00:00<00:00, 67.3MB/s]]\n",
      "Downloading data: 100%|██████████| 10.0M/10.0M [00:00<00:00, 59.6MB/s]]\n",
      "Downloading data: 100%|██████████| 12.7M/12.7M [00:00<00:00, 59.0MB/s]]\n",
      "Downloading data: 100%|██████████| 10.2M/10.2M [00:00<00:00, 65.7MB/s]]\n",
      "Downloading data: 100%|██████████| 6.82M/6.82M [00:00<00:00, 46.3MB/s]]\n",
      "Downloading data: 100%|██████████| 9.41M/9.41M [00:00<00:00, 66.7MB/s]]\n",
      "Downloading data: 100%|██████████| 5.88M/5.88M [00:00<00:00, 51.9MB/s]]\n",
      "Downloading data: 100%|██████████| 8.35M/8.35M [00:00<00:00, 56.1MB/s]]\n",
      "Downloading data: 100%|██████████| 5.85M/5.85M [00:00<00:00, 30.1MB/s]]\n",
      "Downloading data: 100%|██████████| 6.82M/6.82M [00:00<00:00, 51.2MB/s]]\n",
      "Downloading data: 100%|██████████| 9.42M/9.42M [00:00<00:00, 60.0MB/s]]\n",
      "Downloading data: 100%|██████████| 5.88M/5.88M [00:00<00:00, 52.3MB/s]]\n",
      "Downloading data: 100%|██████████| 8.35M/8.35M [00:00<00:00, 63.1MB/s]]\n",
      "Downloading data: 100%|██████████| 5.85M/5.85M [00:00<00:00, 47.0MB/s]]\n",
      "Downloading data files: 100%|██████████| 61/61 [00:35<00:00,  1.70it/s]\n",
      "Extracting data files: 100%|██████████| 61/61 [00:00<00:00, 2301.41it/s]\n",
      "                                                                                                             \r"
     ]
    },
    {
     "ename": "UnexpectedSplits",
     "evalue": "{'BOSS_Sentiment_ID_Falcon_Translate', 'BOSS_Sentiment_ID_Falcon_Insert', 'AgNewsTweets_ID_BERT_Translate', 'AgNewsTweets_ID_BERT_Substitute', 'AgNewsTweets_ID_T5_Insert', 'BOSS_Sentiment_ID_T5_Translate', 'BOSS_Sentiment_ID_Falcon_Substitute', 'BOSS_Sentiment_ID_T5_Insert', 'AgNewsTweets_ID_T5_Translate', 'BOSS_Sentiment_ID_BERT_Insert', 'AgNewsTweets_ID_T5_Substitute', 'AgNewsTweets_ID_BERT_ICR', 'AgNewsTweets_ID_BERT_Paraphrase', 'AgNewsTweets_ID_BERT_Insert', 'BOSS_Sentiment_ID_T5_Substitute'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedSplits\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/kyobrien/repos/In-Context-Domain-Transfer-Improves-Out-of-Domain-Robustness/notebooks/method_analysis_paper.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22417a757265227d/home/kyobrien/repos/In-Context-Domain-Transfer-Improves-Out-of-Domain-Robustness/notebooks/method_analysis_paper.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mKyle1668/LLM-TTA-Augmentation-Logs\u001b[39;49m\u001b[39m\"\u001b[39;49m, cache_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./cache\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/conda/envs/icdt/lib/python3.10/site-packages/datasets/load.py:1809\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1806\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1808\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1809\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   1810\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1811\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1812\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m   1813\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   1814\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   1815\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   1816\u001b[0m )\n\u001b[1;32m   1818\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1819\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1820\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1821\u001b[0m )\n",
      "File \u001b[0;32m~/conda/envs/icdt/lib/python3.10/site-packages/datasets/builder.py:909\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 909\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    910\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    911\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m    912\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    913\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    914\u001b[0m     )\n\u001b[1;32m    915\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/conda/envs/icdt/lib/python3.10/site-packages/datasets/builder.py:1022\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     dl_manager\u001b[39m.\u001b[39mmanage_extracted_files()\n\u001b[1;32m   1021\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS \u001b[39mor\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS:\n\u001b[0;32m-> 1022\u001b[0m     verify_splits(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49msplits, split_dict)\n\u001b[1;32m   1024\u001b[0m \u001b[39m# Update the info object with the splits.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits \u001b[39m=\u001b[39m split_dict\n",
      "File \u001b[0;32m~/conda/envs/icdt/lib/python3.10/site-packages/datasets/utils/info_utils.py:93\u001b[0m, in \u001b[0;36mverify_splits\u001b[0;34m(expected_splits, recorded_splits)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[39mraise\u001b[39;00m ExpectedMoreSplits(\u001b[39mstr\u001b[39m(\u001b[39mset\u001b[39m(expected_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(recorded_splits)))\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(recorded_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(expected_splits)) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m UnexpectedSplits(\u001b[39mstr\u001b[39m(\u001b[39mset\u001b[39m(recorded_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(expected_splits)))\n\u001b[1;32m     94\u001b[0m bad_splits \u001b[39m=\u001b[39m [\n\u001b[1;32m     95\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mexpected\u001b[39m\u001b[39m\"\u001b[39m: expected_splits[name], \u001b[39m\"\u001b[39m\u001b[39mrecorded\u001b[39m\u001b[39m\"\u001b[39m: recorded_splits[name]}\n\u001b[1;32m     96\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m expected_splits\n\u001b[1;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m expected_splits[name]\u001b[39m.\u001b[39mnum_examples \u001b[39m!=\u001b[39m recorded_splits[name]\u001b[39m.\u001b[39mnum_examples\n\u001b[1;32m     98\u001b[0m ]\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(bad_splits) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mUnexpectedSplits\u001b[0m: {'BOSS_Sentiment_ID_Falcon_Translate', 'BOSS_Sentiment_ID_Falcon_Insert', 'AgNewsTweets_ID_BERT_Translate', 'AgNewsTweets_ID_BERT_Substitute', 'AgNewsTweets_ID_T5_Insert', 'BOSS_Sentiment_ID_T5_Translate', 'BOSS_Sentiment_ID_Falcon_Substitute', 'BOSS_Sentiment_ID_T5_Insert', 'AgNewsTweets_ID_T5_Translate', 'BOSS_Sentiment_ID_BERT_Insert', 'AgNewsTweets_ID_T5_Substitute', 'AgNewsTweets_ID_BERT_ICR', 'AgNewsTweets_ID_BERT_Paraphrase', 'AgNewsTweets_ID_BERT_Insert', 'BOSS_Sentiment_ID_T5_Substitute'}"
     ]
    }
   ],
   "source": [
    "load_dataset(\"Kyle1668/LLM-TTA-Augmentation-Logs\", cache_dir=\"./cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/kyobrien/.cache/huggingface/datasets/Kyle1668___parquet/Kyle1668--LLM-TTA-Augmentation-Logs-9e1afd14f02a2d65/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 61/61 [00:00<00:00, 10736.57it/s]\n",
      "Extracting data files: 100%|██████████| 61/61 [00:00<00:00, 2399.22it/s]\n",
      "                                                                                                             \r"
     ]
    },
    {
     "ename": "UnexpectedSplits",
     "evalue": "{'BOSS_Sentiment_ID_Falcon_Translate', 'BOSS_Sentiment_ID_Falcon_Insert', 'AgNewsTweets_ID_BERT_Translate', 'AgNewsTweets_ID_BERT_Substitute', 'AgNewsTweets_ID_T5_Insert', 'BOSS_Sentiment_ID_T5_Translate', 'BOSS_Sentiment_ID_Falcon_Substitute', 'BOSS_Sentiment_ID_T5_Insert', 'AgNewsTweets_ID_T5_Translate', 'BOSS_Sentiment_ID_BERT_Insert', 'AgNewsTweets_ID_T5_Substitute', 'AgNewsTweets_ID_BERT_ICR', 'AgNewsTweets_ID_BERT_Paraphrase', 'AgNewsTweets_ID_BERT_Insert', 'BOSS_Sentiment_ID_T5_Substitute'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedSplits\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/kyobrien/repos/In-Context-Domain-Transfer-Improves-Out-of-Domain-Robustness/notebooks/method_analysis_paper.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22417a757265227d/home/kyobrien/repos/In-Context-Domain-Transfer-Improves-Out-of-Domain-Robustness/notebooks/method_analysis_paper.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sst5_data \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mKyle1668/LLM-TTA-Augmentation-Logs\u001b[39;49m\u001b[39m\"\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBOSS_Sentiment_ID_Falcon_Translate\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mto_pandas()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22417a757265227d/home/kyobrien/repos/In-Context-Domain-Transfer-Improves-Out-of-Domain-Robustness/notebooks/method_analysis_paper.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m display(sst5_data\u001b[39m.\u001b[39mhead(\u001b[39m1\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22417a757265227d/home/kyobrien/repos/In-Context-Domain-Transfer-Improves-Out-of-Domain-Robustness/notebooks/method_analysis_paper.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m display(sst5_data\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/conda/envs/icdt/lib/python3.10/site-packages/datasets/load.py:1809\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1806\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1808\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1809\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   1810\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1811\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1812\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m   1813\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   1814\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   1815\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   1816\u001b[0m )\n\u001b[1;32m   1818\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1819\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1820\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1821\u001b[0m )\n",
      "File \u001b[0;32m~/conda/envs/icdt/lib/python3.10/site-packages/datasets/builder.py:909\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 909\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    910\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    911\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m    912\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    913\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    914\u001b[0m     )\n\u001b[1;32m    915\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/conda/envs/icdt/lib/python3.10/site-packages/datasets/builder.py:1022\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     dl_manager\u001b[39m.\u001b[39mmanage_extracted_files()\n\u001b[1;32m   1021\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS \u001b[39mor\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS:\n\u001b[0;32m-> 1022\u001b[0m     verify_splits(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49msplits, split_dict)\n\u001b[1;32m   1024\u001b[0m \u001b[39m# Update the info object with the splits.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits \u001b[39m=\u001b[39m split_dict\n",
      "File \u001b[0;32m~/conda/envs/icdt/lib/python3.10/site-packages/datasets/utils/info_utils.py:93\u001b[0m, in \u001b[0;36mverify_splits\u001b[0;34m(expected_splits, recorded_splits)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[39mraise\u001b[39;00m ExpectedMoreSplits(\u001b[39mstr\u001b[39m(\u001b[39mset\u001b[39m(expected_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(recorded_splits)))\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(recorded_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(expected_splits)) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m UnexpectedSplits(\u001b[39mstr\u001b[39m(\u001b[39mset\u001b[39m(recorded_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(expected_splits)))\n\u001b[1;32m     94\u001b[0m bad_splits \u001b[39m=\u001b[39m [\n\u001b[1;32m     95\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mexpected\u001b[39m\u001b[39m\"\u001b[39m: expected_splits[name], \u001b[39m\"\u001b[39m\u001b[39mrecorded\u001b[39m\u001b[39m\"\u001b[39m: recorded_splits[name]}\n\u001b[1;32m     96\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m expected_splits\n\u001b[1;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m expected_splits[name]\u001b[39m.\u001b[39mnum_examples \u001b[39m!=\u001b[39m recorded_splits[name]\u001b[39m.\u001b[39mnum_examples\n\u001b[1;32m     98\u001b[0m ]\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(bad_splits) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mUnexpectedSplits\u001b[0m: {'BOSS_Sentiment_ID_Falcon_Translate', 'BOSS_Sentiment_ID_Falcon_Insert', 'AgNewsTweets_ID_BERT_Translate', 'AgNewsTweets_ID_BERT_Substitute', 'AgNewsTweets_ID_T5_Insert', 'BOSS_Sentiment_ID_T5_Translate', 'BOSS_Sentiment_ID_Falcon_Substitute', 'BOSS_Sentiment_ID_T5_Insert', 'AgNewsTweets_ID_T5_Translate', 'BOSS_Sentiment_ID_BERT_Insert', 'AgNewsTweets_ID_T5_Substitute', 'AgNewsTweets_ID_BERT_ICR', 'AgNewsTweets_ID_BERT_Paraphrase', 'AgNewsTweets_ID_BERT_Insert', 'BOSS_Sentiment_ID_T5_Substitute'}"
     ]
    }
   ],
   "source": [
    "sst5_data = load_dataset(\"Kyle1668/LLM-TTA-Augmentation-Logs\", split=\"BOSS_Sentiment_SST5_BERT_ICR\").to_pandas()\n",
    "display(sst5_data.head(1))\n",
    "display(sst5_data.shape)\n",
    "\n",
    "# toxigen_data = load_dataset(\"Kyle1668/LLM-TTA-Augmentation-Logs\", split=\"BOSS_Toxicity_Toxigen_BERT_ICR\").to_pandas()\n",
    "# display(toxigen_data.head(1))\n",
    "# display(toxigen_data.shape)\n",
    "\n",
    "# agt_data = load_dataset(\"Kyle1668/LLM-TTA-Augmentation-Logs\", split=\"AgNewsTweets_Tweets_BERT_ICR\").to_pandas()\n",
    "# display(agt_data.head(1))\n",
    "# display(agt_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze ICR Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst5_data[\"input\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does TTA Effect Some Classes More Than Others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst5_data.value_counts([\"label\", \"outcome\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the overall ratio of New Corrections to New Mistakes for sst5_data, toxigen_data, and agt_data\n",
    "pd.concat([sst5_data, toxigen_data, agt_data]).value_counts([\"outcome\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataset, get the percent of examples that are unchanged vs new\n",
    "sst5_outcomes = sst5_data[\"outcome\"].value_counts(normalize=True)\n",
    "new_predcitions_percent = 100 * sst5_outcomes[sst5_outcomes.index == \"New Correct\"].values[0] + sst5_outcomes[sst5_outcomes.index == \"New Mistake\"].values[0]\n",
    "print(f\"SST-5: {new_predcitions_percent:.2f}% of examples are new predictions\")\n",
    "\n",
    "toxicgen_outcomes = toxigen_data[\"outcome\"].value_counts(normalize=True)\n",
    "new_predcitions_percent = 100 * toxicgen_outcomes[toxicgen_outcomes.index == \"New Correct\"].values[0] + toxicgen_outcomes[toxicgen_outcomes.index == \"New Mistake\"].values[0]\n",
    "print(f\"ToxicGen: {new_predcitions_percent:.2f}% of examples are new predictions\")\n",
    "\n",
    "agt_outcomes = agt_data[\"outcome\"].value_counts(normalize=True)\n",
    "new_predcitions_percent = 100 * agt_outcomes[agt_outcomes.index == \"New Correct\"].values[0] + agt_outcomes[agt_outcomes.index == \"New Mistake\"].values[0]\n",
    "print(f\"AGT: {new_predcitions_percent:.2f}% of examples are new predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear plots\n",
    "plt.clf()\n",
    "\n",
    "# Create three histograms on one row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "sst5_labels = {\n",
    "    0: \"Negative\",\n",
    "    1: \"Positive\",\n",
    "    2: \"Neutral\",\n",
    "}\n",
    "sst5_corruptions_corrections = sst5_data[(sst5_data[\"outcome\"] == \"New Correct\") | (sst5_data[\"outcome\"] == \"New Mistake\")]\n",
    "sst5_corruptions_corrections.sort_values(by=[\"label\", \"outcome\"], inplace=True)\n",
    "sst5_corruptions_corrections[\"label\"] = sst5_corruptions_corrections[\"label\"].apply(lambda l: sst5_labels[l])\n",
    "# sort values by Negative, Neutral, Positive in that order\n",
    "sst5_corruptions_corrections.sort_values(by=[\"label\"], inplace=True, key=lambda x: x.map({\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}))\n",
    "sns.histplot(data=sst5_corruptions_corrections, x=\"label\", hue=\"outcome\", multiple=\"dodge\", shrink=.8, ax=axes[0])\n",
    "\n",
    "toxigen_labels = {\n",
    "    0: \"Non-Toxic\",\n",
    "    1: \"Toxic\",\n",
    "}\n",
    "toxigen_corruptions_corrections = toxigen_data[(toxigen_data[\"outcome\"] == \"New Correct\") | (toxigen_data[\"outcome\"] == \"New Mistake\")]\n",
    "toxigen_corruptions_corrections.sort_values(by=[\"label\", \"outcome\"], inplace=True)\n",
    "toxigen_corruptions_corrections[\"label\"] = toxigen_corruptions_corrections[\"label\"].apply(lambda l: toxigen_labels[l])\n",
    "sns.histplot(data=toxigen_corruptions_corrections, x=\"label\", hue=\"outcome\", multiple=\"dodge\", shrink=.8, ax=axes[1])\n",
    "\n",
    "agt_labels = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\",\n",
    "}\n",
    "agt_corruptions_corrections = agt_data[(agt_data[\"outcome\"] == \"New Correct\") | (agt_data[\"outcome\"] == \"New Mistake\")]\n",
    "agt_corruptions_corrections.sort_values(by=[\"label\", \"outcome\"], inplace=True)\n",
    "agt_corruptions_corrections[\"label\"] = agt_corruptions_corrections[\"label\"].apply(lambda l: agt_labels[l])\n",
    "sns.histplot(data=agt_corruptions_corrections, x=\"label\", hue=\"outcome\", multiple=\"dodge\", shrink=.8, ax=axes[2])\n",
    "\n",
    "axes[0].set_ylabel(\"Count\", labelpad=20, fontsize=14)\n",
    "axes[1].set_ylabel(\"\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "axes[0].set_xlabel(\"SST-5\", labelpad=20, fontsize=14)\n",
    "axes[1].set_xlabel(\"Toxigen\", labelpad=20, fontsize=14)\n",
    "axes[2].set_xlabel(\"AG News Tweets\", labelpad=20, fontsize=14)\n",
    "\n",
    "# set x labels above the plots\n",
    "axes[0].xaxis.set_label_position('top')\n",
    "axes[1].xaxis.set_label_position('top')\n",
    "axes[2].xaxis.set_label_position('top')\n",
    "\n",
    "# Have a shared legend\n",
    "axes[0].get_legend().remove()\n",
    "axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), labels=[\"Corruptions\", \"Corrections\"], ncol=2, fancybox=False, frameon=False, fontsize=14)\n",
    "axes[2].get_legend().remove()\n",
    "\n",
    "\n",
    "# add padding\n",
    "fig.tight_layout(pad=3.0)\n",
    "fig.savefig(\"../datasets/analysis/figures/corruptions_corrections_histograms.png\", bbox_inches='tight', dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy-Based Selective Augmentation# Entropy Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rewrites in sst5_data[sst5_data[\"input\"].str.contains(\"`\")][\"input\"].values:\n",
    "    for current_rewrite in re.findall(aug_regex, rewrites):\n",
    "        print(current_rewrite)\n",
    "        print(ast.literal_eval(current_rewrite))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0, 1, 0.00001)\n",
    "# thresholds = np.arange(0, 1, 0.05)\n",
    "\n",
    "baseline_perf = {\n",
    "    \"SST-5\": 0.6847,\n",
    "    \"Sem Eval\": 0.4498,\n",
    "    \"Dynasent\": 0.4271,\n",
    "    \"ToxiGen\": 0.6670,\n",
    "    \"Adv Civil\": 0.3050,\n",
    "    \"Implicit Hate\": 0.6454,\n",
    "    \"AG News Tweets\": 0.8857,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fix where each plot is 5 inches wide and 5 inches tall with 2 padding\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(15, 6))\n",
    "\n",
    "def calculate_entropy_threshold_jugments(inference_log_frame, dataset_name, half=False):\n",
    "    threshold_scores = []\n",
    "    threshold_rewrite_rates = []\n",
    "    for t in tqdm(thresholds, desc=\"Calculating entropy threshold scores\"):\n",
    "        t_perf, t_rate = get_threshold_accuracy(t, inference_log_frame)\n",
    "        threshold_scores.append(t_perf)\n",
    "        threshold_rewrite_rates.append(t_rate)\n",
    "\n",
    "    thresholds_frame = pd.DataFrame({\"threshold\": thresholds, \"accuracy\": threshold_scores, \"rewrite_rate\": threshold_rewrite_rates})\n",
    "\n",
    "    # Set line splot\n",
    "    coordinates = {\n",
    "        \"SST-5\": 0,\n",
    "        \"ToxiGen\": 1,\n",
    "        \"AG News Tweets\": 2,\n",
    "    }\n",
    "\n",
    "    # Create a line plot with the coordinates in the grid\n",
    "    figure = axs[coordinates[dataset_name]]\n",
    "    figure = sns.lineplot(data=thresholds_frame, x=\"rewrite_rate\", y=\"accuracy\", label=\"TTA\", ax=figure)\n",
    "    figure.set_title(dataset_name, fontsize=18, pad=15)\n",
    "    figure.set_xlabel(\"Augmentation Rate\" if dataset_name == \"ToxiGen\" else \"\", labelpad=20, fontsize=14)\n",
    "    figure.set_ylabel(\"Accuracy\" if dataset_name == \"SST-5\" else \"\", labelpad=20, fontsize=14)\n",
    "    figure.title.set_size(18)\n",
    "    figure.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:.0%}\".format(x)))\n",
    "    figure.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:.2%}\".format(x)))\n",
    "    figure.set_xlim(left=0)\n",
    "    figure.lines[0].set_linewidth(2)\n",
    "    figure.legend_.remove()\n",
    "\n",
    "    # Display max accuracy point\n",
    "    accuracy_max_point = thresholds_frame[thresholds_frame[\"accuracy\"] == thresholds_frame.max()[\"accuracy\"]].sort_values(\"rewrite_rate\").iloc[-1].to_dict()\n",
    "    figure.plot(accuracy_max_point[\"rewrite_rate\"],\n",
    "                accuracy_max_point[\"accuracy\"],\n",
    "                marker=\"o\",\n",
    "                markersize=6,\n",
    "                label=\"Optimal\",\n",
    "                )\n",
    "    figure.annotate(f\"{accuracy_max_point['accuracy']:.2%}\",\n",
    "                    (accuracy_max_point[\"rewrite_rate\"], accuracy_max_point[\"accuracy\"]),\n",
    "                    textcoords=\"offset points\",\n",
    "                    xytext=(10, 0),\n",
    "                    ha=\"left\",\n",
    "                    fontsize=10)\n",
    "\n",
    "    # plot dashed gray line representing the baseline withour augmentation\n",
    "    figure.plot([0, 1], [baseline_perf[dataset_name], baseline_perf[dataset_name]], color=\"gray\", linestyle=\"--\", linewidth=1.5, alpha=0.75, label=\"No TTA (Baseline)\")\n",
    "    # figure.axhline(baseline_perf[dataset_name], color=\"gray\", linestyle=\"--\", linewidth=1.5, alpha=0.75)\n",
    "    if dataset_name == \"SST-5\":\n",
    "        figure.set_ylim(bottom=baseline_perf[dataset_name] - 0.005)\n",
    "\n",
    "    if dataset_name == \"ToxiGen\":\n",
    "        figure.legend(loc=\"upper center\", fontsize=12, frameon=False, ncol=3,\n",
    "                      bbox_to_anchor=(0.5, -0.2),\n",
    "                      )\n",
    "\n",
    "    target_threshold = None\n",
    "    if half is False:\n",
    "        target_threshold = thresholds_frame[thresholds_frame[\"accuracy\"] == thresholds_frame.max()[\"accuracy\"]].sort_values(\"rewrite_rate\").iloc[-1]\n",
    "    else:\n",
    "        thresholds_deltas_list = abs(thresholds_frame[\"rewrite_rate\"] - 50).tolist()\n",
    "        closest_half_delta = min(thresholds_deltas_list)\n",
    "        closest_threshold_index = thresholds_deltas_list.index(closest_half_delta)\n",
    "        target_threshold = thresholds_frame.iloc[closest_threshold_index]\n",
    "\n",
    "    rewrite_rate = target_threshold[\"rewrite_rate\"] / 100\n",
    "    original_judgments = inference_log_frame.apply(lambda row: row[\"original judgment\"] if row[\"original entropy\"] < target_threshold[\"threshold\"] else row[\"judgment\"], axis=1)\n",
    "    return original_judgments, rewrite_rate\n",
    "\n",
    "\n",
    "def get_threshold_accuracy(threshold, inference_logs_frame):\n",
    "    threshold_judgments = inference_logs_frame.apply(lambda row: row[\"original judgment\"] if row[\"original entropy\"] < threshold else row[\"judgment\"], axis=1)\n",
    "    report = classification_report(inference_logs_frame[\"label\"], threshold_judgments, digits=4, output_dict=True)\n",
    "    llm_call_count = (inference_logs_frame[\"original entropy\"] >= threshold).sum()\n",
    "    llm_call_rate = llm_call_count / len(inference_logs_frame)\n",
    "    return report[\"accuracy\"], llm_call_rate\n",
    "\n",
    "calculate_entropy_threshold_jugments(sst5_data, \"SST-5\")\n",
    "calculate_entropy_threshold_jugments(toxigen_data, \"ToxiGen\")\n",
    "calculate_entropy_threshold_jugments(agt_data, \"AG News Tweets\")\n",
    "fig.tight_layout(pad=1.0)\n",
    "fig.savefig(\"../datasets/analysis/entropy_figures/main_acc_rewrite_curves.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix Entropy Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(20, 10), nrows=2)\n",
    "\n",
    "def calculate_entropy_threshold_jugments(inference_log_frame, dataset_name, half=False):\n",
    "    # thresholds = np.arange(0, 1, 0.0001)\n",
    "    threshold_scores = []\n",
    "    threshold_rewrite_rates = []\n",
    "    for t in tqdm(thresholds, desc=\"Calculating entropy threshold scores\"):\n",
    "        t_perf, t_rate = get_threshold_accuracy(t, inference_log_frame)\n",
    "        threshold_scores.append(t_perf)\n",
    "        threshold_rewrite_rates.append(t_rate)\n",
    "\n",
    "    thresholds_frame = pd.DataFrame({\"threshold\": thresholds, \"accuracy\": threshold_scores, \"rewrite_rate\": threshold_rewrite_rates})\n",
    "\n",
    "    # Set line splot\n",
    "    coordinates = {\n",
    "        \"SST-5\": (0, 0),\n",
    "        \"Sem Eval\": (0, 1),\n",
    "        \"Dynasent\": (0, 2),\n",
    "        \"ToxiGen\": (0, 3),\n",
    "        \"Adv Civil\": (1, 0),\n",
    "        \"Implicit Hate\": (1, 1),\n",
    "        \"AG News Tweets\": (1, 2),\n",
    "    }\n",
    "\n",
    "    # Create a line plot with the coordinates in the grid\n",
    "    figure = axs[coordinates[dataset_name][0]][coordinates[dataset_name][1]]\n",
    "    figure = sns.lineplot(data=thresholds_frame, x=\"rewrite_rate\", y=\"accuracy\", label=\"TTA\", ax=figure)\n",
    "    figure.set_title(dataset_name, fontsize=18, pad=15)\n",
    "    figure.set_xlabel(\"Augmentation Rate\" if dataset_name in [\"ToxiGen\", \"Adv Civil\", \"Implicit Hate\", \"AG News Tweets\"] else \"\", labelpad=20, fontsize=14)\n",
    "    figure.set_ylabel(\"Accuracy\" if dataset_name in [\"SST-5\", \"Adv Civil\"] else \"\", labelpad=20, fontsize=14)\n",
    "    figure.title.set_size(18)\n",
    "    figure.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:.0%}\".format(x)))\n",
    "    figure.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:.2%}\".format(x)))\n",
    "    figure.set_xlim(left=0)\n",
    "    figure.lines[0].set_linewidth(2)\n",
    "    figure.legend_.remove()\n",
    "\n",
    "    # Display max accuracy point\n",
    "    accuracy_max_point = thresholds_frame[thresholds_frame[\"accuracy\"] == thresholds_frame.max()[\"accuracy\"]].sort_values(\"rewrite_rate\").iloc[-1].to_dict()\n",
    "    figure.plot(accuracy_max_point[\"rewrite_rate\"],\n",
    "                accuracy_max_point[\"accuracy\"],\n",
    "                marker=\"o\",\n",
    "                markersize=6,\n",
    "                label=\"Optimal\",\n",
    "                )\n",
    "    figure.annotate(f\"{accuracy_max_point['accuracy']:.2%}\",\n",
    "                    (accuracy_max_point[\"rewrite_rate\"], accuracy_max_point[\"accuracy\"]),\n",
    "                    textcoords=\"offset points\",\n",
    "                    xytext=(10, 0),\n",
    "                    ha=\"left\",\n",
    "                    fontsize=10)\n",
    "\n",
    "    figure.plot([0, 1], [baseline_perf[dataset_name], baseline_perf[dataset_name]], color=\"gray\", linestyle=\"--\", linewidth=1.5, alpha=0.75, label=\"No TTA (Baseline)\")\n",
    "\n",
    "    if dataset_name == \"SST-5\":\n",
    "        figure.set_ylim(bottom=baseline_perf[dataset_name] - 0.005)\n",
    "\n",
    "    target_threshold = None\n",
    "    if half is False:\n",
    "        target_threshold = thresholds_frame[thresholds_frame[\"accuracy\"] == thresholds_frame.max()[\"accuracy\"]].sort_values(\"rewrite_rate\").iloc[-1]\n",
    "    else:\n",
    "        thresholds_deltas_list = abs(thresholds_frame[\"rewrite_rate\"] - 50).tolist()\n",
    "        closest_half_delta = min(thresholds_deltas_list)\n",
    "        closest_threshold_index = thresholds_deltas_list.index(closest_half_delta)\n",
    "        target_threshold = thresholds_frame.iloc[closest_threshold_index]\n",
    "\n",
    "    rewrite_rate = target_threshold[\"rewrite_rate\"] / 100\n",
    "    original_judgments = inference_log_frame.apply(lambda row: row[\"original judgment\"] if row[\"original entropy\"] < target_threshold[\"threshold\"] else row[\"judgment\"], axis=1)\n",
    "    return original_judgments, rewrite_rate\n",
    "\n",
    "\n",
    "def get_threshold_accuracy(threshold, inference_logs_frame):\n",
    "    threshold_judgments = inference_logs_frame.apply(lambda row: row[\"original judgment\"] if row[\"original entropy\"] < threshold else row[\"judgment\"], axis=1)\n",
    "    report = classification_report(inference_logs_frame[\"label\"], threshold_judgments, digits=4, output_dict=True)\n",
    "    llm_call_count = (inference_logs_frame[\"original entropy\"] >= threshold).sum()\n",
    "    llm_call_rate = llm_call_count / len(inference_logs_frame)\n",
    "    return report[\"accuracy\"], llm_call_rate\n",
    "\n",
    "\n",
    "calculate_entropy_threshold_jugments(sst5_data, \"SST-5\")\n",
    "calculate_entropy_threshold_jugments(semval_data, \"Sem Eval\")\n",
    "calculate_entropy_threshold_jugments(dynasent_data, \"Dynasent\")\n",
    "calculate_entropy_threshold_jugments(toxigen_data, \"ToxiGen\")\n",
    "calculate_entropy_threshold_jugments(adv_civil_data, \"Adv Civil\")\n",
    "calculate_entropy_threshold_jugments(implicit_hate_data, \"Implicit Hate\")\n",
    "calculate_entropy_threshold_jugments(agt_data, \"AG News Tweets\")\n",
    "\n",
    "fig.delaxes(axs[1, -1])\n",
    "fig.legend(loc=\"lower center\", fontsize=12, frameon=False, ncol=3, labels=[\"TTA\", \"No TTA (Baseline)\", \"Optimal Aug Rate\"], bbox_to_anchor=(0.5, -0.025))\n",
    "fig.tight_layout(pad=2.0)\n",
    "fig.savefig(\"../datasets/analysis/entropy_figures/appendix_acc_rewrite_curves.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_eval_original_entropies = semval_data[\"original entropy\"].tolist()\n",
    "figure = sns.scatterplot(data=semval_data, x=range(len(sem_eval_original_entropies)), y=\"original entropy\", hue=\"outcome\", s=5)\n",
    "# set legend to the right vertically\n",
    "figure.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0, frameon=False, title=\"Outcome\")\n",
    "# make y axis log scale\n",
    "figure.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst5_data[[\"original entropy\", \"outcome\"]].groupby(\"outcome\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap between original entropy and outcome\n",
    "pd.crosstab(sst5_data[\"original entropy\"], semval_data[\"outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
