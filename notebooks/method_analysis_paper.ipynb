{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/miniconda3/envs/eval-aug/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "from torch.nn import CosineSimilarity\n",
    "from scipy.stats import pointbiserialr\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Seaborn settings\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.color_palette(\"pastel\")\n",
    "aug_regex = re.compile(r\"<aug>(.*?)</aug>\", re.DOTALL)\n",
    "\n",
    "# Matplotlib settings\n",
    "os.environ[\"PATH\"] += os.pathsep + '/Library/TeX/texbin'\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.rcParams.update({\n",
    "  \"text.usetex\": True,\n",
    "  \"font.family\": \"serif\",\n",
    "  \"font.serif\": \"cm\",\n",
    "  \"font.size\": 16,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display full dataframes\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting constants\n",
    "TITLE_FONT_SIZE = 16\n",
    "WSPACE = 0.3\n",
    "FIGURE_HEIGHT = 3\n",
    "LINE_WIDTH = 2\n",
    "FIG_SIZE = 4\n",
    "MARKER_SIZE = 8\n",
    "X_LABEL_ROTATION = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_logs = load_from_disk(\"data/combined_dataset\")\n",
    "list(inference_logs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_task_name(split_name):\n",
    "    return \"Sentiment\" if \"Sentiment\" in split_name else \"Toxicity\" if \"Toxicity\" in split_name else \"News\"\n",
    "\n",
    "def parse_distribution(split_name):\n",
    "    return split_name.split(\"_\")[-3]\n",
    "\n",
    "def parse_model(split_name):\n",
    "    return split_name.split(\"_\")[-2]\n",
    "\n",
    "def parse_tta_method(split_name):\n",
    "    return split_name.split(\"_\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No TTA Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_tta_accuracies = {}\n",
    "for split_name in tqdm(inference_logs):\n",
    "    split_frame = inference_logs[split_name].to_pandas()\n",
    "    split_no_tta_accuracy = classification_report(split_frame[\"label\"], split_frame[\"original_predicted_class\"], output_dict=True)[\"accuracy\"]\n",
    "    no_tta_accuracies[split_name] = split_no_tta_accuracy\n",
    "\n",
    "no_tta_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Main Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results_bert_splits = [split for split in inference_logs.keys() if \"Ablate\" not in split]\n",
    "datasets = [\"BOSS_Sentiment\", \"BOSS_Toxicity\", \"AgNewsTweets\"]\n",
    "split_data = {}\n",
    "\n",
    "for task_name in datasets:\n",
    "    if task_name not in split_data:\n",
    "        split_data[task_name] = {}\n",
    "\n",
    "    for split in tqdm(main_results_bert_splits):\n",
    "        if task_name in split:\n",
    "            distribution = split.split(\"_\")[-3]\n",
    "            model = split.split(\"_\")[-2]\n",
    "            tta_method = split.split(\"_\")[-1]\n",
    "            baseline_accuracy = classification_report(inference_logs[split][\"label\"], inference_logs[split][\"original_predicted_class\"], output_dict=True)[\"accuracy\"]\n",
    "            tta_accuracy = classification_report(inference_logs[split][\"label\"], inference_logs[split][\"tta_predicted_class\"], output_dict=True)[\"accuracy\"]\n",
    "            accuracy_delta = tta_accuracy - baseline_accuracy\n",
    "\n",
    "            if model not in split_data[task_name]:\n",
    "                split_data[task_name][model] = {}\n",
    "\n",
    "            if tta_method not in split_data[task_name][model]:\n",
    "                split_data[task_name][model][tta_method] = {}\n",
    "\n",
    "            split_data[task_name][model][tta_method][distribution] = {\n",
    "                \"distribution\": distribution,\n",
    "                \"model\": model,\n",
    "                \"tta_method\": tta_method,\n",
    "                \"baseline_accuracy\": baseline_accuracy,\n",
    "                \"tta_accuracy\": tta_accuracy,\n",
    "                \"accuracy_delta\": accuracy_delta\n",
    "            }\n",
    "\n",
    "    # get the Accuracy Gain for each method excluding ID\n",
    "    for model_name in split_data[task_name]:\n",
    "        for tta_method in split_data[task_name][model_name]:\n",
    "            accuracy_deltas = []\n",
    "            baseline_accuracies = []\n",
    "            for distribution in split_data[task_name][model_name][tta_method]:\n",
    "                if distribution == \"ID\":\n",
    "                    continue\n",
    "\n",
    "                current_tta_result = split_data[task_name][model_name][tta_method][distribution]\n",
    "                accuracy_deltas.append(current_tta_result[\"accuracy_delta\"])\n",
    "                baseline_accuracies.append(current_tta_result[\"baseline_accuracy\"])\n",
    "\n",
    "            split_data[task_name][model_name][tta_method][\"mean_accuracy_delta\"] = np.mean(accuracy_deltas)\n",
    "            split_data[task_name][model_name][tta_method][\"mean_baseline_accuracy\"] = np.mean(baseline_accuracies)\n",
    "\n",
    "print(json.dumps(split_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_results_records = []\n",
    "for task_name in split_data:\n",
    "    for model_name in split_data[task_name]:\n",
    "        for tta_method in split_data[task_name][model_name]:\n",
    "            for dist in [dist for dist in split_data[task_name][model_name][tta_method] if not dist.startswith(\"mean\")]:\n",
    "\n",
    "                perf_record = split_data[task_name][model_name][tta_method][dist]\n",
    "                detailed_results_records.append({\n",
    "                    \"task\": task_name,\n",
    "                    \"model\": model_name,\n",
    "                    \"tta_method\": tta_method,\n",
    "                    \"distribution\": dist,\n",
    "                    # \"baseline_accuracy\": perf_record[\"baseline_accuracy\"] * 100,\n",
    "                    # \"tta_accuracy\": perf_record[\"tta_accuracy\"] * 100,\n",
    "                    \"accuracy_delta\": round(perf_record[\"accuracy_delta\"] * 100, 2),\n",
    "                })\n",
    "\n",
    "detailed_results_frame = pd.DataFrame(detailed_results_records)\n",
    "for task_name in split_data:\n",
    "    task_frame = detailed_results_frame[detailed_results_frame[\"task\"] == task_name]\n",
    "    display(task_frame.groupby([\"task\", \"model\", \"distribution\", \"tta_method\", ]).mean().T)\n",
    "\n",
    "detailed_results_frame.groupby([\"task\", \"model\", \"distribution\", \"tta_method\", ]).mean().to_csv(\"data/detailed_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe where there is a column for dataset, model, id_accuracy_delta, and mean_accuracy_delta\n",
    "records = []\n",
    "\n",
    "for task_name in tqdm(split_data):\n",
    "    for model_name in split_data[task_name]:\n",
    "        for tta_method in split_data[task_name][model_name]:\n",
    "            current_tta_result = split_data[task_name][model_name][tta_method]\n",
    "            records.append({\n",
    "                \"dataset\": task_name,\n",
    "                \"model\": model_name,\n",
    "                \"tta_method\": tta_method,\n",
    "                \"id_accuracy_delta\": current_tta_result[\"ID\"][\"accuracy_delta\"] * 100 if \"ID\" in current_tta_result else None,\n",
    "                \"ood_mean_accuracy_delta\": current_tta_result[\"mean_accuracy_delta\"] * 100,\n",
    "                \"ood_mean_baseline_accuracy\": current_tta_result[\"mean_baseline_accuracy\"] * 100,\n",
    "            })\n",
    "\n",
    "\n",
    "main_results_frame = pd.DataFrame(records)\n",
    "for model in split_data[task_name]:\n",
    "    display(model)\n",
    "    display(main_results_frame[main_results_frame[\"model\"] == model].drop(columns=\"model\").groupby([\"dataset\", \"tta_method\"]).mean().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_label_value_counts = {}\n",
    "\n",
    "for split_name in main_results_bert_splits:\n",
    "    if \"BERT\" not in split_name:\n",
    "        continue\n",
    "\n",
    "    set_name = split_name.split(\"_BERT\")[0]\n",
    "    if set_name in split_label_value_counts:\n",
    "        continue\n",
    "\n",
    "    split_frame = inference_logs[split_name].to_pandas()\n",
    "    label_counts = split_frame[\"label\"].value_counts().to_dict()\n",
    "    label_counts[-1] = len(split_frame)\n",
    "    label_counts = {k: label_counts[k] for k in sorted(label_counts)}\n",
    "    split_label_value_counts[set_name] = label_counts\n",
    "\n",
    "pd.DataFrame(split_label_value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Across Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = [split for split in inference_logs.keys() if \"Ablate_Data\" in split]\n",
    "datasets = set([(\"_\".join(split.split(\"_\")[:2]).replace(\"_Ablate\", \"\")) for split in all_splits if \"Ablate_Data\" in split])\n",
    "results = {}\n",
    "# print(datasets)\n",
    "\n",
    "for task_name in datasets:\n",
    "    results[task_name] = {}\n",
    "\n",
    "    for split_name in tqdm(all_splits, desc=task_name):\n",
    "        if task_name in split_name:\n",
    "            data_count = int(split_name.split(\"_\")[-2].replace(\"BERT\", \"\"))\n",
    "            tta_method = split_name.split(\"_\")[-1]\n",
    "            shift_name = split_name.split(\"_\")[-3]\n",
    "            # print(dataset, data_count, tta_method, shift_name)\n",
    "\n",
    "            baseline_accuracy = classification_report(inference_logs[split_name][\"label\"], inference_logs[split_name][\"original_predicted_class\"], output_dict=True)[\"accuracy\"]\n",
    "            tta_accuracy = classification_report(inference_logs[split_name][\"label\"], inference_logs[split_name][\"tta_predicted_class\"], output_dict=True)[\"accuracy\"]\n",
    "\n",
    "            if task_name not in results:\n",
    "                results[task_name] = {}\n",
    "            if data_count not in results[task_name]:\n",
    "                results[task_name][data_count] = {}\n",
    "            if tta_method not in results[task_name][data_count]:\n",
    "                results[task_name][data_count][tta_method] = {}\n",
    "            if shift_name not in results[task_name][data_count][tta_method]:\n",
    "                results[task_name][data_count][tta_method][shift_name] = {}\n",
    "\n",
    "            results[task_name][data_count][tta_method][shift_name] = {\n",
    "                \"method\": tta_method,\n",
    "                \"baseline_accuracy\": baseline_accuracy,\n",
    "                \"tta_accuracy\": tta_accuracy,\n",
    "                \"baseline_delta\": tta_accuracy - baseline_accuracy,\n",
    "            }\n",
    "\n",
    "            # inference_frames[task_name] = inference_logs[split].to_pandas()\n",
    "            # break\n",
    "\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_method_names = {\n",
    "    \"Insert\": \"Insert\",\n",
    "    \"Substitute\": \"Substitute\",\n",
    "    \"Translate\": \"Translate\",\n",
    "    \"Paraphrase\": \"LLM-TTA: Paraphrase\",\n",
    "    \"ICR\": \"LLM-TTA: ICR\",\n",
    "}\n",
    "\n",
    "method_avg_delta = {}\n",
    "for task_name in results:\n",
    "    for data_count in results[task_name]:\n",
    "        tta_perf_deltas = {}\n",
    "        for tta_method in results[task_name][data_count]:\n",
    "            for ood_shift in results[task_name][data_count][tta_method]:\n",
    "                if tta_method not in tta_perf_deltas:\n",
    "                    tta_perf_deltas[tta_method] = []\n",
    "\n",
    "                shift_method_perf_deta = results[task_name][data_count][tta_method][ood_shift][\"baseline_delta\"]\n",
    "                tta_perf_deltas[tta_method].append(shift_method_perf_deta)\n",
    "\n",
    "        if task_name not in method_avg_delta:\n",
    "            method_avg_delta[task_name] = {}\n",
    "\n",
    "        method_avg_delta[task_name][data_count] = { tta_method: np.mean(tta_perf_deltas[tta_method]) for tta_method in tta_perf_deltas }\n",
    "        baseline = np.mean([method_avg_delta[task_name][data_count][\"Insert\"], method_avg_delta[task_name][data_count][\"Substitute\"], method_avg_delta[task_name][data_count][\"Translate\"]])\n",
    "        method_avg_delta[task_name][data_count][\"Conventional Augmentation\"] = baseline\n",
    "\n",
    "pandas_form = {task_name: {} for task_name in results}\n",
    "for task_name in pandas_form:\n",
    "    method_perfs = {}\n",
    "\n",
    "    for data_count in method_avg_delta[task_name]:\n",
    "        for tta_method in [\"Insert\", \"Substitute\", \"Translate\", \"Paraphrase\", \"ICR\"]:\n",
    "            if data_count not in pandas_form[task_name]:\n",
    "                pandas_form[task_name][data_count] = []\n",
    "\n",
    "            pandas_form[task_name][data_count].append({\n",
    "                \"data_count\": data_count,\n",
    "                \"tta_method\": formatted_method_names[tta_method],\n",
    "                \"avg_delta\": method_avg_delta[task_name][data_count][tta_method],\n",
    "            })\n",
    "\n",
    "            if tta_method not in method_perfs:\n",
    "                method_perfs[tta_method] = []\n",
    "            method_perfs[tta_method].append(method_avg_delta[task_name][data_count][tta_method])\n",
    "\n",
    "    # calculate the standard deviation in avg_delta\n",
    "    for data_count in pandas_form[task_name]:\n",
    "        for tta_method in [\"Insert\", \"Substitute\", \"Translate\", \"Paraphrase\", \"ICR\"]:\n",
    "            for perf_record in pandas_form[task_name][data_count]:\n",
    "                if perf_record[\"tta_method\"] == formatted_method_names[tta_method]:\n",
    "                    perf_record[\"std_delta\"] = np.std(method_perfs[tta_method])\n",
    "\n",
    "\n",
    "print(json.dumps(pandas_form, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_method_names = {\n",
    "#     \"Insert\": \"Insert\",\n",
    "#     \"Substitute\": \"Substitute\",\n",
    "#     \"Translate\": \"Translate\",\n",
    "#     \"Paraphrase\": \"LLM-TTA: Paraphrase\",\n",
    "#     \"ICR\": \"LLM-TTA: ICR\",\n",
    "# }\n",
    "\n",
    "# # method_avg_delta = {}\n",
    "# # for task_name in results:\n",
    "# #     for data_count in results[task_name]:\n",
    "# #         tta_perf_deltas = {}\n",
    "# #         for tta_method in results[task_name][data_count]:\n",
    "# #             for ood_shift in results[task_name][data_count][tta_method]:\n",
    "# #                 if tta_method not in tta_perf_deltas:\n",
    "# #                     tta_perf_deltas[tta_method] = []\n",
    "\n",
    "# #                 shift_method_perf_deta = results[task_name][data_count][tta_method][ood_shift][\"baseline_delta\"]\n",
    "# #                 tta_perf_deltas[tta_method].append(shift_method_perf_deta)\n",
    "\n",
    "# #         if task_name not in method_avg_delta:\n",
    "# #             method_avg_delta[task_name] = {}\n",
    "\n",
    "# #         method_avg_delta[task_name][data_count] = { tta_method: np.mean(tta_perf_deltas[tta_method]) for tta_method in tta_perf_deltas }\n",
    "# #         baseline = np.mean([method_avg_delta[task_name][data_count][\"Insert\"], method_avg_delta[task_name][data_count][\"Substitute\"], method_avg_delta[task_name][data_count][\"Translate\"]])\n",
    "# #         method_avg_delta[task_name][data_count][\"Conventional Augmentation\"] = baseline\n",
    "\n",
    "# pandas_form = {task_name: {} for task_name in results}\n",
    "# for task_name in results:\n",
    "#     method_perfs = {}\n",
    "\n",
    "#     for data_count in results[task_name]:\n",
    "#         for tta_method in [\"Insert\", \"Substitute\", \"Translate\", \"Paraphrase\", \"ICR\"]:\n",
    "#             for ood_split in results[task_name][data_count][tta_method]:\n",
    "#                 if data_count not in pandas_form[task_name]:\n",
    "#                     pandas_form[task_name][data_count] = []\n",
    "\n",
    "#                 data_ablation_results_record = results[task_name][data_count][tta_method][ood_split]\n",
    "#                 pandas_form[task_name][data_count].append({\n",
    "#                     \"data_count\": data_count,\n",
    "#                     \"tta_method\": data_ablation_results_record[\"method\"],\n",
    "#                     \"avg_delta\": data_ablation_results_record[\"baseline_delta\"],\n",
    "#                 })\n",
    "\n",
    "#             # if tta_method not in method_perfs:\n",
    "#             #     method_perfs[tta_method] = []\n",
    "#             # method_perfs[tta_method].append(method_avg_delta[task_name][data_count][tta_method])\n",
    "\n",
    "#     # calculate the standard deviation in avg_delta\n",
    "#     # for data_count in pandas_form[task_name]:\n",
    "#     #     for tta_method in [\"Insert\", \"Substitute\", \"Translate\", \"Paraphrase\", \"ICR\"]:\n",
    "#     #         for perf_record in pandas_form[task_name][data_count]:\n",
    "#     #             if perf_record[\"tta_method\"] == formatted_method_names[tta_method]:\n",
    "#     #                 perf_record[\"std_delta\"] = np.std(method_perfs[tta_method])\n",
    "\n",
    "\n",
    "# print(json.dumps(pandas_form, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(3 * FIG_SIZE, FIG_SIZE - 1))\n",
    "for i, task_name in enumerate([\"BOSS_Sentiment\", \"BOSS_Toxicity\", \"AgNewsTweets\"]):\n",
    "    df = pd.concat([pd.DataFrame(pandas_form[task_name][data_count]) for data_count in pandas_form[task_name]])\n",
    "    sns.lineplot(\n",
    "        data=df,\n",
    "        x=\"data_count\",\n",
    "        y=\"avg_delta\",\n",
    "        hue=\"tta_method\",\n",
    "        ax=axes[i],\n",
    "        linewidth=LINE_WIDTH,\n",
    "        markersize=MARKER_SIZE,\n",
    "        style=\"tta_method\",\n",
    "        dashes=False,\n",
    "        markers=True,\n",
    "        errorbar=\"sd\",\n",
    "    )\n",
    "\n",
    "    # set x label to Training Set Size\n",
    "    axes[i].set_xlabel(\"Training Set Size\")\n",
    "\n",
    "    # set y label to Mean Absolute Accuracy Delta\n",
    "    axes[i].set_ylabel(\"Accuracy Gain\")\n",
    "\n",
    "    # make the y axis percents that go to the hundreds place\n",
    "    axes[i].yaxis.set_major_formatter(lambda x, pos: f\"{x:.0%}\")\n",
    "\n",
    "    # standardize the y axis between the min and max of df\n",
    "    axes[i].set_ylim(df[\"avg_delta\"].min() - 0.005, df[\"avg_delta\"].max() + 0.005)\n",
    "\n",
    "    title_text = {\n",
    "        \"BOSS_Sentiment\": \"Sentiment\",\n",
    "        \"BOSS_Toxicity\": \"Toxicity\",\n",
    "        \"AgNewsTweets\": \"News\",\n",
    "    }\n",
    "    axes[i].set_title(title_text[task_name], fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "    # se legend to the bottom left\n",
    "    axes[i].legend(loc=\"lower right\")\n",
    "\n",
    "    # make x axis log\n",
    "    axes[i].set_xscale(\"log\")\n",
    "\n",
    "    # show more x axis ticks at 10%, 20%, 40%, 80%\n",
    "    x_ticks = sorted(df[\"data_count\"].unique().tolist())\n",
    "    tick_labels = [str(int(x)) for x in x_ticks]\n",
    "    axes[i].set_xticks(x_ticks, tick_labels, minor=False)\n",
    "\n",
    "    # remove extra ticks not in x_ticks\n",
    "    axes[i].set_xticks(x_ticks, minor=True)\n",
    "\n",
    "    # rotate x axis labels\n",
    "    axes[i].tick_params(axis='x', rotation=X_LABEL_ROTATION)\n",
    "\n",
    "    # if news add addiitonal precision to the y axis\n",
    "    # if task_name == \"AgNewsTweets\":\n",
    "    axes[i].yaxis.set_major_formatter(lambda x, pos: f\"{x:.1%}\")\n",
    "\n",
    "    # remove leegnd in not middle plot\n",
    "    if i != 1:\n",
    "        axes[i].get_legend().remove()\n",
    "    else:\n",
    "        # center below plot with no frame\n",
    "        axes[i].legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.30), ncol=5, frameon=False, fontsize=14)\n",
    "\n",
    "# add padding for labels\n",
    "fig.subplots_adjust(wspace=WSPACE + 0.05, hspace=WSPACE)\n",
    "\n",
    "if not os.path.exists(\"figures/\"):\n",
    "    os.makedirs(\"figures/\")\n",
    "fig.savefig(\"figures/method_analysis_data_ablation.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_sentiment_icr_data = None\n",
    "ood_toxicity_icr_data = None\n",
    "ood_tweets_icr_data = None\n",
    "for split_name in tqdm(inference_logs.keys()):\n",
    "    current_frame = inference_logs[split_name].to_pandas()\n",
    "    current_frame[\"dataset\"] = split_name\n",
    "\n",
    "    if \"Sentiment\" in split_name and \"ICR\" in split_name and \"ID\" not in split_name and \"BERT\" in split_name and \"Ablate\" not in split_name:\n",
    "        if ood_sentiment_icr_data is None:\n",
    "            ood_sentiment_icr_data = current_frame\n",
    "        else:\n",
    "            ood_sentiment_icr_data = pd.concat([ood_sentiment_icr_data, current_frame])\n",
    "\n",
    "    if \"Toxicity\" in split_name and \"ICR\" in split_name and \"ID\" not in split_name and \"BERT\" in split_name and \"Ablate\" not in split_name:\n",
    "        if ood_toxicity_icr_data is None:\n",
    "            ood_toxicity_icr_data = current_frame\n",
    "        else:\n",
    "            ood_toxicity_icr_data = pd.concat([ood_toxicity_icr_data, current_frame])\n",
    "\n",
    "    if \"Tweets\" in split_name and \"ICR\" in split_name and \"ID\" not in split_name and \"BERT\" in split_name and \"Ablate\" not in split_name:\n",
    "        if ood_tweets_icr_data is None:\n",
    "            ood_tweets_icr_data = current_frame\n",
    "        else:\n",
    "            ood_tweets_icr_data = pd.concat([ood_tweets_icr_data, current_frame])\n",
    "\n",
    "\n",
    "display(ood_sentiment_icr_data.value_counts(\"dataset\"))\n",
    "display(ood_toxicity_icr_data.value_counts(\"dataset\"))\n",
    "display(ood_tweets_icr_data.value_counts(\"dataset\"))\n",
    "assert len(ood_sentiment_icr_data.value_counts(\"dataset\")) == 3\n",
    "assert len(ood_toxicity_icr_data.value_counts(\"dataset\")) == 3\n",
    "assert len(ood_tweets_icr_data.value_counts(\"dataset\")) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_predictions(predictions, num_predictions, use_test_input):\n",
    "    try:\n",
    "        ablation_preds = predictions[:num_predictions]\n",
    "        if use_test_input:\n",
    "            ablation_preds = ablation_preds.tolist() + [predictions[-1]]\n",
    "        \n",
    "        mean_distribution = np.mean(ablation_preds, axis=0) if len(ablation_preds) > 1 else ablation_preds[0]\n",
    "        predicted_class = np.argmax(mean_distribution)\n",
    "        return predicted_class\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "\n",
    "perf_records = []\n",
    "inference_ablation_splits = [split for split in inference_logs.keys() if \"Ablate\" not in split and \"BERT\" in split]\n",
    "for split_name in tqdm(inference_ablation_splits):\n",
    "    dataset = parse_task_name(split_name)\n",
    "    distribution = parse_distribution(split_name)\n",
    "    model = parse_model(split_name)\n",
    "    method = parse_tta_method(split_name)\n",
    "\n",
    "    current_frame = inference_logs[split_name].to_pandas()\n",
    "    for use_source in [False, True]:\n",
    "        for num_augmentations in range(1, 5):\n",
    "            judgments = current_frame[\"tta_all_class_probs\"].apply(lambda x: aggregate_predictions(x, num_augmentations, use_source))\n",
    "            accuracy = classification_report(current_frame[\"label\"], judgments, output_dict=True)[\"accuracy\"]\n",
    "            perf_records.append({\n",
    "                \"dataset\": dataset,\n",
    "                \"distribution\": distribution,\n",
    "                \"model\": model,\n",
    "                \"method\": formatted_method_names[method],\n",
    "                \"use_source\": use_source,\n",
    "                \"num_augmentations\": num_augmentations,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"delta\": accuracy - no_tta_accuracies[split_name],\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_ablation_frame = pd.DataFrame(perf_records)\n",
    "\n",
    "# select where distribution != ID\n",
    "aggregation_ablation_frame = aggregation_ablation_frame[aggregation_ablation_frame[\"distribution\"] != \"ID\"]\n",
    "\n",
    "# set method to TTA if not Paraphrase or ICR and LLM-TTA if Paraphrase or ICR\n",
    "# aggregation_ablation_frame[\"method\"] = aggregation_ablation_frame[\"method\"].apply(lambda x: \"TTA\" if x not in [\"Paraphrase\", \"ICR\"] else \"LLM-TTA\")\n",
    "\n",
    "# for tta_method in [\"Conventional Augmentation\", \"Paraphrase\", \"ICR\"]:\n",
    "# aggregation_ablation_frame[\"method\"] = aggregation_ablation_frame[\"method\"].apply(lambda x: \"ICR\" if \"ICR\" in x else \"Paraphrase\" if \"Paraphrase\" in x else \"Conventional Augmentation\")\n",
    "\n",
    "# create three figures, one for Sentiment, Toxicity, and News\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(3 * FIG_SIZE, FIG_SIZE - 1))\n",
    "for index, task_name in enumerate([\"Sentiment\", \"Toxicity\", \"News\"]):\n",
    "    # get the current task frame\n",
    "    current_frame = aggregation_ablation_frame[aggregation_ablation_frame[\"dataset\"] == task_name]\n",
    "\n",
    "    # where use_source is True\n",
    "    current_frame_source = current_frame[current_frame[\"use_source\"] == True]\n",
    "\n",
    "    # plot the current task frame\n",
    "    sns.lineplot(\n",
    "        data=current_frame,\n",
    "        x=\"num_augmentations\",\n",
    "        y=\"delta\",\n",
    "        hue=\"method\",\n",
    "        ax=axes[index],\n",
    "        linewidth=LINE_WIDTH,\n",
    "        ci=None,\n",
    "        style=\"method\",\n",
    "        dashes=False,\n",
    "        markers=True,\n",
    "        markersize=MARKER_SIZE)\n",
    "\n",
    "    # set x label to Number of Augmentations\n",
    "    axes[index].set_xlabel(\"Number of Augmentations\")\n",
    "\n",
    "    # set y label to Accuracy\n",
    "    axes[index].set_ylabel(\"Accuracy Gain\")\n",
    "\n",
    "    # make the y axis percents that go to the hundreds place\n",
    "    axes[index].yaxis.set_major_formatter(lambda x, pos: f\"{x:.0%}\")\n",
    "\n",
    "    # set title to Sentiment, Toxicity, or News\n",
    "    axes[index].set_title(task_name, fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "    # se legend to the bottom left\n",
    "    axes[index].legend(loc=\"lower right\")\n",
    "\n",
    "    # x axis ticks are five equally spaced ticks between the min and max of the x axis\n",
    "    axes[index].set_xticks(np.linspace(current_frame[\"num_augmentations\"].min(), current_frame[\"num_augmentations\"].max(), 4))\n",
    "\n",
    "    # remove leegnd in not middle plot\n",
    "    if index != 1:\n",
    "        axes[index].get_legend().remove()\n",
    "    else:\n",
    "        # center below plot with no frame\n",
    "        axes[index].legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.20), ncol=5, frameon=False, fontsize=14)\n",
    "    \n",
    "    # have y ticks to the tenths place\n",
    "    axes[index].yaxis.set_major_formatter(lambda x, pos: f\"{x:.1%}\")\n",
    "\n",
    "# set font size for labels and ticks\n",
    "# for ax in axes:\n",
    "#     ax.tick_params(axis=\"both\", labelsize=14)\n",
    "#     ax.xaxis.label.set_size(TITLE_FONT_SIZE)\n",
    "#     ax.yaxis.label.set_size(TITLE_FONT_SIZE)\n",
    "\n",
    "# add padding for labels\n",
    "fig.subplots_adjust(wspace=WSPACE + 0.05, hspace=WSPACE)\n",
    "\n",
    "fig.savefig(\"figures/method_analysis_aggrgeation_ablation.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't crop the display frame\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "aggregation_ablation_frame = pd.DataFrame(perf_records)\n",
    "for dataset in [\"Sentiment\", \"Toxicity\", \"News\"]:\n",
    "    dataset_frame = aggregation_ablation_frame[aggregation_ablation_frame[\"dataset\"] == dataset]\n",
    "    dataset_frame[\"distribution\"] = dataset_frame[\"distribution\"].apply(lambda x: \"ID\" if \"ID\" in x else \"OOD\")\n",
    "    display(dataset_frame.groupby([\"dataset\", \"distribution\", \"model\", \"method\", \"use_source\", \"num_augmentations\"]).mean().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does TTA Effect Some Classes More Than Others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataset, get the percent of examples that are unchanged vs new\n",
    "sentiment_outcomes = ood_sentiment_icr_data[\"outcome\"].apply(lambda x: \"Correction\" if x == \"New Correct\" else \"Corruption\" if x == \"New Mistake\" else x)\n",
    "sentiment_percents = sentiment_outcomes.value_counts(normalize=True)\n",
    "new_sentiment_percents = 100 * sentiment_percents[sentiment_percents.index == \"Correction\"].values[0] + sentiment_percents[sentiment_percents.index == \"Corruption\"].values[0]\n",
    "print(f\"Sentiment: {new_sentiment_percents:.2f}% of examples are new predictions\")\n",
    "\n",
    "toxicity_outcomes = ood_toxicity_icr_data[\"outcome\"].apply(lambda x: \"Correction\" if x == \"New Correct\" else \"Corruption\" if x == \"New Mistake\" else x)\n",
    "toxicity_percents = toxicity_outcomes.value_counts(normalize=True)\n",
    "new_toxicity_percents = 100 * toxicity_percents[toxicity_percents.index == \"Correction\"].values[0] + toxicity_percents[toxicity_percents.index == \"Corruption\"].values[0]\n",
    "print(f\"Toxicity: {new_toxicity_percents:.2f}% of examples are new predictions\")\n",
    "\n",
    "agt_outcomes = ood_tweets_icr_data[\"outcome\"].apply(lambda x: \"Correction\" if x == \"New Correct\" else \"Corruption\" if x == \"New Mistake\" else x)\n",
    "agt_percents = agt_outcomes.value_counts(normalize=True)\n",
    "new_agt_percents = 100 * agt_percents[agt_percents.index == \"Correction\"].values[0] + agt_percents[agt_percents.index == \"Corruption\"].values[0]\n",
    "print(f\"AGT: {new_agt_percents:.2f}% of examples are new predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_outcomes = [\"Correction\", \"Corruption\"]\n",
    "# ood_sentiment_icr_data[\"outcome\"] = ood_sentiment_icr_data[\"outcome\"].apply(lambda x: \"Correction\" if x == \"New Correct\" else \"Corruption\" if x == \"New Mistake\" else None)\n",
    "# ood_sentiment_icr_data.dropna(inplace=True)\n",
    "# sentiment_icr_outcome_percents = ood_sentiment_icr_data[[\"outcome\", \"label\"]].sort_values(\"outcome\").value_counts([\"outcome\", \"label\"], normalize=True).sort_index()\n",
    "# print(\"Sentiment ICR\")\n",
    "# display(sentiment_icr_outcome_percents)\n",
    "\n",
    "# ood_toxicity_icr_data[\"outcome\"] = ood_toxicity_icr_data[\"outcome\"].apply(lambda x: \"Correction\" if x == \"New Correct\" else \"Corruption\" if x == \"New Mistake\" else None)\n",
    "# ood_toxicity_icr_data.dropna(inplace=True)\n",
    "# toxicity_icr_outcome_percents = ood_toxicity_icr_data[[\"outcome\", \"label\"]].sort_values(\"outcome\").value_counts([\"outcome\", \"label\"], normalize=True).sort_index()\n",
    "# print(\"Toxicity ICR\")\n",
    "# display(toxicity_icr_outcome_percents)\n",
    "\n",
    "# ood_tweets_icr_data[\"outcome\"] = ood_tweets_icr_data[\"outcome\"].apply(lambda x: \"Correction\" if x == \"New Correct\" else \"Corruption\" if x == \"New Mistake\" else None)\n",
    "# ood_tweets_icr_data.dropna(inplace=True)\n",
    "# tweets_icr_outcome_percents = ood_tweets_icr_data[[\"outcome\", \"label\"]].sort_values(\"outcome\").value_counts([\"outcome\", \"label\"], normalize=True).sort_index()\n",
    "# print(\"Tweets ICR\")\n",
    "# display(tweets_icr_outcome_percents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_icr_outcome_percents = ood_sentiment_icr_data[[\"outcome\", \"label\"]].sort_values(\"outcome\").value_counts([\"outcome\", \"label\"], normalize=True).sort_index()\n",
    "sentiment_icr_outcome_percents = sentiment_icr_outcome_percents[sentiment_icr_outcome_percents.index.get_level_values(\"outcome\").str.contains(\"New\")]\n",
    "print(\"Sentiment ICR\")\n",
    "display(sentiment_icr_outcome_percents)\n",
    "\n",
    "toxicity_icr_outcome_percents = ood_toxicity_icr_data[[\"outcome\", \"label\"]].sort_values(\"outcome\").value_counts([\"outcome\", \"label\"], normalize=True).sort_index()\n",
    "toxicity_icr_outcome_percents = toxicity_icr_outcome_percents[toxicity_icr_outcome_percents.index.get_level_values(\"outcome\").str.contains(\"New\")]\n",
    "print(\"Toxicity ICR\")\n",
    "display(toxicity_icr_outcome_percents)\n",
    "\n",
    "tweets_icr_outcome_percents = ood_tweets_icr_data[[\"outcome\", \"label\"]].sort_values(\"outcome\").value_counts([\"outcome\", \"label\"], normalize=True).sort_index()\n",
    "tweets_icr_outcome_percents = tweets_icr_outcome_percents[tweets_icr_outcome_percents.index.get_level_values(\"outcome\").str.contains(\"New\")]\n",
    "print(\"Tweets ICR\")\n",
    "display(tweets_icr_outcome_percents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig, axes = plt.subplots(1, 3, figsize=(3 * FIG_SIZE, FIG_SIZE - 1))\n",
    "\n",
    "sentiment_labels = {\n",
    "    0: \"Negative\",\n",
    "    1: \"Positive\",\n",
    "    2: \"Neutral\",\n",
    "}\n",
    "sns.barplot(ax=axes[0], \n",
    "            x=sentiment_icr_outcome_percents.index.get_level_values(\"label\").map(sentiment_labels), \n",
    "            y=sentiment_icr_outcome_percents.values,\n",
    "            hue=sentiment_icr_outcome_percents.index.get_level_values(\"outcome\"),\n",
    ")\n",
    "\n",
    "toxicity_labels = {\n",
    "    0: \"Non-Toxic\",\n",
    "    1: \"Toxic\",\n",
    "}\n",
    "sns.barplot(ax=axes[1],\n",
    "            x=toxicity_icr_outcome_percents.index.get_level_values(\"label\").map(toxicity_labels),\n",
    "            y=toxicity_icr_outcome_percents.values,\n",
    "            hue=toxicity_icr_outcome_percents.index.get_level_values(\"outcome\"),\n",
    ")\n",
    "\n",
    "agt_labels = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\",\n",
    "}\n",
    "sns.barplot(ax=axes[2],\n",
    "            x=tweets_icr_outcome_percents.index.get_level_values(\"label\").map(agt_labels),\n",
    "            y=tweets_icr_outcome_percents.values,\n",
    "            hue=tweets_icr_outcome_percents.index.get_level_values(\"outcome\"),\n",
    ")\n",
    "\n",
    "for i in range(3):\n",
    "    # make y axis percents\n",
    "    axes[i].yaxis.set_major_formatter(lambda x, pos: f\"{x:.0%}\")\n",
    "\n",
    "    # standardize between 0 and 0.1\n",
    "    axes[i].set_ylim(0, 0.09)\n",
    "\n",
    "    # set y label to percent of overall outcomes\n",
    "    axes[i].set_ylabel(\"Percent of Outcomes\")\n",
    "\n",
    "    # remove x label\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "    if i == 2:\n",
    "        # rotate the x labels\n",
    "        axes[i].tick_params(axis=\"x\", rotation=X_LABEL_ROTATION)\n",
    "\n",
    "    # have fewer ticks on the y axis\n",
    "    axes[i].locator_params(axis=\"y\", nbins=5)\n",
    "\n",
    "    # set titles\n",
    "    title_text = {\n",
    "        0: \"Sentiment\",\n",
    "        1: \"Toxicity\",\n",
    "        2: \"News\",\n",
    "    }\n",
    "    axes[i].set_title(title_text[i], fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "    # have a single legend which is centered below the plot\n",
    "    if i == 1:\n",
    "        axes[i].legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.20), ncol=3, frameon=False, fontsize=14)\n",
    "    else:\n",
    "        axes[i].get_legend().remove()\n",
    "\n",
    "# add more horizental spacing for y labels\n",
    "fig.subplots_adjust(wspace=WSPACE)\n",
    "fig.savefig(\"figures/method_analysis_class_analysis.png\", bbox_inches=\"tight\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three figures on single line\n",
    "fig, axes = plt.subplots(1, 2, figsize=(3 * FIG_SIZE, FIG_SIZE))\n",
    "plottng_datas = [(\"Sentiment\", ood_sentiment_icr_data), (\"Toxicity\", ood_toxicity_icr_data), (\"News\", ood_tweets_icr_data)]\n",
    "change_int_mapping = {\n",
    "    \"Unchanged\": 0,\n",
    "    \"New\": 1,\n",
    "}\n",
    "outcome_int_mapping = {\n",
    "    \"Unchanged Correct\": 0,\n",
    "    \"Unfixed Mistake\": 1,\n",
    "    \"New Correct\": 2,\n",
    "    \"New Mistake\": 3,\n",
    "}\n",
    "\n",
    "change_correlations = {}\n",
    "change_p_values = []\n",
    "outcome_correlations = []\n",
    "outcome_p_values = {}\n",
    "for index, (title, icr_frame) in enumerate(plottng_datas):\n",
    "    icr_frame[\"max_softmax\"] = icr_frame[\"tta_mean_class_probs\"].apply(lambda p: max(p))\n",
    "    plotting_frame = icr_frame[[\"max_softmax\", \"original_prediction_entropy\", \"tta_prediction_entropy\", \"outcome\"]].sort_values(\"outcome\")\n",
    "    plotting_frame[\"Change\"] = plotting_frame[\"outcome\"].apply(lambda x: 0 if x == \"Unchanged Correct\" or x == \"Unfixed Mistake\" else 1)\n",
    "\n",
    "    correlation_metric = \"original_prediction_entropy\"\n",
    "    pbc = pointbiserialr(plotting_frame[correlation_metric], plotting_frame[\"Change\"])\n",
    "    # change_correlations.append([pbc[0]])\n",
    "    change_correlations[title] = pbc[0]\n",
    "    change_p_values.append([pbc[1]])\n",
    "\n",
    "    set_outcome_correlations = []\n",
    "    for outcome in outcome_int_mapping:\n",
    "        one_hot_outcomes = plotting_frame[\"outcome\"].apply(lambda x: 1 if x == outcome else 0)\n",
    "        pbc = pointbiserialr(plotting_frame[correlation_metric], one_hot_outcomes)\n",
    "        set_outcome_correlations.append(pbc[0])\n",
    "\n",
    "        if title not in outcome_p_values:\n",
    "            outcome_p_values[title] = {}\n",
    "        \n",
    "        outcome_p_values[title][outcome] = pbc[1]\n",
    "    \n",
    "    outcome_correlations.append(set_outcome_correlations)\n",
    "\n",
    "display(change_correlations)\n",
    "sns.barplot(ax=axes[0],\n",
    "            data=pd.DataFrame(change_correlations, index=[\"Correlation\"]),\n",
    "            color=\"#3274A1\",\n",
    "            width=0.5,\n",
    "            )\n",
    "\n",
    "\n",
    "# Y label as \"Correlation\"\n",
    "axes[0].set_ylabel(\"Correlation\")\n",
    "# have five ticks on the y axis\n",
    "axes[0].locator_params(axis=\"y\", nbins=8)\n",
    "\n",
    "sns.heatmap(outcome_correlations,\n",
    "            annot=True,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            ax=axes[1],\n",
    "            cmap=\"coolwarm\",\n",
    "            xticklabels=[\"Unchanged Correct\", \"Unfixed Mistake\", \"New Correct\", \"New Mistake\"],\n",
    "            yticklabels=[\"Sentiment\", \"Toxicity\", \"News\"])\n",
    "\n",
    "plt.xticks(rotation=X_LABEL_ROTATION, ha=\"right\")\n",
    "fig.subplots_adjust(wspace=WSPACE / 1.5)\n",
    "fig.savefig(\"figures/method_analysis_entropy_correlations.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three figures on single line\n",
    "fig, axes = plt.subplots(1, 3, figsize=(3 * FIG_SIZE, FIG_SIZE - 1))\n",
    "plottng_datas = [(\"Sentiment\", ood_sentiment_icr_data), (\"Toxicity\", ood_toxicity_icr_data), (\"News\", ood_tweets_icr_data)]\n",
    "\n",
    "correlations = []\n",
    "\n",
    "for index, (title, icr_frame) in tqdm(enumerate(plottng_datas)):\n",
    "    axes[index].set_title(title, fontsize=TITLE_FONT_SIZE)\n",
    "    icr_frame[\"max_softmax\"] = icr_frame[\"tta_mean_class_probs\"].apply(lambda p: max(p))\n",
    "    plotting_frame = icr_frame[[\"max_softmax\", \"original_prediction_entropy\", \"tta_prediction_entropy\", \"outcome\"]].sort_values(\"outcome\")\n",
    "    plotting_frame[\"Prediction\"] = plotting_frame[\"outcome\"].apply(lambda x: \"Unchanged Prediction\" if x == \"Unchanged Correct\" or x == \"Unfixed Mistake\" else \"New Prediction\")\n",
    "\n",
    "    # create kernal density estimate plot for original prediction entropy by outcome\n",
    "    sns.kdeplot(\n",
    "        data=plotting_frame,\n",
    "        x=\"original_prediction_entropy\",\n",
    "        hue=\"Prediction\",\n",
    "        fill=True,\n",
    "        ax=axes[index],\n",
    "        linewidth=LINE_WIDTH,\n",
    "        # alpha=0.5,\n",
    "        common_norm=False,\n",
    "    )\n",
    "\n",
    "    # log y and x axis\n",
    "    # axes[index].set_yscale(\"log\")\n",
    "    axes[index].set_xscale(\"log\")\n",
    "\n",
    "    # move legend to the top left\n",
    "    if index == 1:\n",
    "        axes[index].legend(loc=\"upper center\", labels=[\"Unchanged\", \"New\"], bbox_to_anchor=(0.5, -0.20), ncol=2, frameon=False, fontsize=14)\n",
    "    else:\n",
    "        axes[index].get_legend().remove()\n",
    "\n",
    "# set font size for labels and ticks\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis=\"both\", labelsize=14)\n",
    "    ax.xaxis.label.set_size(TITLE_FONT_SIZE)\n",
    "    ax.yaxis.label.set_size(TITLE_FONT_SIZE)\n",
    "\n",
    "fig.subplots_adjust(wspace=WSPACE)\n",
    "fig.savefig(\"figures/method_analysis_entropy_distributions.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Optimal ID Entropy Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_augment_entropy(threshold, row):\n",
    "    return row[\"original_prediction_entropy\"] >= threshold\n",
    "\n",
    "\n",
    "def get_entropy_threshold_accuracy(threshold, inference_logs_frame):\n",
    "    threshold_judgments = inference_logs_frame.apply(lambda row: row[\"tta_predicted_class\"] if should_augment_entropy(threshold, row) else row[\"original_predicted_class\"], axis=1)\n",
    "    report = classification_report(inference_logs_frame[\"label\"], threshold_judgments, digits=4, output_dict=True, zero_division=0)\n",
    "    llm_call_count = inference_logs_frame.apply(lambda row: should_augment_entropy(threshold, row), axis=1).sum()\n",
    "    llm_call_rate = llm_call_count / len(inference_logs_frame)\n",
    "    return report[\"accuracy\"], llm_call_rate\n",
    "\n",
    "\n",
    "def should_augment_softmax(threshold, row):\n",
    "    try:\n",
    "        return row[\"tta_all_class_probs\"][-1].max() < threshold\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_max_softmax_threshold_accuracy(threshold, inference_logs_frame):\n",
    "    threshold_judgments = inference_logs_frame.apply(lambda row: row[\"tta_predicted_class\"] if should_augment_softmax(threshold, row) else row[\"original_predicted_class\"], axis=1)\n",
    "    report = classification_report(inference_logs_frame[\"label\"], threshold_judgments, digits=4, output_dict=True, zero_division=0)\n",
    "    llm_call_count = (inference_logs_frame[\"original_prediction_entropy\"] >= threshold).sum()\n",
    "    llm_call_rate = llm_call_count / len(inference_logs_frame)\n",
    "    return report[\"accuracy\"], llm_call_rate\n",
    "\n",
    "thresholds = np.arange(0, 1.2, 0.0001)\n",
    "print(f\"Number of thresholds: {len(thresholds)}\")\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOD Entropy Threshold Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate all the tresholds for each OOD split for BERT ICR. \n",
    "# 2. Get the manual threshold for each OOD split for BERT ICR at different augmentation rates.\n",
    "\n",
    "thresholds_dict = {}\n",
    "thresholds_path = f\"data/thresholds_dict_{len(thresholds)}.json\"\n",
    "if not os.path.exists(thresholds_path):\n",
    "    for ood_icr_data in [ood_sentiment_icr_data, ood_toxicity_icr_data, ood_tweets_icr_data]:\n",
    "        for split in ood_icr_data[\"dataset\"].unique():\n",
    "            print(split)\n",
    "            thresholds_dict[split] = {}\n",
    "            split_frame = ood_icr_data[ood_icr_data[\"dataset\"] == split]\n",
    "            original_accuracy = classification_report(split_frame[\"label\"], split_frame[\"original_predicted_class\"], output_dict=True)[\"accuracy\"]\n",
    "            \n",
    "            for threshold in tqdm(thresholds):\n",
    "                accuracy, llm_call_rate = get_entropy_threshold_accuracy(threshold, split_frame)\n",
    "                thresholds_dict[split][threshold] = {\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"accuracy_delta\": accuracy - original_accuracy,\n",
    "                    \"llm_call_rate\": llm_call_rate,\n",
    "                }\n",
    "    json.dump(thresholds_dict, open(thresholds_path, \"w\"), indent=4)\n",
    "else:\n",
    "    with open(thresholds_path, \"r\") as f:\n",
    "        thresholds_dict = json.load(f)\n",
    "\n",
    "print(json.dumps(thresholds_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subplot for each dataset in the thresholds dict with three on each row\n",
    "fig, axes = plt.subplots(3, 3, figsize=(3 * FIG_SIZE, 3 * FIG_SIZE))\n",
    "\n",
    "# use the first row for sentiment, second for toxicity, and third for agt\n",
    "task_keys = [\n",
    "    [key for key in thresholds_dict.keys() if \"Sentiment\" in key],\n",
    "    [key for key in thresholds_dict.keys() if \"Toxicity\" in key],\n",
    "    [key for key in thresholds_dict.keys() if \"Tweets\" in key],\n",
    "]\n",
    "for row_index, task_splits in enumerate(task_keys):\n",
    "    for col_index in range(len(task_splits)):\n",
    "        split_name = task_splits[col_index]\n",
    "        thresholds_split_frame = pd.DataFrame(thresholds_dict[split_name]).T.reset_index().sort_values(\"llm_call_rate\")\n",
    "        sns.lineplot(\n",
    "            ax=axes[row_index, col_index],\n",
    "            data=thresholds_split_frame,\n",
    "            x=\"llm_call_rate\",\n",
    "            y=\"accuracy_delta\",\n",
    "            linewidth=LINE_WIDTH)\n",
    "\n",
    "        row_titles = {\n",
    "            0: \"Sentiment\",\n",
    "            1: \"Toxicity\",\n",
    "            2: \"News\",\n",
    "        }\n",
    "        shift_name = split_name.split(\"_\")[-3]\n",
    "        axes[row_index, col_index].set_title(f\"{row_titles[row_index]}: {shift_name}\", fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "for col_index in range(3):\n",
    "    for row_index in range(3):\n",
    "        axes[row_index, col_index].set_ylabel(\"Accuracy Delta\")\n",
    "        axes[row_index, col_index].set_xlabel(\"Augmentation Rate\")\n",
    "\n",
    "        # set x and y axis to percents\n",
    "        axes[row_index, col_index].xaxis.set_major_formatter(lambda x, pos: f\"{x:.0%}\")\n",
    "\n",
    "        # multiple delta by 100 to get percent\n",
    "        axes[row_index, col_index].yaxis.set_major_formatter(lambda x, pos: f\"{round(x * 100, 2)}\")\n",
    "\n",
    "        # set y axis between -0.1 and 0.1\n",
    "        # axes[row_index, col_index].set_ylim(-0.01, 0.15)\n",
    "\n",
    "        # have five ticks on the y axis\n",
    "        axes[row_index, col_index].locator_params(axis=\"y\", nbins=8)\n",
    "\n",
    "        # delete last two plots on the final row\n",
    "        if row_index == 2 and col_index > 0:\n",
    "            axes[row_index, col_index].remove()\n",
    "\n",
    "# add padding for labels\n",
    "fig.subplots_adjust(wspace=WSPACE + 0.05, hspace=WSPACE + 0.25)\n",
    "\n",
    "# save figure\n",
    "fig.savefig(\"figures/method_analysis_all_entropy_thresholds.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_sentiment_thresholds = {}\n",
    "for split in thresholds_dict:\n",
    "    task = \"Sentiment\" if \"Sentiment\" in split else \"Toxicity\" if \"Toxicity\" in split else \"News\"\n",
    "    if task not in aggregated_sentiment_thresholds:\n",
    "        aggregated_sentiment_thresholds[task] = {}\n",
    "\n",
    "    for threshold in thresholds_dict[split]:\n",
    "        if threshold not in aggregated_sentiment_thresholds:\n",
    "            aggregated_sentiment_thresholds[task][threshold] = {\n",
    "                \"accuracy_delta\": 0,\n",
    "                \"llm_call_rate\": 0,\n",
    "            }\n",
    "\n",
    "        aggregated_sentiment_thresholds[task][threshold][\"accuracy_delta\"] += thresholds_dict[split][threshold][\"accuracy_delta\"]\n",
    "        aggregated_sentiment_thresholds[task][threshold][\"llm_call_rate\"] += thresholds_dict[split][threshold][\"llm_call_rate\"]\n",
    "\n",
    "# divide each accuracy delta by the number of splits to get the average\n",
    "for task in aggregated_sentiment_thresholds:\n",
    "    for threshold in aggregated_sentiment_thresholds[task]:\n",
    "        aggregated_sentiment_thresholds[task][threshold][\"accuracy_delta\"] /= 3\n",
    "        # aggregated_sentiment_thresholds[task][threshold][\"llm_call_rate\"] /= 3\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(3 * FIG_SIZE, FIG_SIZE - 1))\n",
    "for col_index, task in enumerate(aggregated_sentiment_thresholds):\n",
    "    sns.lineplot(\n",
    "        ax=axes[col_index],\n",
    "        data=pd.DataFrame(aggregated_sentiment_thresholds[task]).T.reset_index().sort_values(\"llm_call_rate\"),\n",
    "        x=\"llm_call_rate\",\n",
    "        y=\"accuracy_delta\",\n",
    "        linewidth=LINE_WIDTH)\n",
    "    \n",
    "\n",
    "    axes[col_index].set_title(task, fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "    # add padding for labels\n",
    "    axes[col_index].set_ylabel(\"Accuracy Delta\")\n",
    "    axes[col_index].set_xlabel(\"Augmentation Rate\")\n",
    "\n",
    "    # set x and y axis to percents\n",
    "    axes[col_index].xaxis.set_major_formatter(lambda x, pos: f\"{x:.0%}\")\n",
    "    axes[col_index].yaxis.set_major_formatter(lambda x, pos: f\"{round(x * 100, 2)}\")\n",
    "\n",
    "    # set y value between 0 and 0.1\n",
    "    # axes[col_index].set_ylim(-0.01, 0.08)\n",
    "\n",
    "    # have few ticks on the x axis\n",
    "    axes[col_index].locator_params(axis=\"x\", nbins=5)\n",
    "    axes[col_index].locator_params(axis=\"y\", nbins=5)\n",
    "\n",
    "# set font size for labels and ticks\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis=\"both\", labelsize=14)\n",
    "    ax.xaxis.label.set_size(TITLE_FONT_SIZE)\n",
    "    ax.yaxis.label.set_size(TITLE_FONT_SIZE)\n",
    "\n",
    "fig.subplots_adjust(wspace=WSPACE, hspace=WSPACE)\n",
    "fig.savefig(\"figures/method_analysis_aggregated_entropy_thresholds.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_tta_preds(row):\n",
    "    if row[\"generations\"] is not None and len(row[\"generations\"]) > 0:\n",
    "        return row[\"generations\"][:5]\n",
    "    \n",
    "    if row[\"tta_all_class_probs\"] is None:\n",
    "        return None\n",
    "\n",
    "    all_probs = row[\"tta_all_class_probs\"][:5]\n",
    "    arg_maxes = [prob_dist.argmax() for prob_dist in all_probs]\n",
    "    return arg_maxes\n",
    "\n",
    "def is_entropy_split(split_name):\n",
    "    if \"BERT\" not in split_name:\n",
    "        return False\n",
    "    if \"Ablate\" in split_name:\n",
    "        return False\n",
    "    if \"ID\" in split_name:\n",
    "        return False\n",
    "\n",
    "    return \"Paraphrase\" in split_name or \"ICR\" in split_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_entropy_thresholds = {}\n",
    "optimal_softmax_thresholds = {}\n",
    "thresholds = np.arange(0, 1.2, 0.0005)\n",
    "SAMPLE_SIZE = 250\n",
    "print(f\"Number of thresholds: {len(thresholds)}\")\n",
    "\n",
    "for split in [dataset for dataset in inference_logs if is_entropy_split(dataset)]:\n",
    "    print(split)\n",
    "    best_entropy_threshold = None\n",
    "    best_softmax_threshold = None\n",
    "    split_frame = inference_logs[split].to_pandas()\n",
    "    sample_frame = None\n",
    "    unique_predicted_classes = [class_label for class_label in split_frame[\"tta_predicted_class\"].unique() if class_label != -1] \n",
    "    for class_prediction in unique_predicted_classes:\n",
    "        sample_size = SAMPLE_SIZE // len(unique_predicted_classes)\n",
    "        class_sample_frame = split_frame[split_frame[\"tta_predicted_class\"] == class_prediction].sample(sample_size, random_state=RANDOM_SEED)\n",
    "        if sample_frame is None:\n",
    "            sample_frame = class_sample_frame\n",
    "        else:\n",
    "            sample_frame = pd.concat([sample_frame, class_sample_frame])\n",
    "    \n",
    "    threshold_performances = []\n",
    "    for threshold in tqdm(thresholds):\n",
    "        accuracy, llm_call_rate = get_entropy_threshold_accuracy(threshold, sample_frame)\n",
    "        beta = 1/500\n",
    "        rate_term = 1 - llm_call_rate\n",
    "        threshold_score = (1 + beta ** 2) * ((accuracy * rate_term) / ((beta ** 2) * accuracy + rate_term))\n",
    "        threshold_perf = {\n",
    "            \"threshold\": threshold,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"score\": threshold_score,\n",
    "            \"llm_call_rate\": f\"{llm_call_rate:.2f}%\",\n",
    "        }\n",
    "        threshold_performances.append(threshold_perf)\n",
    "\n",
    "        # if best_entropy_threshold is None or accuracy > best_entropy_threshold[\"accuracy\"]:\n",
    "        if best_entropy_threshold is None or threshold_score > best_entropy_threshold[\"score\"]:\n",
    "            best_entropy_threshold = threshold_perf\n",
    "\n",
    "    pd.DataFrame(threshold_performances).to_csv(f\"data/threshold_performances_{split}.csv\", index=False)\n",
    "    optimal_entropy_thresholds[split] = best_entropy_threshold\n",
    "    print(f\"Best Entropy Threshold: {best_entropy_threshold}\")\n",
    "\n",
    "# print(json.dumps(optimal_entropy_thresholds, indent=4))\n",
    "# print(json.dumps(optimal_softmax_thresholds, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_records = []\n",
    "\n",
    "for split_name in tqdm(optimal_entropy_thresholds):\n",
    "    split_logs = inference_logs[split_name].to_pandas()\n",
    "\n",
    "    perf_records.append({\n",
    "        \"split\": split_name,\n",
    "        \"tta\": \"None\",\n",
    "        \"accuracy\": classification_report(split_logs[\"label\"], split_logs[\"original_predicted_class\"], digits=4, zero_division=0, output_dict=True)[\"accuracy\"],\n",
    "    })\n",
    "\n",
    "    optimal_entropy_threshold = optimal_entropy_thresholds[split_name][\"threshold\"]\n",
    "    accuracy = get_entropy_threshold_accuracy(optimal_entropy_threshold, split_logs)[0]\n",
    "    perf_records.append({\n",
    "        \"split\": split_name,\n",
    "        \"tta\": \"entropy-based\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"augmentation_rate\": split_logs.apply(lambda row: should_augment_entropy(optimal_entropy_threshold, row), axis=1).sum() / len(split_logs),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_frame = pd.DataFrame(perf_records)\n",
    "results_frame[\"Dataset\"] = results_frame[\"split\"].apply(lambda s: s.split(\"_\")[-4])\n",
    "results_frame[\"Distribution\"] = results_frame[\"split\"].apply(lambda s: s.split(\"_\")[-3])\n",
    "results_frame[\"Model\"] = results_frame[\"split\"].apply(lambda s: s.split(\"_\")[-2])\n",
    "results_frame[\"TTA Method\"] = results_frame[\"split\"].apply(lambda s: s.split(\"_\")[-1])\n",
    "results_frame[\"Baseline Delta\"] = results_frame.apply(lambda row: row[\"accuracy\"] - results_frame[(results_frame[\"split\"] == row[\"split\"]) & (results_frame[\"tta\"] == \"None\")][\"accuracy\"].values[0], axis=1)\n",
    "results_frame.drop(columns=[\"split\"], inplace=True)\n",
    "results_frame.rename(columns={\"tta\": \"Selective Method\", \"accuracy\": \"Accuracy\", \"augmentation_rate\": \"Augmentation Rate\"}, inplace=True)\n",
    "\n",
    "aggregated_results = results_frame.groupby([\"Dataset\", \"Distribution\", \"Model\", \"TTA Method\", \"Selective Method\"]).mean().round(4) * 100\n",
    "aggregated_results = aggregated_results.sort_values(by=[\"Dataset\", \"Distribution\", \"Model\", \"TTA Method\", \"Accuracy\"], ascending=False)\n",
    "\n",
    "print(\"Overall Results\")\n",
    "# display(aggregated_results)\n",
    "\n",
    "# Average each TTA Method and Selective Method over distributions\n",
    "results_frame[\"ID\"] = results_frame[\"Distribution\"].apply(lambda d: \"ID\" in d)\n",
    "results_frame.drop(columns=[\"Distribution\", \"Accuracy\"], inplace=True)\n",
    "results_frame = results_frame[[\"Dataset\", \"ID\", \"Model\", \"TTA Method\", \"Selective Method\", \"Baseline Delta\", \"Augmentation Rate\"]]\n",
    "aggregated_results = results_frame.groupby([\"Dataset\", \"ID\", \"Model\", \"TTA Method\", \"Selective Method\"]).mean().round(4) * 100\n",
    "aggregated_results = aggregated_results.sort_values(by=[\"Dataset\", \"ID\", \"Model\", \"TTA Method\", \"Baseline Delta\"], ascending=False)\n",
    "print(\"Average Results\")\n",
    "for dataset in [\"Sentiment\", \"Toxicity\", \"AgNewsTweets\"]:\n",
    "    for tta_method in [\"Paraphrase\", \"ICR\"]:\n",
    "    # for tta_method in [\"ICR\"]:\n",
    "        print(f\"Dataset: {dataset}, TTA Method: {tta_method}\")\n",
    "        # display(aggregated_results.loc[dataset, :, :, tta_method, :])\n",
    "        # only show entropy-based\n",
    "        display(aggregated_results.loc[dataset, :, :, tta_method, \"entropy-based\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try ID Entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_optimal_entropy_thresholds = {}\n",
    "thresholds = np.arange(0, 1.2, 0.0001)\n",
    "print(f\"Number of thresholds: {len(thresholds)}\")\n",
    "\n",
    "if os.path.exists(\"data/id_optimal_entropy_thresholds.json\"):\n",
    "    with open(\"data/id_optimal_entropy_thresholds.json\", \"r\") as f:\n",
    "        id_optimal_entropy_thresholds = json.load(f)\n",
    "else:\n",
    "    for split in [dataset for dataset in inference_logs if not is_entropy_split(dataset) and \"Ablate\" not in dataset and \"ID_BERT\" in dataset]:\n",
    "        if \"Paraphrase\" not in split and \"ICR\" not in split:\n",
    "            continue\n",
    "\n",
    "        print(split)\n",
    "        best_entropy_threshold = None\n",
    "        split_frame = inference_logs[split].to_pandas()\n",
    "        unique_predicted_classes = [class_label for class_label in split_frame[\"tta_predicted_class\"].unique() if class_label != -1] \n",
    "        \n",
    "        threshold_performances = []\n",
    "        for threshold in tqdm(thresholds):\n",
    "            accuracy, llm_call_rate = get_entropy_threshold_accuracy(threshold, split_frame)\n",
    "            beta = 1/500\n",
    "            rate_term = 1 - llm_call_rate\n",
    "            threshold_score = (1 + beta ** 2) * ((accuracy * rate_term) / ((beta ** 2) * accuracy + rate_term))\n",
    "            threshold_perf = {\n",
    "                \"threshold\": threshold,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"score\": threshold_score,\n",
    "                \"llm_call_rate\": f\"{llm_call_rate:.2f}%\",\n",
    "            }\n",
    "            threshold_performances.append(threshold_perf)\n",
    "\n",
    "            # if best_entropy_threshold is None or accuracy > best_entropy_threshold[\"accuracy\"]:\n",
    "            if best_entropy_threshold is None or threshold_score > best_entropy_threshold[\"score\"]:\n",
    "                best_entropy_threshold = threshold_perf\n",
    "\n",
    "        pd.DataFrame(threshold_performances).to_csv(f\"data/threshold_performances_{split}.csv\", index=False)\n",
    "        id_optimal_entropy_thresholds[split] = best_entropy_threshold\n",
    "        print(f\"Best Entropy Threshold: {best_entropy_threshold}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best ID test set entropy instead of the best OOD entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ood_to_id_entropy(split_name):\n",
    "    method_name = split_name.split(\"_\")[-1]\n",
    "    model_name = split_name.split(\"_\")[-2]\n",
    "    distribution_name = split_name.split(\"_\")[-3]\n",
    "    task_name = split_name.replace(f\"_{distribution_name}_{model_name}_{method_name}\", \"\")\n",
    "    return f\"{task_name}_ID_{model_name}_{method_name}\"\n",
    "\n",
    "map_ood_to_id_entropy(\"BOSS_Toxicity_Toxigen_BERT_ICR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data/id_optimal_entropy_thresholds.json\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    with open(file_name, \"w\") as f:\n",
    "        json.dump(id_optimal_entropy_thresholds, f, indent=4)\n",
    "else:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_records = []\n",
    "\n",
    "# display(id_logs)\n",
    "# display(split_names)\n",
    "for split_name in tqdm(optimal_entropy_thresholds):\n",
    "    split_logs = inference_logs[split_name].to_pandas()\n",
    "\n",
    "    perf_records.append({\n",
    "        \"split\": split_name,\n",
    "        \"tta\": \"None\",\n",
    "        \"accuracy\": classification_report(split_logs[\"label\"], split_logs[\"original_predicted_class\"], digits=4, zero_division=0, output_dict=True)[\"accuracy\"],\n",
    "    })\n",
    "\n",
    "    id_split_name = map_ood_to_id_entropy(split_name)\n",
    "    optimal_entropy_threshold = id_optimal_entropy_thresholds[id_split_name][\"threshold\"]\n",
    "    accuracy = get_entropy_threshold_accuracy(optimal_entropy_threshold, split_logs)[0]\n",
    "    perf_records.append({\n",
    "        \"split\": split_name,\n",
    "        \"tta\": \"entropy-based\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"augmentation_rate\": split_logs.apply(lambda row: should_augment_entropy(optimal_entropy_threshold, row), axis=1).sum() / len(split_logs),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_frame = pd.DataFrame(perf_records)\n",
    "results_frame[\"Dataset\"] = results_frame[\"split\"].apply(lambda s: s.split(\"_\")[-4])\n",
    "results_frame[\"Distribution\"] = results_frame[\"split\"].apply(lambda s: s.split(\"_\")[-3])\n",
    "results_frame[\"Model\"] = results_frame[\"split\"].apply(lambda s: s.split(\"_\")[-2])\n",
    "results_frame[\"TTA Method\"] = results_frame[\"split\"].apply(lambda s: s.split(\"_\")[-1])\n",
    "results_frame[\"Baseline Delta\"] = results_frame.apply(lambda row: row[\"accuracy\"] - results_frame[(results_frame[\"split\"] == row[\"split\"]) & (results_frame[\"tta\"] == \"None\")][\"accuracy\"].values[0], axis=1)\n",
    "results_frame.drop(columns=[\"split\"], inplace=True)\n",
    "results_frame.rename(columns={\"tta\": \"Selective Method\", \"accuracy\": \"Accuracy\", \"augmentation_rate\": \"Augmentation Rate\"}, inplace=True)\n",
    "\n",
    "aggregated_results = results_frame.groupby([\"Dataset\", \"Distribution\", \"Model\", \"TTA Method\", \"Selective Method\"]).mean().round(4) * 100\n",
    "aggregated_results = aggregated_results.sort_values(by=[\"Dataset\", \"Distribution\", \"Model\", \"TTA Method\", \"Accuracy\"], ascending=False)\n",
    "\n",
    "print(\"Overall Results\")\n",
    "# display(aggregated_results)\n",
    "\n",
    "# Average each TTA Method and Selective Method over distributions\n",
    "results_frame[\"ID\"] = results_frame[\"Distribution\"].apply(lambda d: \"ID\" in d)\n",
    "results_frame.drop(columns=[\"Distribution\", \"Accuracy\"], inplace=True)\n",
    "results_frame = results_frame[[\"Dataset\", \"ID\", \"Model\", \"TTA Method\", \"Selective Method\", \"Baseline Delta\", \"Augmentation Rate\"]]\n",
    "aggregated_results = results_frame.groupby([\"Dataset\", \"ID\", \"Model\", \"TTA Method\", \"Selective Method\"]).mean().round(4) * 100\n",
    "aggregated_results = aggregated_results.sort_values(by=[\"Dataset\", \"ID\", \"Model\", \"TTA Method\", \"Baseline Delta\"], ascending=False)\n",
    "print(\"Average Results\")\n",
    "for dataset in [\"Sentiment\", \"Toxicity\", \"AgNewsTweets\"]:\n",
    "    for tta_method in [\"Paraphrase\", \"ICR\"]:\n",
    "    # for tta_method in [\"ICR\"]:\n",
    "        print(f\"Dataset: {dataset}, TTA Method: {tta_method}\")\n",
    "        # display(aggregated_results.loc[dataset, :, :, tta_method, :])\n",
    "        # only show entropy-based\n",
    "        display(aggregated_results.loc[dataset, :, :, tta_method, \"entropy-based\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID vs OOD Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_analysis_records = []\n",
    "for split_name in tqdm(inference_logs):\n",
    "    if \"BERT\" not in split_name or \"Ablate\" in split_name:\n",
    "        continue\n",
    "    \n",
    "    split_frame = inference_logs[split_name].to_pandas()\n",
    "    mean_original_prediction_entropy = split_frame[\"original_prediction_entropy\"].mean()\n",
    "    is_id = \"ID\" if split_name.split(\"_\")[-3] == \"ID\" else \"OOD\"\n",
    "    task_name = split_name.split(\"_\")[-4]\n",
    "    if task_name == \"AgNewsTweets\":\n",
    "        task_name = \"News\"\n",
    "\n",
    "    entropy_analysis_records.append({\n",
    "        \"id\": is_id,\n",
    "        \"task\": task_name,\n",
    "        \"mean_original_prediction_entropy\": mean_original_prediction_entropy,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_frame = pd.DataFrame(entropy_analysis_records).groupby([\"task\", \"id\"]).mean()\n",
    "display(entropy_frame)\n",
    "\n",
    "# plot bar chart of mean original prediction entropy for each task\n",
    "fig, axes = plt.subplots(1, 3, figsize=(3 * FIG_SIZE, FIG_SIZE - 1))\n",
    "for index, task in enumerate([\"Sentiment\", \"Toxicity\", \"News\"]):\n",
    "    sns.barplot(\n",
    "        ax=axes[index],\n",
    "        data=entropy_frame.loc[task].reset_index(),\n",
    "        x=\"id\",\n",
    "        y=\"mean_original_prediction_entropy\",\n",
    "        width=0.5,\n",
    "        hue=\"id\",\n",
    "    )\n",
    "\n",
    "    axes[index].set_title(task, fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "    # add padding for labels\n",
    "    axes[index].set_ylabel(\"Mean Entropy\")\n",
    "    axes[index].set_xlabel(\"Distribution\")\n",
    "\n",
    "    # set x and y axis to percents\n",
    "    axes[index].yaxis.set_major_formatter(lambda x, pos: f\"{x:.2f}\")\n",
    "\n",
    "    axes[index].locator_params(axis=\"y\", nbins=5)\n",
    "\n",
    "fig.subplots_adjust(wspace=WSPACE + 0.05, hspace=WSPACE)\n",
    "fig.savefig(\"figures/id_vs_ood_entropy.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy-Based Selective Aggregation\n",
    "\n",
    "What if we only aggregate predictions which are below the ID entropy threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don'y show the whole dataframe in display\n",
    "pd.set_option('display.max_rows', 6)\n",
    "\n",
    "sst5_frame = inference_logs[\"BOSS_Sentiment_SST5_BERT_ICR\"].to_pandas()\n",
    "# display(sst5_frame.head())\n",
    "\n",
    "def select_aggregations(entropies, test_input, test_input_entropy, aug_probs, entropy_threshold):\n",
    "    aug_probs = aug_probs[:5]\n",
    "    selected_augmentations = [aug_probs[-1]]\n",
    "    aug_probs = aug_probs[:-1]\n",
    "\n",
    "    if test_input_entropy < entropy_threshold:\n",
    "        return selected_augmentations\n",
    "\n",
    "    for entropy, augmentation_prob_dist in zip(entropies, aug_probs):\n",
    "        if entropy < entropy_threshold:\n",
    "            selected_augmentations.append(augmentation_prob_dist)\n",
    "\n",
    "    return selected_augmentations\n",
    "\n",
    "sst5_frame[\"selected_augmentations\"] = sst5_frame.apply(lambda row: select_aggregations(\n",
    "    row[\"tta_all_class_entropy\"],\n",
    "    row[\"original_text\"],\n",
    "    row[\"original_prediction_entropy\"],\n",
    "    row[\"tta_all_class_probs\"],\n",
    "    id_optimal_entropy_thresholds[\"BOSS_Sentiment_ID_BERT_ICR\"][\"threshold\"]),\n",
    "    axis=1)\n",
    "\n",
    "display(sst5_frame[\"selected_augmentations\"].apply(lambda x: len(x)).sum())\n",
    "sst5_frame[\"mean_selected_augmentation\"] = sst5_frame[\"selected_augmentations\"].apply(lambda x: np.array(x).mean(axis=0))\n",
    "sst5_frame[\"selected_prediction\"] = sst5_frame[\"mean_selected_augmentation\"].apply(lambda x: x.argmax())\n",
    "# display(sst5_frame.head(3))\n",
    "\n",
    "print(classification_report(sst5_frame[\"label\"], sst5_frame[\"original_predicted_class\"], digits=4, zero_division=0))\n",
    "print(classification_report(sst5_frame[\"label\"], sst5_frame[\"tta_predicted_class\"], digits=4, zero_division=0))\n",
    "print(classification_report(sst5_frame[\"label\"], sst5_frame[\"selected_prediction\"], digits=4, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICR Embeddings Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-roberta-large\")\n",
    "model = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-roberta-large\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_aug_embedding(augmentations):\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "        for augmentation in augmentations:\n",
    "            tokenized_text = tokenizer(augmentation, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            input_ids = tokenized_text[\"input_ids\"].to(device)\n",
    "            outputs = model(input_ids).pooler_output[0]\n",
    "            embeddings.append(outputs)\n",
    "\n",
    "        return torch.stack(embeddings).mean(dim=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icr_sst_logs = inference_logs[\"BOSS_Sentiment_SST5_BERT_ICR\"].to_pandas().sample(1072, random_state=RANDOM_SEED)\n",
    "icr_sst_logs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icr_amazon_logs = inference_logs[\"BOSS_Sentiment_ID_BERT_ICR\"].to_pandas().sample(1072, random_state=RANDOM_SEED)\n",
    "icr_amazon_logs[\"orig_embedding\"] = icr_amazon_logs.progress_apply(lambda row: get_mean_aug_embedding([row[\"original_text\"]]), axis=1)\n",
    "icr_amazon_logs[\"Distribution\"] = \"In-Distribution\"\n",
    "icr_amazon_logs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icr_sst_logs[\"orig_embedding\"] = icr_sst_logs.progress_apply(lambda row: get_mean_aug_embedding([row[\"original_text\"]]), axis=1)\n",
    "icr_sst_logs[\"augs_embedding\"] = icr_sst_logs.progress_apply(lambda row: get_mean_aug_embedding(row[\"augmentations\"]), axis=1)\n",
    "icr_sst_logs[\"Distribution\"] = \"Out-of-Distribution\"\n",
    "icr_sst_logs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_embeddings_frame = icr_amazon_logs[[\"Distribution\", \"orig_embedding\"]].rename(columns={\"orig_embedding\": \"Embedding\"})\n",
    "ood_embeddings_frame = icr_sst_logs[[\"Distribution\", \"orig_embedding\"]].rename(columns={\"orig_embedding\": \"Embedding\"})\n",
    "ood_augs_embeddings_frame = icr_sst_logs[[\"Distribution\", \"augs_embedding\"]].rename(columns={\"augs_embedding\": \"Embedding\"})\n",
    "ood_augs_embeddings_frame[\"Distribution\"] = \"Out-of-Distribution Augmented\"\n",
    "all_embeddings_frame = pd.concat([id_embeddings_frame, ood_embeddings_frame, ood_augs_embeddings_frame])\n",
    "all_embeddings_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_embeddings_frame_centroid = torch.Tensor(id_embeddings_frame[\"Embedding\"].mean())\n",
    "ood_embeddings_frame_centroid = torch.Tensor(ood_embeddings_frame[\"Embedding\"].mean())\n",
    "ood_augs_embeddings_frame_centroid = torch.Tensor(ood_augs_embeddings_frame[\"Embedding\"].mean())\n",
    "\n",
    "cos = CosineSimilarity(dim=0, eps=1e-6)\n",
    "print(f\"OOD Original vs ID: {cos(ood_embeddings_frame_centroid, id_embeddings_frame_centroid):.4f}\")\n",
    "print(f\"OOD Augmented vs ID: {cos(ood_augs_embeddings_frame_centroid, id_embeddings_frame_centroid):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_input = all_embeddings_frame[\"Embedding\"].to_list()\n",
    "umap_2d = UMAP(n_components=2, init='random', random_state=0)\n",
    "all_embeddings_projections = umap_2d.fit_transform(fit_input)\n",
    "all_embeddings_projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings_frame[\"UMAP 1\"] = all_embeddings_projections[:, 0]\n",
    "all_embeddings_frame[\"UMAP 2\"] = all_embeddings_projections[:, 1]\n",
    "fig, axes = plt.subplots(1, 1, figsize=(FIG_SIZE * 1.5, FIG_SIZE * 1.5))\n",
    "sns.scatterplot(data=all_embeddings_frame, x=\"UMAP 1\", y=\"UMAP 2\", hue=\"Distribution\", ax=axes)\n",
    "\n",
    "# no x and y labels\n",
    "axes.set_xlabel(\"\")\n",
    "axes.set_ylabel(\"\")\n",
    "\n",
    "# set legend below the figure\n",
    "axes.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.10), ncol=3, frameon=False, fontsize=14)\n",
    "\n",
    "# no grid\n",
    "axes.grid(False)\n",
    "\n",
    "# no ticks\n",
    "axes.set_xticks([])\n",
    "axes.set_yticks([])\n",
    "\n",
    "# sns.set_style(\"whitegrid\")\n",
    "fig.savefig(\"figures/method_analysis_umap_embeddings.png\", bbox_inches=\"tight\")\n",
    "# sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Cosine Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_embeddings = all_embeddings_frame[all_embeddings_frame[\"Distribution\"] == \"In-Distribution\"][\"Embedding\"].to_list()\n",
    "ood_embeddings = all_embeddings_frame[all_embeddings_frame[\"Distribution\"] == \"Out-of-Distribution\"][\"Embedding\"].to_list()\n",
    "ood_augs_embeddings = all_embeddings_frame[all_embeddings_frame[\"Distribution\"] == \"Out-of-Distribution Augmented\"][\"Embedding\"].to_list()\n",
    "\n",
    "all_embeddings_frame[\"ID Distances\"] = all_embeddings_frame.progress_apply(lambda row: [cos(torch.Tensor(row[\"Embedding\"]), torch.Tensor(id_embedding)) for id_embedding in id_embeddings], axis=1)\n",
    "all_embeddings_frame[\"OOD Distances\"] = all_embeddings_frame.progress_apply(lambda row: [cos(torch.Tensor(row[\"Embedding\"]), torch.Tensor(ood_embedding)) for ood_embedding in ood_embeddings], axis=1)\n",
    "all_embeddings_frame[\"OOD Augmented Distances\"] = all_embeddings_frame.progress_apply(lambda row: [cos(torch.Tensor(row[\"Embedding\"]), torch.Tensor(ood_aug_embedding)) for ood_aug_embedding in ood_augs_embeddings], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_id_distance_points = []\n",
    "for distance in all_embeddings_frame[all_embeddings_frame[\"Distribution\"] == \"In-Distribution\"][\"ID Distances\"]:\n",
    "    all_id_distance_points.extend(distance)\n",
    "\n",
    "all_ood_distance_points = []\n",
    "for distance in all_embeddings_frame[all_embeddings_frame[\"Distribution\"] == \"Out-of-Distribution\"][\"ID Distances\"]:\n",
    "    all_ood_distance_points.extend(distance)\n",
    "\n",
    "all_ood_aug_distance_points = []\n",
    "for distance in all_embeddings_frame[all_embeddings_frame[\"Distribution\"] == \"Out-of-Distribution Augmented\"][\"ID Distances\"]:\n",
    "    all_ood_aug_distance_points.extend(distance)\n",
    "\n",
    "print(f\"Mean similarity (ID, ID): {np.mean(all_id_distance_points):.4f}\")\n",
    "print(f\"Mean similarity (ID, OOD): {np.mean(all_ood_distance_points):.4f}\")\n",
    "print(f\"Mean similarity (ID, OOD Augmented): {np.mean(all_ood_aug_distance_points):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings_frame.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
