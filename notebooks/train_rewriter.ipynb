{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyle/miniconda3/envs/icdt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, LlamaTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, AutoModelForQuestionAnswering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_objects(model_name, num_labels, training=False):\n",
    "    model_config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "    is_seq2seq_lm = model_config.architectures[0].endswith(\"ForConditionalGeneration\")\n",
    "    is_qa_model = model_config.architectures[0].endswith(\"ForQuestionAnswering\")\n",
    "    is_llm = model_config.architectures[0].endswith(\"ForCausalLM\")\n",
    "    is_llama_based_model = is_llm and \"llama\" in model_name or \"vicuna\" in model_name\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name) if is_llama_based_model else AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = \"<s>\" if tokenizer.pad_token in [None, \"\"] and str(tokenizer.eos_token) in [None, \"\"] else tokenizer.eos_token\n",
    "\n",
    "    model = None\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    numerical_precision = torch.float32 if training else torch.float16\n",
    "    if is_llm:\n",
    "        num_billions = [int(entry[:-1]) for entry in model_name.split(\"-\") if entry[0].isdigit() and entry.lower().endswith(\"b\")]\n",
    "        load_in_8bit = (len(num_billions) > 0 and num_billions[0] > 7) or training\n",
    "        if load_in_8bit:\n",
    "            print(\"Loading in 8-bit mode since the model has more than 7B parameters or we are training.\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, load_in_8bit=True, llm_int8_threshold=0, device_map=\"auto\").eval()\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=numerical_precision).eval().to(device)\n",
    "    elif is_qa_model:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(model_name, trust_remote_code=True, torch_dtype=numerical_precision).eval().to(device)\n",
    "    elif is_seq2seq_lm:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=numerical_precision).eval().to(device)\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True, num_labels=num_labels).eval().to(device)\n",
    "    return tokenizer, model\n",
    "\n",
    "paraphrase_tokenizer, paraphrase_model = get_model_objects(\"humarin/chatgpt_paraphraser_on_T5_base\", num_labels=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Demonstrates that the director of Hollywood blockbuster hits like Patriot Games can still produce a small, personal film with an emotional element.',\n",
       " 'Shows that the director of Hollywood blockbuster hits like Patriot Games can still produce a small, personal film with an emotional element.',\n",
       " 'It demonstrates that the director of Hollywood blockbuster hits like Patriot Games can still make a small, personal film with an emotional depth.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_paraphrase_augmentations(\n",
    "    question,\n",
    "    paraphrase_tokenizer,\n",
    "    paraphrase_model,\n",
    "    device,\n",
    "    num_beams=5,\n",
    "    num_beam_groups=5,\n",
    "    num_return_sequences=3,\n",
    "    repetition_penalty=100.0,\n",
    "    diversity_penalty=100.0,\n",
    "    no_repeat_ngram_size=10,\n",
    "    temperature=0.7,\n",
    "    max_length=128,\n",
    "):\n",
    "    input_ids = paraphrase_tokenizer(\n",
    "        f\"paraphrase: {question}\",\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids.to(device)\n",
    "\n",
    "    outputs = paraphrase_model.generate(\n",
    "        input_ids,\n",
    "        temperature=temperature,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams,\n",
    "        num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length,\n",
    "        diversity_penalty=diversity_penalty,\n",
    "    )\n",
    "\n",
    "    res = paraphrase_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return res\n",
    "\n",
    "example_text = \"demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \"\n",
    "example_augs = get_paraphrase_augmentations(example_text, paraphrase_tokenizer, paraphrase_model, paraphrase_model.device)\n",
    "example_augs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def get_embeddings(tokenizer, model, left_text, right_text):\n",
    "    encoded_input = tokenizer(left_text, right_text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    return mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "\n",
    "def get_cosine_similarity(sentence_tokenizer, sentence_model, left_text, right_text):\n",
    "    left_embedding = get_embeddings(sentence_tokenizer, sentence_model, left_text, right_text)\n",
    "    right_embedding = get_embeddings(sentence_tokenizer, sentence_model, right_text, left_text)\n",
    "    return F.cosine_similarity(left_embedding, right_embedding).item()\n",
    "\n",
    "\n",
    "hf_model_path = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "sentence_tokenizer = AutoTokenizer.from_pretrained(hf_model_path)\n",
    "sentence_model = AutoModel.from_pretrained(hf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9837890863418579\n",
      "0.987225353717804\n",
      "0.9868114590644836\n"
     ]
    }
   ],
   "source": [
    "for aug in example_augs:\n",
    "    print(get_cosine_similarity(sentence_tokenizer, sentence_model, aug, example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['demonstrates first that the of hollywood blockbuster series hits which patriot still to produce remain small, personal with emotional.',\n",
       " 'also shows that just the director of transforming blockbuster hits into games can still make small, personal film with.',\n",
       " 'demonstrates films that the creative director of many hollywood blockbuster hits patriot games can and still make, and personal film helps with.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "word_augmenter = naw.ContextualWordEmbsAug(device=\"cuda\", action=\"insert\")\n",
    "random_deleter = naw.RandomWordAug(action=\"delete\", aug_p=0.30)\n",
    "\n",
    "sub_augs = [word_augmenter.augment(random_deleter.augment(aug)) for aug in example_augs]\n",
    "sub_augs = [aug[0] for aug in sub_augs if len(aug) > 0]\n",
    "sub_augs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('demonstrates first that the of hollywood blockbuster series hits which patriot still to produce remain small, personal with emotional.',\n",
       "  0.9846742749214172),\n",
       " ('also shows that just the director of transforming blockbuster hits into games can still make small, personal film with.',\n",
       "  0.9737756848335266),\n",
       " ('demonstrates films that the creative director of many hollywood blockbuster hits patriot games can and still make, and personal film helps with.',\n",
       "  0.9829103946685791)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cosines = [get_cosine_similarity(sentence_tokenizer, sentence_model, aug, example_text) for aug in sub_augs]\n",
    "aug_cosine_pairs = list(zip(sub_augs, cosines))\n",
    "display(aug_cosine_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " ('demonstrates first that the of hollywood blockbuster series hits which patriot still to produce remain small, personal with emotional.',\n",
       "  0.9846742749214172))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(enumerate(aug_cosine_pairs), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corrupted IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/kyle/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>A hit at the time but now better categorised a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>I love this movie like no other. Another time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>This film and it's sequel Barry Mckenzie holds...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>'The Adventures Of Barry McKenzie' started lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>The story centers around Barry McKenzie who mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   label  class\n",
       "0      I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1      \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2      If only to avoid making this type of film in t...      0\n",
       "3      This film was probably inspired by Godard's Ma...      0\n",
       "4      Oh, brother...after hearing about this ridicul...      0\n",
       "...                                                  ...    ...\n",
       "24995  A hit at the time but now better categorised a...      1\n",
       "24996  I love this movie like no other. Another time ...      1\n",
       "24997  This film and it's sequel Barry Mckenzie holds...      1\n",
       "24998  'The Adventures Of Barry McKenzie' started lif...      1\n",
       "24999  The story centers around Barry McKenzie who mu...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "imdb_train_split = load_dataset(\"imdb\", split=\"train\").to_pandas()\n",
    "imdb_train_split.rename(columns={\"label\": \"class\",\"text\": \"label\"}, inplace=True)\n",
    "imdb_train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [35:31<00:00,  2.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19762</th>\n",
       "      <td>Very good dramatic comedy about a playwright t...</td>\n",
       "      <td>1</td>\n",
       "      <td>(demonstrates first that the of hollywood bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3902</th>\n",
       "      <td>Forget Plan 9, this is the ultimate fiasco, a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(demonstrates first that the of hollywood bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21922</th>\n",
       "      <td>Some movies you just know you're going to love...</td>\n",
       "      <td>1</td>\n",
       "      <td>(demonstrates first that the of hollywood bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>The past creeps up on a rehab-addict when he r...</td>\n",
       "      <td>0</td>\n",
       "      <td>(demonstrates first that the of hollywood bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9288</th>\n",
       "      <td>To the small minority seen here praising this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(demonstrates first that the of hollywood bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13089</th>\n",
       "      <td>This is mostly a story about the growing relat...</td>\n",
       "      <td>1</td>\n",
       "      <td>(demonstrates first that the of hollywood bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>This film has nothing whatever to do with the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(demonstrates first that the of hollywood bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20587</th>\n",
       "      <td>The Dentist starts on the morning of Dr. Alan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>(demonstrates first that the of hollywood bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16399</th>\n",
       "      <td>I had no idea that Mr. Izzard was so damn funn...</td>\n",
       "      <td>1</td>\n",
       "      <td>(demonstrates first that the of hollywood bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24476</th>\n",
       "      <td>Greetings again from the darkness. Remember al...</td>\n",
       "      <td>1</td>\n",
       "      <td>(demonstrates first that the of hollywood bloc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   label  class  \\\n",
       "19762  Very good dramatic comedy about a playwright t...      1   \n",
       "3902   Forget Plan 9, this is the ultimate fiasco, a ...      0   \n",
       "21922  Some movies you just know you're going to love...      1   \n",
       "2869   The past creeps up on a rehab-addict when he r...      0   \n",
       "9288   To the small minority seen here praising this ...      0   \n",
       "...                                                  ...    ...   \n",
       "13089  This is mostly a story about the growing relat...      1   \n",
       "4560   This film has nothing whatever to do with the ...      0   \n",
       "20587  The Dentist starts on the morning of Dr. Alan ...      1   \n",
       "16399  I had no idea that Mr. Izzard was so damn funn...      1   \n",
       "24476  Greetings again from the darkness. Remember al...      1   \n",
       "\n",
       "                                                    text  \n",
       "19762  (demonstrates first that the of hollywood bloc...  \n",
       "3902   (demonstrates first that the of hollywood bloc...  \n",
       "21922  (demonstrates first that the of hollywood bloc...  \n",
       "2869   (demonstrates first that the of hollywood bloc...  \n",
       "9288   (demonstrates first that the of hollywood bloc...  \n",
       "...                                                  ...  \n",
       "13089  (demonstrates first that the of hollywood bloc...  \n",
       "4560   (demonstrates first that the of hollywood bloc...  \n",
       "20587  (demonstrates first that the of hollywood bloc...  \n",
       "16399  (demonstrates first that the of hollywood bloc...  \n",
       "24476  (demonstrates first that the of hollywood bloc...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_augmentation(current_text):\n",
    "    paraphrases = get_paraphrase_augmentations(current_text, paraphrase_tokenizer, paraphrase_model, paraphrase_model.device)\n",
    "    corrupted_paraphrases = [word_augmenter.augment(random_deleter.augment(aug)) for aug in example_augs]\n",
    "    corrupted_paraphrases = [aug for aug in sub_augs if len(aug) > 0]\n",
    "    corrupted_cosines = [get_cosine_similarity(sentence_tokenizer, sentence_model, current_text, aug) for aug in corrupted_paraphrases]\n",
    "    corrupted_aug_cosine_pairs = list(zip(corrupted_paraphrases, corrupted_cosines))\n",
    "    most_corrupted = max(enumerate(corrupted_aug_cosine_pairs), key=lambda x: x[1])[1]\n",
    "    return most_corrupted\n",
    "\n",
    "\n",
    "sample = imdb_train_split.sample(1000)\n",
    "sample[\"text\"] = sample.progress_apply(lambda row: get_augmentation(row[\"label\"]), axis=1)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
