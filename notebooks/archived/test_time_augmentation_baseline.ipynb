{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import nlpaug.augmenter.word as naw\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"fears for t n williams pension continued after talks unions representing workers include at save turner newall say they finally are'disappointed'after these talks began with depression stricken parent oil firm federal mogul.\",\n",
       " \"shareholder fears rise for the t n pension after talks unions in representing workers at turner draper newall say although they are'disappointed'left after talks with stricken parent of firm of federal mogul.\",\n",
       " \"fears for t Â· n pension after talks unions representing uk workers but at turner newall could say either they are'disappointed'dropped after public talks ended with the stricken parent firm federal mogul.\",\n",
       " \"final fears for t a n pension emerge after talks unions representing workers at former turner newall say they both are'deeply disappointed'shortly after agreement talks begin with stricken parent firm federal mogul.\"]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Fears for T N pension after talks Unions representing workers at Turner Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"\n",
    "aug = naw.ContextualWordEmbsAug(action=\"insert\")\n",
    "augmented_text = aug.augment(text, n=4)\n",
    "augmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mosesju/distilbert-base-uncased-finetuned-news\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mosesju/distilbert-base-uncased-finetuned-news\")\n",
    "model.to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2890, -4.1149,  3.5764, -1.7495],\n",
       "        [-0.0573, -3.7940,  3.6894, -1.6535],\n",
       "        [-0.1671, -3.6222,  3.4594, -1.3398],\n",
       "        [ 0.0566, -3.7586,  3.4811, -1.5894],\n",
       "        [-0.3797, -3.5373,  3.6425, -1.3264]], device='cuda:0',\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_logits = []\n",
    "for input_text in [text] + augmented_text:\n",
    "    input_logits.append(model(**tokenizer(input_text, return_tensors=\"pt\").to(device))[0][0])\n",
    "\n",
    "input_logits = torch.stack(input_logits)\n",
    "display(input_logits)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0517, -3.7654,  3.5698, -1.5317], device='cuda:0',\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_logits = input_logits.mean(dim=0)\n",
    "mean_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2, device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_logits.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.5823e-02, 4.3809e-04, 9.5907e-01, 4.6653e-03],\n",
       "        [2.2934e-02, 5.4656e-04, 9.7187e-01, 4.6476e-03],\n",
       "        [2.5692e-02, 8.1146e-04, 9.6554e-01, 7.9523e-03],\n",
       "        [3.1324e-02, 6.9017e-04, 9.6195e-01, 6.0402e-03],\n",
       "        [1.7466e-02, 7.4277e-04, 9.7501e-01, 6.7770e-03]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(input_logits, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
